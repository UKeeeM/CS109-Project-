There's some other funny stuff, like them misusing processor redundancy. The idea is you have two processors running your control system, that way if one gets hit by some fluke EM radiation or something (it happens, though not often), the other one will yield a different result and the system will know they need to rerun the computation.

However, both of these processors were being fed to by the SAME chip, so if that chip got hit by a neutrino burst, you're going to have a bad time.
Used to work in aerospace, and it's quite common for all of the variables of a model to be global, with read/write access, for simulations, testing, etc.

This is news from 2013...
What counts as a global in this case?

Are we talking about variables which are only used within a file, but have external linkage by default since they weren't declared `static`? (which isn't necessarily a problem, especially in embedded systems that don't have `malloc`)

Or are we talking about *actual* global variables which are linked and modified in multiple compilation units? (which is a 'fuck-you no' problem)
So... you have to remember that this is from someone who was paid to be a witness in a trial against Toyota.  In ECU code you see a lot code that was autogenned from a matlab model of the engine.  This autogenned code loves global variables.  That's just one of many ways you could hit 10K globals, but it not actually be as bad as he's saying it is.
Why was NASA reviewing it?
Is there source code for any other cars available? I've read this article a few times and I keep wondering if this is actually any worse than any other cars.
This has been my world for the last year or so.

I'm doing maintenance and bug fixes on a Java (and COBOL) based custom framework. Most of the work was done by an external contract company.

As example, our logs are crazy large. They were talking about either assigning more space or making the window from 90 days to 31. All this shit gets archived as well. One fucking batch process was running with all 4 debug flags set. This made for gigs of logs. When I fixed it, it went to kilobytes of logs. Also there was a bunch of logger/displays that got moved into production.

Another issue was a relatively simple batch process was taking around 6 hours to run (processing about 3k to 4k of records). I re-wrote it, and it now takes around 10 minutes to run.

Fun stuff ...
This will go down as the Therac-25 of our generation. More damning than the code itself (and the numerous ECU hardware design and manufacturing flaws) is the fact that Toyota stonewalled the investigation, was convicted of criminal deceit by a federal court, admitted no fault (although it was fined over $2.5B, a record amount), and subsequently fired the head of the division responsible for producing this unit.

Looking at this thread... I'm sure the coding style in Therac-25 looked reasonable to some practitioners at the time too. 
I don't know much about embedded programming. I've done a little, and there are "global variables" that aren't really "variables" in the usual sense but represent some real-world thing, like whether an input line is high or low or the output of some sensor (e.g. reading the variable actually gets the value from the sensor, it changes without the code modifying it) Does that have anything to do with it? 
If the choice is between a cable running from the gas pedal to the throttle, or 10MB of embedded code running on redundant CPU's requiring watchdog monitors, sounds like the cable is the better solution here.  
10000 global variables are neither a problem nor a code smell in embedded code. Global variables are often the safer choice (compared to dynamic memory allocation) in embedded systems as they are much easier to reason about for static analysis tools. Of course, you have to be disciplined when you write code this way.
You need to get all pretentious about how bad everyone else's code is.
After reading this I firmly believe that liability for software bugs _must_ exist.
Didn't the unintended accelerations turn out to be...user error, and not an actual vehicle flaw? At least according to government investigations? They recalled the floormats on some vehicles, but never actually afiak tweaked the vehicle code or anything hardware related...
&gt; Michael Barr, a well-respected embedded software specialist, spent more than 20 months reviewing Toyota’s source code at one of five cubicles in a hotel-sized room, supervised by security guards, who ensured that entrants brought no paper in or out, and **wore no belts** or watches.

Were they worried the code was so bad that he was going to hang himself?

Seriously, though. Why is this?
TL;DR: Toyota's firmware is the PHP of the automotive world
If your car was accelerating out of control why wouldn't you just hit the clutch and thus remove power from the drivetrain?
I don't get it though, Can't you just turn the engine off?
I think i just threw up a little. 
I'm curious what ya'll think: which is safer for the driver - throttle by wire\* or a mechanical throttle\*\*?

\*As in "fly by wire" where the driver's input is read but the throttle is ultimately controlled by microprocessors etc and the throttle operation could possibly be completely different than the driver's input/intent. 

\*\*Controlled by a cable. 

Edit: \*\*Or controlled by a cable but with a couple of sensors (no complicated microprocessor, just some electronic parts) that perhaps *adjust* throttle response for efficiency, etc but cannot outright control or override the operator's inputs. 


When I wrote my first larger program in Pascal, I declared _10_ loop variables globally. Yeah baby! Go big or go home!
TEN **THOUSAND** globals??????

Well, pluck me a flying duck, but that's just bloody astounding.  (I have written code for a living for decades.  Not webby-developer-stuff, but scientific / technical / process control.)

... *and*, I now drive a Prius!

Muck fe . . . .
Written in India?
Let this be a screenplay: Technical Debt, The Movie. 
Old news.
It doesn't surprise me at all to find that Toyota's coding is horrific. Coding in general (and UI/UX design) here in Japan is shockingly bad.
Our management used to encourage us to do work [Toyota Way](https://en.wikipedia.org/wiki/The_Toyota_Way)

I worked for a software company that bought some code from a third party to incorporate into our software.  I was doing the macintosh port.  The code was written with so many global variables that it overflowed the executable table of contents.

Basically there were so many global variables that the mac executable format couldn't keep them all any more, so without major restructuring there was no way to incorporate the code into the mac port. 


Except if you don't want to know CS.
my_fucking_variable_in_memory[51*52] = 100;
I mean the code is bad or whatever but the bigger problem here is that people don't understand the basic concepts of how cars or more specifically transmissions work because if something has gone amiss and you need to stop accelerating immediately you throw it in neutral and disaster avoided. None of this pulling on the hand brake shit which is a horrible thing to do and one of the easiest ways to lose control of the car. At least pull on it gradually. 
If something fails just kill power and everything falls back to the person who wrote that particular code.
Toyota-Riot Partnership coming soon. 
Which car companies use proper coding practices?
lol
Well at least now I know if I want a job, the bar is set very, very low!!
Whoa. That's a lot of global state.
So between this and the VW thing?  There needs to be a code review.
Lets hope that with more resources available, the people that write firmware and drivers make less hack in their code, and less evil practices. Just now, with the excuse of limited resources, can't be critized (and probably they solve some fucking hard problems) but I hope this excuse have a end, because what they are forced to do, HAS to create a lot of horrible bad code. 
Anyone have a mirror? The article seems to have been taken down.
I recall that there was a lot of investigation into this.

It ended up more often than not being user error. Specifically elderly, IIRC.
It's not about the 10000 global variables, although that is a really bad sign.

The issue is that most people cannot 'see' software.
Most of us are familiar enough with physical things that we can tell when something is poorly constructed. 

A software 'hack' is the equivalent of duct tape. Yet so much software contains little hacks to get it to work. I can almost guarantee you that if on a Toyota assembly line, it was some worker's job to duct tape lots of things to keep the car together, someone would freak out.

Software? No one sees anything, so you can get away with a lot of crap and this can lead poor designers to product spaghetti code.

That's usual in Japanese software, embedded or not. Almost noone understand OOP. Tests are manual, only few heard about unittesting. 
Code has to have variables and functions names in cryptic way
It can be even prohibited by mangers to use OOP and unittesting, because "noone else can understand your code". Even not in embedded app multi-threading can be done as loop with bool variable check sometimes without sleep. 

One of the best samples in my collection of stupid Japanese code is floor double value by converting it to text, searching for dot, cutting everything after and converting back to double. That code is use in one of biggest Japanese financial company. The guy who made it died of karoshi and I had to convert their system from C to Java, as noone could understand and support anything, and there was no any documentation or comments. Even then I was forced to rewrite everything as is including that floor, cryptic names and return errors as return values.
2013 news, seriously ?
They must have some really bad C programmers. I really don't see why they are using any global variables.
Software needs to be an engineering discipline like right now, this bullshit 'wild west' approach to code has gone on too long.
People don't want to know CS.
One day lawmakers are going to come after programmers for mistakes like this. 
[deleted]
What kind of programming controls a car?  Also, is there a reddit for this?
I guess Toyota couldn't pull a VW because there software sucks sooo bad. This is much, much worse than the VW thing, IMHO. Scares me a bit. Used to drive a Toyota...and now a VW (but not diesel).
This is why I worry about self-driving cars. I understand humans make a lot of mistakes, but I also understand how bad some software is written, especially if it's written by companies whose main focus isn't software. 
Shouldn't it have resulted in a complete ban on ALL Toyota cars that used such software?
My blood always boils a bit when academics criticize code.
Well embedded programmers, copy and paste.
Did they pay the programmers minimum wage? I don't see how else anyone could use such shitty practice
More bullshit fearmongering against the evil foreign Japanese car company paid for by Detroit.
If not the real model was shown, this could be seen as contempt of court. 

If Toyota cannot even write software for a non-autonomous car in any decent way, why should an investor even consider them? That technical debt is going to hurt profits and as a consumer I don't want to support broken software either. 

So, Toyota should now hold some public events where you can actually see how they bettered their ways or at least I won't be interested anymore and I am sure many others with me. 
Looking at [Barr's slides](http://www.safetyresearch.net/Library/BarrSlides_FINAL_SCRUBBED.pdf), there is this gem ("EXAMPLE C LANGUAGE SOURCE CODE"):

    int larger_of(int a, int b) 
    { 
      if (a &gt; b) 
      { 
        return a; /* a contains the larger value */ 
      } 
      else 
      { 
        return b; /* b contains the larger value */ 
      } 
    }     

Now, I realize this is an oversimplification, but it contains an error (if a == b, b is returned as the "larger" value), and it also violates testability and MISRA for having two exit points in one function.


Toyota invented Agile... Just sayin'.
This article keeps popping up and it drives me mad.

&gt; Bookout and Schwarz v. Toyota emanated from a September 2007 UA event that caused a fatal crash. Jean Bookout and her friend and passenger Barbara Schwarz were exiting Interstate Highway 69 in Oklahoma, when she lost throttle control of her 2005 Camry. When the service brakes would not stop her speeding sedan, she threw the parking brake, leaving a 150-foot skid mark from right rear tire, and a 25-foot skid mark from the left. The Camry, however, continued speeding down the ramp and across the road at the bottom, crashing into an embankment. Schwarz died of her injuries; Bookout spent five months recovering from head and back injuries.

&gt; Attorney Graham Esdale, of Beasley Allen, who represented the plaintiffs is the first to say that the Bookout verdict – in some measure – rested on those two black skid marks scoring the off- ramp.
&gt; 
&gt; “Toyota just couldn’t explain those away,” Esdale said. “The skid marks showed that she was braking.”

This account of the crash is a perfect description of what happens if you have the gas pedal on the floor and pull the parking brake. Never mind a moving car, many stopped cars can overpower their parking brake, it's not intended to stop a car. The notable feature though is that the parking brake generally only engages the rear brakes. 

The driver floored the gas pedal thinking it was the brake, panicked and pulled the parking brake. The parking brake locked up the rear wheels causing a loss of control while the front wheels continued to fight to keep the car moving. There is no evidence here of anything other than the driver panicking.


 A global variable is any piece of software anywhere in the system can get to that number and read it or write it.) The academic standard is zero. Toyota had more than 10,000 global variables.

“And in practice, five, ten, okay, fine. 10,000, no, we're done. It is not safe, and I don't need to see all 10,000 global variables to know that that is a problem,” Koopman testified.
How many fucking times will articles on this exact topic get posted and add absolutely nothing to the conversation? 

This is literally the same article posted 6 months ago.
Big bowl of spaghetti code? This isn't /r/leagueoflegends
"contains 10,000 global variable." made me laugh out loud.
Why in the Gods name do we need software controlled cars when we managed to do just fine without it for last 100 years? Horrible bloatware. 
All variables are global if you're clever.
lesson: if you use closed source softwares, you will die. 

Now go listen to some RMS speech.
[deleted]
Riot wishes League's spaghetti code only had 10k globals.
Can the Japanese write code?
&gt; GC is a great invention, but the decision to use it for D has instantly alienated a core market

Yes, I think that's exactly right, and I commend Andrei for being so straightforward about that.

I guess there is some argument about how to define a "systems language", but I don't think the label fits well on anything with GC.

Even if you had an extremely fast, state of the art collector (which I suppose is something that can be said about Go at this point), there are many cases in which the ever-present overhead will simply be unacceptable.

And yes, Rust has a disproportionate focus on one specific point (safety), but I think that's basically the problem with all the "Big Agenda" languages, [as Jonathan Blow called them](https://www.youtube.com/watch?v=TH9VCN6UkyQ#t=3m32s).
Title doesn't do this article justice. Andrei actually did a pretty good job of trying to be un-biased here!
A better summary would be:

* D has fast compile times *and* is performant, but has poor library support for non-GC code
* Go is supported by Google and superior for networking code, but has GC and poor abstraction mechanisms
* Rust is safe and modern, but sacrifices too much in the name of safety
Time for a new systems level programming language: C--

its everything you hate about C with none of the good parts!
This person should be commended simply for criticizing his own dog food. It is quite a healthy exercise to write a paper *against* the language you are a fan of. It is a recipe for anti-confirmation-bias and anti-hubris.
Great response but failed to address what is, to me, the most important feature in a C/C++ replacement: a module ecosystem. Building and sharing libraries in C/C++ is an absolute nightmare. There are no standard build tools, no cross-platform (or even cross-distro) packaging options, no conventional directory layout, no tools for dependency management, no standard or conventional testing tools, nothing to help the life of a release manager. Mix in dynamic binaries, which are a brilliant solution to problems that no longer exist, and building a cross-platform library in C that is easy to deploy and test is a complex, time consuming task.

On this front, Rust is a revelation. Cargo makes both creating and using reusable libraries trivial, as it should be, and gives you standard project structures, basic testing tools, and more. Go does a decent job on this front, but oddly cripples `go get` by completely ignoring versioning. Producing static binaries by default makes it very easy to deploy Go code, though, which is one of the main reasons it is so popular for network services. I have no idea how D fairs on this front, but the last I checked, it still seemed oriented around running DMD yourself on individual files.

For me, ecosystem trumps language features all day every day. Any replacement for C has to have a compelling building and packaging story. So far, I feel Rust is winning in this area by a long shot and it's the main reason I am willing to endure its atrocious syntax.

I think in general the biggest mistake language designers are making is thinking all the wisdom should be in the language semantics and none in the standard libraries.  

My experience watching a big language grow (Java) from version 1 onward was that my coworkers frequently copied bad patterns from the libraries, or from our code (or my old code).  

I think this is to be expected.  When you're new to a language you try as hard as you can to emulate the code around you.  If you want smart code written in your language you better pony some up from day one.  People will do just like you do.  I hope that's a lesson Josh Bloch learned, he certainly had ample opportunities to do so (He introduced MethodNotImplementedException as an idiom to Java, and he doesn't understand generics but designed the collections API, then made an almost as weird one called Guava that requires you to memorize it or keep the documentation open all the time because the methods are never where or what you expect them to be).  For a long while people held him up as a gold standard for Java design.  If the J2EE community hadn't hit every step while falling down the 8 Fallacies of Distributed Computing staircase, I would blame him for the fall of Java.  Luckily for him someone else already beat him to it.

Your libraries matter, probably more than anything else.  Stop trying to figure out how many braces your syntax should have and whether null checking should be in the type system, and actually design some libraries.  Once you know what you want the libraries to do the priorities for language semantics will be pretty obvious.  And remember, you're going to see cut and paste of your most hideous blocks of code *everywhere*, so think twice before you tag a release.  Maybe three times.
Great read. Misleading title. But the one with rust skipped leg day is priceless.
The only language out of the three that I have spent enough time with to have an opinion is D - and that was quite a few years ago now.

D missed the mark for me because at the time it did not produce debugging information in a format that the Microsoft debugger could understand.  Or, at least, I couldn't immediately work out how to make debugging work.  The tooling was very lacking.  I'm assuming the situation has improved, but that first bad experience still sticks with me.

The first time I saw the syntax I was really impressed with how clean and simple it was, in comparison to C++.  However I get the feeling a few weirdnesses have crept in as the language has evolved.

I've gone back and forth on the advantages of garbage collection.  Having attempted to write garbage collectors, I know how hard a problem it is.  But f you're going to have a garbage collector - especially in a language billed as a replacement for C++ - the implementation absolutely *must* be top-notch.

One very large project I worked on that relied on a (quite simplistic) garbage collector ended up with pause times measured in minutes.  Garbage collection can introduce more problems than it solves.  A new systems language needs a collector that's state-of-the-art.  It's hard to trust a collector you don't have direct experience with.

In the last few years C++ has really stepped up its game.  I think it's much harder for D to compete against today's crop of standards-compliant C++11 compilers than it was a decade or so ago.
Great article. 
I would like to add my humble opinion: The 10x advantage of C++ is ressource management. If you ask yourself why C++ was even able to replace C in many areas in the first place, I swear it is due to destructors. Destructors are the bee's knees.

I mean, I love my C# as much as the next guy, but only as long as I don't have to manage any ressources besides memory. Ok, the GC works fine for memory. But if I have a class whose objects need a ressource other than memory, I need to implement IDisposable. That's no big deal, but then I need to think about the using statement every single time I use it! And every other class that has a reference to the class with the ressource needs to implement IDisposable! And if I use that other class, I need to use "using" and so on!
And in Java it's the same thing (no clue about D or Go).

On the other hand, in C++, when an object has a ressource, it takes care to dispose of it in the destructor, and we're done. Object out of scope, ressource freed. Simple as that. You care about the ressource once and for all, and then you never think about it again. And if you write modern C++ code, when was the last time you really had to write "delete"? I need it very, very rarely. So basically the only thing GC gives me over idiomatic C++ is that there can't usually be dangling pointers, which aren't such a big deal really.

Personally, what I'd really want is C# with destructors.
I really don't think Go is well poised to replace C (or C++) as a systems language because Go is a terrible systems language.  It is a great networking language, but I can't imagine anyone ever wanting to write an OS (for mass consumption that is) or a program for a 4-bit microcontroller using Go.  D and Rust however both have promise here.
I'm very glad he has a realistic and self critical view of D, and I agree with everything said about Rust and Go, Go is a niche language, Rust takes explicit safety to the point where it's not worth for a large majority of community (number of bugs it prevents not being worth the extra effort to formalize everything, and it goes beyond memory safety - eg. explicit error handling + std lib that enumerates every possible edge case = a bunch of edge cases you don't care about but have to deal with or dismiss explicitly).

In my not so humble opinion D had a great opportunity a few years back right around the time Xamarin came out - if they ditched maintaining their random backed and focused on LLVM + made sure the runtime worked on mobile they could have been the go-to mobile development client language.

They compile to native code which means you don't have to pack CLR along with your app, there is no runtime reflection to begin with so Apple JIT restrictions mean nothing, they have excellent meta programming capabilities which means generating bindings for native APIs could be done cleanly with metaprogramming facilities.

The language is better suited than C# for performance sensitive work and native interop, with meta programming and static compilation it's much easier to get predictable performance out of it. On mobile with weak CPU cores this matters even more and the weak GC wouldn't be as detrimental as it is in real-time use cases.

If the compiler team provided working language tools to interact with those platforms (eg. dalvik/Objective-C bridges) community would easily pick up the API integration part and D could have found it's niche. Right now C++ will get modules and then there will really be no point to D.
But D does have a vision...to consolidate languages into one. The language you use for simplicity on the front end is the same powerful language you use for the back end. At least that is what was conveyed to me before I started working in that language
Small correction:

&gt; It could be claimed that literally no other language can reach that level of integration.

Free Pascal does. Like in D, you need to declare a Free Pascal-ized version of the method to use, but after that you can use the function right away as if it was a Free Pascal function. There is also a tool included that can convert header files to Free Pascal units that simply import the functions.

This also works with Objective C and Free Pascal's "Objective Pascal" subdialect, allowing it to use Objective C APIs (like Cocoa, which was why the subdialect was made) and i think there is some limited C++ support in development.

I also think this is the same with Vala, if you consider VAPI files part of the language. Probably C# too if you stretch things a bit.
That's probably the worst characterization of Andrei's well thought out post. You can do better.
I'm actually not familiar with Rust, Go, or D.  I mostly work in Java (w/ Scala) and JS.

I just think it's great that someone is that impartial about their own language and other languages.  On the surface it seems pretty fair.  

One of the things I dislike about some of the frontend tools I use is that there's always this bias I've noticed that X tool is THE BEST for y reasons.  Very flavor of the month-y.  I wish we had this kind of impartiality prevalent in other areas of programming.  
Andrei's summary is excellent and honest....but at the end of the day, you need to pick something. Every tool has weaknesses. You'll hear unqualified praise only for tools no one is using. Once the rubber hits the road, people will gripe.

All of these tools are good in their own way and miles ahead of some of the junk they seek to replace.
I hate to say it because it's a bit crufty, but none of these languages seem "enough better" than C++11.

Go is possibly very compelling for tasks that are verbose but algorithmically pedestrian. I think that's why it's catching on for web-kerchunking and sysadmin tasks. It's got some advantages over slower dynamic languages there and definitely over cruftwads like Bash. But if (err) if (err) if (err) if (err) if (err) if (err)  if (err)  if (err)  if (err)  if (err)  if (err)  if (err)  if (err)  if (err)  if (err) (gouges eyes out)
"So managers and engineers are nervous about adopting a language that's been around unsuccessfully for so long".

Sounds suspiciously like Erlang...
Interesting comparison, but I actually prefer Rust's syntax over Go and D. (it seems to divide people).

context: I'm coming from C++. 
IMO Rust does the best job of cleaning it up,adding fresh new ideas whilst still being familiar enough. 
 
 - Rust sticks to &lt;T&gt; for template arguments; the messy ``foo::&lt;T&gt;`` used to disambiguate them in expressions is rare because of how good the type-inference is. D's solution foo!(T) is harder to adapt to coming from C++, and Rust keeps that free for macros. 

 - the reversal of 'type argname'  to 'argname type' in Go is much harder for a C++ trained brain to read without the ':' that Rust uses between them. on the other hand, reversing the type itself does make things cleaner. (let foo=...; let foo:T = ...   vs using 'auto', again for really good type inference for most local types.)

 - 'everything is an expression' is a really elegant feature, generalizing 'if{}else{}' for C's ?:, and is amazing with 'match'. This is one of my favourite parts of it. Significant semicolons are a tweak that contribute to this ```val=x&gt;lo?x:lo```; becomes ```val=if x&gt;lo{x}else{lo}```. In turn it helps you write a lot of safe,easily comprehended code where values are always created by assignment, avoiding more error prone spaghetti logic mutation. Its' convenient to write little subexpressions that return a value, and your eye can see quickly that the 'return' isn't affecting program flow (i.e exiting the function). Cleans out the 'const' annotations with fewer 'mut's , without ignoring it altogether like Go.
 
 - immutable by default is the correct default for modern code, e.g. for parallelism. Many functions just take inputs and return an output. Arguments that are actually modified are less frequent. 
Never skip leg day!
There are a lot of comments here along the lines of "if you can afford to use the GC, then D is a choice". As stated in the linked Quora answer, using D without the GC isn't the same experience. But it doesn't mean it can't be done, nor does it mean that the GC is pervasive. Idiomatic D programs allocate most of their memory on the stack, just like Rust and C++.
&gt; [D is] 10x better than any other system language at generic and generative programming. [...] at this game, **Go is so out of depth it doesn't even get the point**

This.
The problem I always had with D was I could never quite figure out how I'm supposed to use it. No particular idiomatic style seems to implied by its feature set.
Quite honest - even though I don't agree on all points, you have to take Andrei credit.

I hope he succeeds with D (I myself gave in to C - it's not that I love it but there just isn't any way around it when complementing a scripting language such as ruby; or python; Java has no appeal to me, I'll rather follow the footsteps of linus, matz, larry or guido then go the java or C++ way though I have less problems with C++ than with java).
It is delightful to see something so sober from someone with such a deep vested interest in one of the compared languages.
It's unfortunate that Nim was not mentioned in this discussion.  Familiar python-inspired syntax, compiles to C, realtime gc, static types, cross-platform, it's my favorite system-programming language by far.
I think the problem is that there's really two different areas:

C - System programming and Device Drivers
C++ - Lots of core systems that are somewhere between the "Service" and the "System", plus Applications that require eeking out the last bit of performance.

All three choices have the capability of replacing C++, though I'm unsure they could fully supplant it.  I suspect instead of 1 choice we might eventually have two choices.  (Currently, however, I have limited choices of Rust/Go/D programmers, so it's a non-starter.)

IMO, none of them have any chance in hell of replacing C as the system programming language of choice.  The main reason is that system programmers simply are not looking for a new language.
&gt; Go ... niche of networking services. This has been a brilliant marketing move, which capitalized on the strengths of the Go team (some of the best engineers in the world at networking services). That market is hot, and Go was a fresh breath of air in a world dominated by Java EE bureaucracy and slow scripting languages. Now Go is a major player in that area, and it would be difficult to displace it.

It's not the only way a language becomes popular, but I'm partial to the idea that languages are carried by platforms that enable users to do something new/better. Unix carried C; the web carried JS.

Microservices seem to me to be the next big platform. While one of its advantages is hetereogeneity, a language that fits the space better (i.e. helps developers deliver better outcomes faster, cheaper, more correctly, more flexibly etc etc etc) will likely do better and quickly dominate the space.

OTOH... it also could be Java, which was designed for "the network is the computer". Though its enterprisy libraries and frameworks may prevent that, as noted in the article.
Good read.  
Wish he had come with some predictions in the conclusion for who will be the champion in the future. "It depends" is playing it safe
So one major problem of D is the lackluster support of alternative memory management  systems to GC. Well, they should tackle that.

Me, I'm not looking for the next enterprise level language. I'm looking for a good, fast, compiled language (creating real binaries) for *personal* projects.

In such a case, it doesn't matter if adoptation rate has been slow. It is only the *current* status that matters.

And if personal adoptation rises, the enterprise may eventually follow.
Need to discuss Nim as well
Nim? 
I keep wondering whether or not Go is really misunderstood.

One of the things I read by the Google folks about Go was about why it as created. A key problem they were facing with a ton of c/c++ code was insanely long build times. The build times where a direct result of c/c++ header inclusion and its overhead. What!?!?!... well in really big projects some headers get included literally 1000's of times. The root of all this header overload was pinned on c/c++ not have any built in dependency management (.h's are just dependencies).

The other, unstated problem, is that Google is a maintenance shop consisting of lots of maintainers. Relatively little brand new applications are written. Most tasks are maintenance, from small bug fixes to large reworks. 

One strategy to handle maintenance of this scale is to keep the code simple and obvious. Linus Torvalds had a rant about this in regards c++ (and mostly what Linus does is _look_ at other peoples code... doesn't write much... so being able to look at one file for one change is a big time saver for _him_). While complex hierarchies, generics, etc etc etc are great when writing a new application they are not so great at 4am trying to diagnose why such and such a service crashes when this is the first time you've even heard of the service.

So... long story short my impression of Go is that it was created to solve some very specific problems Google has/had. Its wider adoption seems a bit cargo-cultish to me because very few organizations have the same set of problems Google has/had at the same scale. In other words, a small development shop of a few dozen developers writing a new SaaS application really ought not be using Go.
[deleted]
I do. You helped connect a lot of pieces I had learned about. I'll look more into it. Thank you for that.
The world would be a better place if D and Rust became standard.
I haven't looked at these languages much but I don't know what more they could offer over C++11 (and the later 14 and 17) that would make me switch. In my opinion if you're using C++ it's because you want predictable memory management, which seems like only Rust is the other real contender. If Rust's main benefit is memory management, good modern C++ code has basically solved this problem.
I thought the title was catchy and fun.
So back to erlang or something?
&gt; D's static introspection, compile-time evaluation, and mixin-driven code generation combine to a powerful cocktail that is very difficult to mix right in other languages, be they new or extant;

Is Andrei aware of Nim?

- compile time evaluation - http://nim-lang.org/docs/manual.html#statements-and-expressions-static-statement-expression
- grown-up macro system - http://nim-lang.org/docs/manual.html#templates and http://nim-lang.org/docs/manual.html#macros
- modest introspection capacities - http://nim-lang.org/docs/system.html#compiles,expr , http://nim-lang.org/docs/system.html#declared,expr , etc.
Seems like everyone wants to hate on you for your title.  I just wanted to let you know I thought it was a perfectly acceptable summary for the article.  The perceptions/prejudices of the readers are your responsibility.
Great article that begs the question, what is holding back the adoption of D?
I really hope that someone mentioned the stupid names for these languages.  "Go" is the worst. You might as well call the language "The".  Moronic!  

D and Rust are not much better.
I'd like to promote my own project, [Mona](http://jancorazza.com/tag/mona/), that aims to bring as many functional elements to systems programming as possible.
Why do people worry about GC so much? It makes things way more smooth. I am not saying that you can write a kernel in a such language but it is nice and convenient nonetheless. Pretty useful for most of the usage cases. And I am saying that as someone who likes and appreciates C.
/u/andralex: I admittedly haven't been keeping up on this, but I thought Clang had managed to get a working modules implementation for C++, moving compile times from impossible to merely difficult to improve?
No mention of Julia? Interesting.
Excellent post by a co-author of the language itself.

D is not going to be adopted by the mainstream specifically because of things like sub-par garbage collection.  Yes, you can work around that, but why would a C/C++ shop with people very knowledgeable in those languages want to switch?

And why would you pick D over those languages, when you have so many programmers already familiar with the intricacies of them?

I just don't see D gaining much ground in this department which means it's going to end up as "just another language".
Swift looks very promising. It has generics, and is functional, all while having objects and some really nice inheritance paradigms, such as "this function takes any object that conforms to this protocol and this protocol". 
The funniest thing is that, in the language wars... c++ was always attacked... Now, some other languages that do similar things differently get mercilessly attacked with clickbait titles and "reading the article? what's that?" comments...

The more they get used, the more new people will be exposed to them and more people will criticize them.  That's a good thing.  It will mean many others are using those languages successfully.

I feel like nothing will replace C as the end-all-be-all language. Things will replace it in certain domains, but I, for one, like Go because it focused on its niche. If I wanted to do something that wasn't networking, I'd probably look to another language, and I think that's fine. Different tools for different jobs. No need for one language to try and be the one-and-only solution for every domain, IMHO.
I think I don't really care about a language vision. What I care is what it makes possible to do, and whether the small stuff is well thought.
&gt;C++17 is hopelessly lost in the woods

what does this mean? C++17 will get "concepts" which is pretty good.
Wanting to be the replacement for C is a fool's errand. I thought the whole reason for D was to be a replacement for *C++*
&gt; C++17 is hopelessly lost in the woods

What's with all the C++17 hate? What is he referring to exactly?
I would've liked to see his take on C++, and specifically C++17. With hopefully the advent of a module system and further expansion of standard libraries, parallellization, networking, template concepts, ... C++ should make another leap forward, comparable to the leap it already made with 11/14.
I read the whole thing. very horrible title OP !
All this, when NIM is clearly the language to replace C. 
&gt; GC is a great invention, but the decision to use it for D has instantly alienated a core market - existing C and C++ programmers. 

I really don't get this criticism.

First off, GC should be something C/C++ programmers would jump at. Memory management is a chore and there's absolutely no plus in doing it by hand. Your program with hand made free/mallocs isn't in any way faster or better than a garbage collected program. Period. So saying "my program doesn't use gc" doesn't make it look better as saying "my program is written in assembly" would imply.

C even has the Boehm GC. Is there even anything like it for C++? C++ without GC is a major pain, I hope tonever have to work with C++ in my life again.

&gt; For those, the party line has historically been "Don't want GC? No problem! You can use D with RAII or manual management style as well!" Though true, that's next to useless because there's little support in the standard library for alternative memory management styles

Second, that's a pretty weak argument. It holds for the subset of programs that absolutely can't afford a garbage collector: embedded microcontrollers with less than 1 MB of RAM, but then again I don't see those as D's target market. 

It can also be problematic for programs with real time restrictions. Granted. But, *those programs will avoid memory management completely in their time-critical portions because free/malloc/new/delete can take arbitrarily long to run*. So it's a non-argument. 

As for the lack of support for not using the GC in the standard library, it doesn't matter. You can disable the GC and forget about it, then when you're program has major memory deallocations (game finished level, web server finished request...) stick a manual collection cycle in the same place you have a bunch of free/deletes. Problem solved.

So, really, how is the GC *ever* a disadvantage?


Still programming in C for performance critical work.  Also "performant" is a noun.  You shouldn't adjectivy nouns.
Haskell is the future
Node.js ftw here as well.

- 10x async io.
- 10x throughput.
- 10x cool angular.js.
- 10x talent straight out of General Assembly.
- 10x CSS animations.
It doesn't look like Andrei understands Rust at all.
&gt; C Replacement

Excuse me for a moment while I go laugh and cry simultaneously at both the absurdity of this statement and the seriousness with which it was delivered.
Ran [the search](https://www.google.com/search?client=safari&amp;rls=en&amp;q=site:chrome.google.com+%22In+order+to+continuously+improve+and+maintain+this+software+we+work+with%22).

- Hangman
- Moon Phases
- My Little Pony Gallery

WTF?? Someone needs to crawl the app store and weed all this crap out.
The whole web ecosystem seems so cancerous at this point that I would not be surprised if they'd discover an actual, living tumor in one of these popular extensions.

For fuck's sake people, this is not what the WWW was supposed to be about.
&gt; Free Smileys &amp; Emoticons (&gt;784 000 users)

Ah yes, free smileys, the most trustworthy thing ever. 
One of my biggest regrets as a developer was releasing a Chrome Extension that I wrote in one weekend (New Tab Redirect). It's been a fun side project to tinker with things like structuring an app using vanilla JavaScript, converting to Angular, and using Chrome's storage APIs. But, I get 5 to 10 requests a week from these companies that just want to "track a little data", which I'm against. At one time when I had close to a million users, one person offered me $50,000 to buy the extension (which is MIT licensed). I'll never sell it or accept more than a small pull request because of this.

I say it was a mistake because of all the annoyances from data collecting companies. But then, I can't make changes without 100+ users personally attacking me and threatening litigation when I honestly couldn't give less of a shit about cat photos and porn or whatever else anybody does. I do get the occasional positive email, which is nice.

If you're thinking about writing a Chrome extension, don't.

But extensions are the least of your worries. If you have Chrome or an Android phone, Google periodically "checks" to see if you're saying "OK Google". If you're not, much like their while WiFi capturing snafu, they're capturing ambient room noise and storing it for analysis and ad suggestions (http://www.google.com/patents/US8138930). I discovered this after discussing a model abstraction at work for about 45 minutes in which the term "slot" came up about 50 times. I had never typed "slot" into Google, which I later verified after some experimentation. Later that day, Google Store displayed game suggestions for slot games. Curious, I found that in a 24 hour period in which I didn't use my phone audio or microphone, Google App had activated the microphone 1185 times.

If you're really worried about security, don't use Chrome.

edit: fix patent link
And there are people that will defend their actions to the death, shouting "You consented when you accepted the EULA".

As if that makes it anything other than pathetic.
Is there any browser or extension that contains a session and its cookies to one tab? I suppose I could open an incognito window in chrome, but I think multiple incognito windows still share the same session (correct me if I'm wrong). That would be one good way to fight this.
As the article notes, popular extensions for Firefox suffer from the same problem.

I’m all for addressing the problem in multiple ways, but the headline is a bit one-sided in that it only mentions Chrome Extensions. Maybe rephrase to “popular browser extensions”?
Anything we can do to limit their access to our data? 
Not that this is any defense against tracking, but I try to only use extensions that publish source code. I'd love an effort to do community-based code reviews of open-source code, specifically to look for hidden things like this. 
Name and Shame.
&gt; Try yourself, just by googling: site:chrome.google.com “In order to continuously improve and maintain this software we work with” will show some of the extensions using one of the tracking providers out there.

But then your extensions will let them know you're onto them!
Is UBLOCK origin ok? 
Looking at this page  
https://www.fairsharelabs.com/monetization/insights  
They pay extension owners to share their data with them, for whatever purposes they desire.

I had the emoji input extension, no more :P
Extensions should have opt in access rights.  

Here's a good thread showing some pitfalls, extension developer thinks leaking tracking data over SSL makes it any better

https://github.com/barbushin/javascript-errors-notifier/issues/28

Why should a JavaScript error notifier need network access.... Time for a better sandbox
&gt;This extension asks for permissions, being able to access web pages you are currently visiting, sometimes restricted to certain web pages. More often than not, these permissions are set to &lt;all_urls&gt; which means that they work on every web page.

This "all urls" permission is always a hint to better check the extension source code before installing it, if you're unsure.
awwww crap... I installed one of these for a grandparent once on my computer... 

That will teach me for installing one Chrome Extension once...
I came for the Chrome bug.  I left with the knowledge that you have to use common sense when you agree to extension permissions.  I am disappoint at click bait.
You know what. Here's a problem. 

GMail introduced native notifications, and I was like, great, I don't have to use a third party extension anymore. 

But they still don't let you configure shit. Least I want is for an email to open in a tab instead of a motherfucking popup. It's not IE6, cabron.
About a year ago I remember a vocal minority raising red flags about Ghostery's policies and the likelihood that they were selling your information. At that point I removed all extensions, with the exceptions of a few from Google themselves (lets face it they already have my data, I just feel best knowing my data is shared with fewer entities and particularly best about those entities being large enough to draw scrutiny if they make for an unseemly usage of that data) and AdBlock.

I no longer even feel confident in what access AdBlock has to my information and what it may be using it for, and as I find myself able to white list more and more domains I wonder about its practicality in terms of benefit vs potential risks.
For a security group, they sure have done well with their unverified/unsigned, unencrypted website.
It's all about getting marketing data to sell to companies. Companies want to know who out there will buy their products, or what products to make, or who to target with marketing. Because nobody wants to take financial risks. 
Together with a friend of mine I build a chrome extension and a server component which acts as a Command &amp; Control server for the extension, allowing to inject arbitrary Javascript into every tab users with the extension have opened via WebSockets. The extension is installed in less than a minute if someone has left their laptop unlocked and uses "Google Docs Offline" as a name with the icon that goes with it typically. It is not easily recognizable as an extension which was not installed via the store. 

Makes for great pranks but is also kind of scary. We may or may not open-source this thing at some point to raise awareness on how broken the extension system is.
Sad result of using proprietary software - you should not use it unless it's safety is guaranteed by trusted third party.
It's all advertising dollars.  Gotta make that ch-change no matter the topic at hand:  Jared's molestation sandwich or Paris's suicidal sycophants.  What you consume sets the price of everything.

http://www.auditbureau.org/sitemap.html

They just want the master key to all your info.  We're just data in the stream now.  
What is this web site?

&gt;Detectify is a SaaS based website security scanner that will help you stay safe. **We audit your site’s security so you can focus on web development.**

Oh, great! Free security scans by this well known organization!!

[Pricing](https://detectify.com/pricing) 

Wait. What? You mean you have something to sell?!
When I first started using lastpass.com, I installed their chrome extension. Then I actually read what permissions it required and realized it was doing the exact opposite of what lastpass.com claims is their core function, namely keeping your passwords safe.  
Well, all of my extensions are FOSS. They seem to be fine :)
It's pretty damn easy for extension developers to track you without you even noticing. You know that "Do you allow this site to track your location" notification? That can easily be circumvented and set as an "extension permission" that your average user doesn't read. I did [a proof-of-concept extension](https://github.com/crisbeto/evil-hodor-tracker), for a school project, that tracks your location, keypresses, cookies etc. and sends them back to a server. Didn't take more than basic JavaScript knowledge and reading through the Chrome docs.
How do I check this in Firefox?
The problem is that Firefox is going to implement Chrome Extension API. I guess Mozilla is trying to import malicious extensions from Google's store.
I had to stop using the imgur extension because it updated to want to access and change all data on all sites you visit.
what the fuck
Download a shitty extension get a shitty tracking experience?  The list of example extensions in the article  triggered my "spammy-sense" based on their name alone. 
Anyone knows if request policy addon ( https://addons.mozilla.org/en-us/firefox/addon/requestpolicy/ ) can help with these?
And this is why you should never install any extension that asks for "All data on all websites", because that's exactly what it means.
And yet chrome will still block my homemade extension to help at work for not being safe just because it's not in their store
Pentadactyl and Firefox will be my drivers until death parts us.

I don't care how laggy and incompatible it becomes over time. I am not using the competition. I'd rather use Seamonkey and co over them.
Damn. I use (as of now, use to use) ANT video downloader on Firefox. buh-bye!

Interesting read, will pass it on.
Fuck the web and its poor attempt at a VM.

Fuck the unholy html/css/javascript trinity.

Fuck ad-based everything.
To find an open-source alternative, I went looking. Didn't have to look very long, found [Midori](http://midori-browser.org/). Thoughts?
Pardon me if it's already been posted but...

Follow the money.
I have been waiting for this.  Browser extensions are a petri dish for malware.
Sooooooo, is RES safe?
People are surprised by this?
[Google dropping "Don't Be Evil" ](https://en.wikipedia.org/wiki/Don%27t_be_evil) should've been a tip-off.
Well, chrome users don't really care about privacy.
Condoleeza Rice is an executive with Dropbox. 

So there's that. 
I still use Firefox (aka Netscape)
Google spying on everyone? Hold the press we have a scoop! /s
Ahaha... It was obvious that Chrome was going to be a joke a very long time ago.  I've used Chrome for less than a year when it first came out, then I realized Google will never resist the temptation to install data leaks into Chrome for profit reasons, and I switched to Firefox.  Firefox is awesome in almost every way, although they're trying to put commercial crap in Firefox too now, but I still trust it more.
OP, you fucking Karma Whore.
No, they are not.  Google manually reviews every single extension that requests the &lt;all_urls&gt; permission. 
It would be nice to see a sentence or two about binary, since you need to know it's in binary to understand why the example operation isn't exact. In a decimal floating point system the example operation would not have any rounding. It should also be noted that the difference in output between languages lies in how they choose to truncate the printout, not in the accuracy of the calculation. Also, it would be nice to see C among the examples.
highly recommend this Computerphile video about floating points - https://www.youtube.com/watch?v=PZRI1IfStY0
I'd recommend the venerable [What Every Computer Scientist Should Know About Floating-Point Arithmetic](https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html) over this one.
Decimal sucks, hexadecimal floats for the win.

    #include &lt;stdio.h&gt;
    int main(void) { printf("%a\n", 0.1 + 0.2); }

    gcc -std=c99 .1+.2.c &amp;&amp; ./a.out
    0x1.3333333333334p-2


Kind of a CS 101 thing ...
Go is a wierd one. Float32 string is 0.3 float64 is 3.00...04. Not sure exactly why though...
&gt; perl -e 'printf q{%.17f}, 0.1+0.2'

They make a note about the PHP precision, but don't mention that they are setting the precision in perl.

    perl -e 'printf 0.1+0.2'

That just outputs 0.3
How do programmers of financial software deal with floating point imprecision? I know the roundoff error is many places below the value of a penny, but it can still change something like 3.30 to 3.2999999..., which ought to send auditors into convulsions. Do they just work in pennies and convert on display?
&gt; Objective-C

*chortle*
Java to the rescue:

    import java.math.BigDecimal;
    
    class FunWithFloats
    {
        public static void main(String[] args)
        {
            BigDecimal a = new BigDecimal(0.1);
            BigDecimal b = new BigDecimal(0.2);
            BigDecimal c = new BigDecimal(0.1 + 0.2);
            BigDecimal d = new BigDecimal(0.3);
            System.out.println(a);
            System.out.println(b);
            System.out.println(c);
            System.out.println(d);
        }
    }

Output:

    0.1000000000000000055511151231257827021181583404541015625
    0.200000000000000011102230246251565404236316680908203125
    0.3000000000000000444089209850062616169452667236328125
    0.299999999999999988897769753748434595763683319091796875

Now you know.
Isn't the compiler do it and not the language itself? doesn't languages like C++ have more then 1 compiler?
Appendix D
What Every Computer Scientist Should Know About Floating-Point Arithmetic

https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html
&gt; GHC (Haskell)	0.1 + 0.2	0.30000000000000004

&gt; Haskell supports rational numbers. To get the math right, (1 % 10) + (2 % 10) returns 3 % 10.

&gt; Hugs (Haskell)	0.1 + 0.2	0.3

This is weird. Doesn't Haskell define what `x.y` means, if it is a float or a decimal?
Why does Ruby list alternative methods and nothing else does?  PHP's BC Math Functions (http://php.net/manual/en/ref.bc.php) resolve this as well...
Why was C++ output 0.3? Does cout do truncation as well?
Seems wildly unfair to only show Java as being "wrong", while showing Scala as being correct if you use `new BigDecimal("0.1")`, seeing as BigDecimal comes *straight from Java*, and isn't actually a Scala thing at all. 

EDIT: Also, while it may be a *bit* more understandable, it seems a little strange to say that this is all caused by "computers can only store integers", because that's just as untrue as "computers can store decimal numbers"...computers can only store binary, it's just that integers are a *lot* easier to represent precisely in binary than a decimal number is. 
dude is serious about evangelizing the issue

http://0.30000000000000004.com/

nice url!

ELI5?
&gt; Computers can only natively store integers

Computers can only *natively* store 0 and 1. You can choose to interpret sets [edit: strings, not sets] of 1s and 0s as as digits of an integer, or floating point number, or whatever. The fact that the integer interpretation is by far the most common doesn't make it more "native". It's the operations performed on the data, not the data that's stored, that determine its interpretation.
Lol objective C.
And that's why you never do == with floats, but you check for a range with an epsilon. Fixed point math is also interesting to look into if you need predictable precision.
    C#	Console.WriteLine(.1 + .2);	0.3  
wouldn't these literals be doubles instead of floats?  
It should be like this:  

    Console.WriteLine(.1f + .2f);
So if it's an integer vs float issue, why not multiply by tens or thousands or whatever then shift the decimal back?
 
Are there cases where you can't do that?
I wasn't sure myself if printf("%f", .1+.2); in C was doing any rounding so I wrote a little test on x86_64 cpu

    #include&lt;stdio.h&gt;
    main() {
        float a = .1;
        float b = .2;
        float c = .3;
        if (a+b == c) {
    	printf("equal\n");
        } else {
    	printf("not equal\n");
        }
        return 0;
    }

Prints "equal"


EDIT:

According to this:
https://github.com/erikwiffin/0.30000000000000004/commit/44f7a7e0b9c73eef5b1198b39bc10f5cfea46e3e

printf("%f") is rounding, and the result should be 0.30...04... but then why does my above code return "equal" instead of "not equal"?

this is fun btw, good collaboration and learning going on

EDIT AGAIN:

After corresponding with another contributor on GitHub, cowens, and doing some studying I have learned a few things. Here's what I can report.

* the behavior of the code is highly dependent on the architecture and the compiler (of course most of us familiar with C know this)
* IEEE 754 compliant behavior would be to print "not equal\n"
* As a general rule you should never compare floating points for equality

Here is code provided to me by cowens showing the right way:

    #include &lt;stdio.h&gt;
    #include &lt;float.h&gt;

    int main(int argc, char** argv) {
            float a      = .1;
            float b      = .2;
            float n      = a + b;
            float target = .3;
    
            if (n &gt; target - FLT_EPSILON &amp;&amp; n &lt; target + FLT_EPSILON) {
                    printf("%.32f is close enough to %.32f\n", n, target);
            } else {
                    printf("%.32f is not close enough to %.32f\n", n, target);
            }
    
            return 0;
    }


I don't know why computing 0.3 is so special: just the literal 0.1 would demonstrate the same thing.


What's up with C#? Surprised it yields different result than Java
If a language uses floating point math by default, but gives you an answer that is clearly not a result of floating point math, then there's something wrong with the language. because it means inconsistency, and inconsistency leads to the dark side


Good lesson.

Rust uses 64-bit floats by default, so:

    let a = 0.1;
    let b = 0.2;
    let c = a + b;
    println!("c = {}", c);
    println!("c == 0.3 : {}", c == 0.3);

produces:

    c = 0.30000000000000004
    c == 0.3 : false

Using 32-bits explicitly works:

    let t = 0.1f32;
    let u = 0.2f32;
    let v = t + u;
    println!("v = {}", v);
    println!("v == 0.3 : {}", v == 0.3f32);
    
... 

    v = 0.3
    v == 0.3 : true
Woo! Hooray for OCaml for having that extra digit! Obviously it is the best since it computed the highest value! :D
http://dec64.com/
Very interesting! Something similar happens with e^(pi)-pi. It's a standard test for calculation accuracy.
Why does the C++ example show the result with the precision set to 17, but the PHP one doesn't? (and tells you that you have the ability to do so) It strikes me as inconsistent.
Obligatory post:

&gt; https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html
What is D doing ? 
So how do I know I'm safe? Is this a problem with a complete solution that's only a matter of implementing it? Or each language is making an *educated guess* on my behalf based on some self-proclaimed best solution, and if it happens to be right, we all cheer, otherwise the market burns?
I don't know what the table is supposed to mean, bc and dc do exact arithmetic.
C# is partially incorrect. While default floating-point math would indeed create such an error, you can specify qualifiers that would treat 0.1, 0.2 and 0.3 as *decimal* type numbers - and those are going to be added correctly.

0.1m + 0.2m == 0.3 m
Why adjust the precision for just C++ and Perl?
That's why I prefer decimal floating-point.
Interesting, thanks!
Shouldn't most of these be using CPU float add instructions anyway, with the differences mainly being due to formatting?
huh I'm teaching undergrads in Germany and we had this exact same problem on our assignment last week, you're not by chance studying here?
There's a library to fix this. https://github.com/sakabako/fix-a-float
Well at least JavaScript and C# have it closer than Objective-C does!
wouldn't 1/8 be 12.5 and NOT an even division?
reminds me of

http://www.sscc.edu/home/jdavidso/music/musicnotes/Piano.pdf

and 

http://blogs.scientificamerican.com/roots-of-unity/2014/11/30/the-saddest-thing-i-know-about-the-integers/      

and

https://www.youtube.com/watch?v=1Hqm0dYKUx4                                                                     


Ancient abaci have had a floating point for thousands of years. You just float the point.

The reason we don't is a daft hack to include scientific notation. Really important, because we want to work with huge numbers, but it leaves us with stuff like that.

I hate working with fractions. So much that I once wrote a point floating adder thing that worked with characters in a string. I didn't know what a register was or how an adder worked at the time, so weird things like 0.1 + 0.2 really pissed me off (as did the thing that used to happen on my computer which meant that if you added two massive numbers together you got a seemingly unrelated massive negative number). 

If you're ever looking through code and see `int hundredthOfaPenny;`or `double infiniteAbacus(string n1, string n2) {` it was probably me.
|0.0|0.1|0.2|0.3|0.4|0.5|0.6|0.7|0.8|0.9|
|---|---|---|---|---|---|---|---|---|---|
|0.1|0.2|0.30000000000000004|0.4|0.5|0.6|0.7|0.7999999999999999|0.9|1|
|0.2|0.30000000000000004|0.4|0.5|0.6000000000000001|0.7|0.8|0.8999999999999999|1|1.1|
|0.3|0.4|0.5|0.6|0.7|0.8|0.8999999999999999|1|1.1|1.2|
|0.4|0.5|0.6000000000000001|0.7|0.8|0.9|1|1.1|1.2000000000000002|1.3|
|0.5|0.6|0.7|0.8|0.9|1|1.1|1.2|1.3|1.4|
|0.6|0.7|0.8|0.8999999999999999|1|1.1|1.2|1.2999999999999998|1.4|1.5|
|0.7|0.7999999999999999|0.8999999999999999|1|1.1|1.2|1.2999999999999998|1.4|1.5|1.6|
|0.8|0.9|1|1.1|1.2000000000000002|1.3|1.4|1.5|1.6|1.7000000000000002|
|0.9|1|1.1|1.2|1.3|1.4|1.5|1.6|1.7000000000000002|1.8|

It happens less often than I would have expected.
TIL power shell can do math at prompt. As a Linux guy I've  been getting more and more impressed by it.
You'd think that after all these years we could do better... Well apparently not. 
It would be nice if it had a short tutorial on floating point math along with how common operations are done.
Understanding the limitations of floating point is a fundamental part of learning programming

The site seems to be more about examples of formatted output

Some numbers can never be expressed exactly in any number system

There are rules that make the problem manageable
The trick is to avoid touching the floating point storage in the first place.

For example, in Squeak Smalltalk:

0.1s1 + 0.2s1asScaledDecimal:100 

produces an output of:

0.3000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000s100
Why don't we use aEb notation? ax10^b
Where a is a signed int and b is a signed int (or better yet, a signed tiny). That gives us everything from +/-32767 followed or preceded by as many zeroes. 

This is not a proposal, but a question: I assume there is a reason. What is it?
&gt;     0.30000001192092896

RIP D language.
I always try to stick with fractions as much as I can.
I give it a shot:
The decimal system is based on prime numbers 2 and 5. Any combination of multiplied or divided by 2 and/or 5. 2 , 5, 2 * 5, 1/2, 1/5, 1/(2 * 5) can be represented exactly (with a finite number of digits).

Dividing by any other prime number gives a fraction with a periodic number in decimal:
Simplest example is 1/3  = 0.333333333333 (with more 3s forever). At some point you cut it off due to space limits. Once you multiply the truncated value by three again the truncation gets visible again.

Computers binary representation of numbers ONLY uses prime number 2. Any combination of multiplied or divided by 2 can be represented exactly (with a finite number of digits).

Dividing by any other prime number gives a fraction with a periodic number in binary: Simplest example is 1/3 = 0.0 100 100 100 100 {repeat the '100' binary digits } Same as above - truncation because of space limits.

However now with binary also dividing by 5 does not work any more. This also includes the desirable numbers 1/10, 1/100, 1/1000. Even representing 1 cent as $0.01 will not work well any more. 1/5 = 0.0011 0011 0011 {repeat the '0011' binary digits} At some point you cut it off due to space limits. Once you multiply the truncated value by five again the truncation gets visible again.
Honestly, by the time I got to the end of thread I forgot why I gave a shit in the first place.
If these are the numbers it can store accurately:

&gt;The prime factors of 10 are 2 and 5. So 1/2, 1/4, 1/5, 1/8, and 1/10 can all be expressed cleanly because the denominators all use prime factors of 10

Then why does this example work when using 0.1 and 0.2? Or, 1/10 and 1/5. Wat? These numbers should work according to the author. 
Why did python change? I don't see any reason not to use a shortest possible representation when this will still be converted to the binary equivalent. The whole point of a decimal representation is human readability after all.
should read:

Below are some really poor examples of sending .1 + .2 to standard output in a variety of languages.
I didn't know you could have ~~top level~~ domain names that are just numbers
&gt; Haskell supports rational numbers. To get the math right, `(1 % 10) + (2 % 10)` returns `3 % 10`. 

Ah, good ol' Haskell.
OCaml wins

FATALITY
Erm isn't this common knowledge o_0?
My head hurts
Welcome to the world of float.  Never use it unless you're doing scientific calculations.  

Especially don't use it if you're dealing with money.  If you're dealing with money, either convert to cents (or the currency equivalent) or use Binary Coded Decimals.
Alternate solution: use fixed-point.
You could store it as strings and be really inefficient but never have floating point errors.
As a php developer I'm not surprised php isn't on the list.
    function IsCloseEnough(a, b: Double): Boolean;
    begin
      Exit((a&lt;b+0.00000001) and (a&gt;b-0.00000001));
    end;
You can resize the emulator now..finally.
holy instant run batman! that is SO amazing! I bet JRebel are ... disappointed by this - they had a paid add in to do this. (Maybe google bought them!) Cant wait to try it.
The Intel HAX-based emulator was pretty good, but then again, you needed a compatible Intel CPU. Sucks for AMD users though

VMWare + Android x86 sometimes worked, and when it did, it really flew because of VMWare's x86-to-x86 emulation performance
Hopefully this reduces the annual number of Android dev related suicides.
Is ndk support still in alpha?
[Link](http://tools.android.com/download/studio/canary/latest) for those looking.
&gt; Improved Android Emulator

That's a low bar if I've ever heard one...

So... how good is it? I'm a little too busy to play with it right now. Is it better than Genymotion?
I've been using the Microsoft Visual Studio Android emulator, it's actually very very good.
A debugging environment that you can run your programs in without waiting an eternity for it to start. Looks like Android development has finally arrived in the 80s...
Does it support Bluetooth? 
&gt; Improved Android emulator

hmm how improved are we talking about? 'Cause I still have PTSD after trying out the emulator and Genymotion was godsend for me. I'm not touching the built-in emulator until it significantly surpasses Genymotion in performance. 
Actually it appears it's 141.2422023, not 2.0.

Because GOOGLE.

(Actually that download installs something with version 1.5 in the title. So is this release or isn't it?)
Canary channel.....never again.
I'm waiting for the svg to drawable convention they mentioned at the Google IO. It's such a hustle that I have to manage all resolutions. Even in iOS you can input PDFs and let them convert to images in the build process. 
When we can open multiple projects in single instance of IDE?
Yeah, but will I be able to compile "hello world" for my 4.2.2 device?

PROBABLY NOT.

CLICK HERE TO INSTALL REPOSITORY.


REPOSITORY NOT FOUND.
As usual, I have to ask: does it still take a supercomputer to run it properly? Eclipse is known for being a resource hog, but I switched back to it from Android Studio because it actually runs.
nice it's about time!
I can't even get the freaking examples to run under the hypervisor, so it's a VM.
And [here's a very fast Android Studio tutorial I made](https://www.youtube.com/watch?v=-feVvHv-vpE) in case you haven't learned how to do the basics in android studio
Now to learn how to program Android...
I find it works well.
Out of curiosity, is anyone else experiencing problems with permissions? I'm giving the app permissions for several things, and it works in genymotion and on devices, but it doesn't work on the new emulator.
Can someone explain to me what this is?
Will this work on 4K does anyone know? I know eclipse and netbeans don't for me. They look really really tiny. And when I change the resolution the scaling of all the other desktop apps goes crazy.
Wasn't this just a preview release, not final?
Does this work like Jrebel when changing your xml layout files?
I didn't even know there was a 1.0.

Not having to use Eclipse? Well that's nice if I ever try to do something on Android again.
IntelliJ is probably the best IDE and Eclipse while feature rich was absolutely awful. This is a huge improvement :D
It's still **Java** and **over-engineered** pile of **sh-t** _(the whole android ecosystem)_
Anybody know why I'm getting this empty error and can't build gradle? https://www.dropbox.com/s/d87btex1jrth5ki/Screenshot%202015-11-23%2011.24.20.png?dl=0


Edit: this is from the log: 2015-11-23 11:24:14,750 [ 344138]   WARN - radle.project.ProjectSetUpTask -  
2015-11-23 11:24:14,751 [ 344139]   INFO - radle.project.ProjectSetUpTask - /Applications/Android Studio.app/Contents/gradle/gradle-2.4/lib/plugins/gradle-diagnostics-2.4.jar (No such file or directory)

Consult IDE log for more details (Help | Show Log) 
2015-11-23 11:24:14,752 [ 344140]   INFO - ls.idea.gradle.GradleSyncState - Sync with Gradle for project 'YOLO' failed: /Applications/Android Studio.app/Contents/gradle/gradle-2.4/lib/plugins/gradle-diagnostics-2.4.jar (No such file or directory)


Should I download that .jar somewhere?

Edit 2:

It seems my folder is "gradle-2.8" and I have "gradle-diagnostics-2.8.jar"
Does it work well with Scala yet?
So if instant run requires latest gradle plugin, it will not work with experimental one?
Nox App Player is worth a try than this, but it only runs on Windows. http://en.bignox.com/
I don't get why an emulator is needed? I thought Android used Java so that it could abstract away the underlying hardware? It should be possible to run apps natively with a specialized JVM that provides a virtual Android environment, without having to emulate another whole CPU architecture? Or is that what it's actually doing?
Huh. I might actually switch from Eclipse now.

/s
But does it work with Kotlin yet?
... And it still requires a lame, half-assed 2-major-versions-behind imitation of a disgusting oracle proprietary pathetic, horrible language from the 90's, instead of something usable that doesn't make me vomit every 50 seconds.

Thanks but I'll continue to use F# on Xamarin.

Oh, btw, Xamarin's Android emulator is years old now, and much better than this useless java based crap.
Is this in IntelliJ, if so, in which version?
Couldn't care less about the emulator. Thanks for the fast instant run but the only feature that I really need is good ndk support.
I've been using Genymotion for awhile, will be nice to try this for the instant run stuff.
So basically:

"Google fixes something they should've done ages ago"

Went through hell trying to emulate my siimple vuforia app. I don't even code.
Does this mean they will accept pull requests?
Glad to see they didn't just dump it on that piece of shit opensource.apple.com website and call it a day.

I wonder if the swift-package-manager is going to obsolete cocoapods, or if it's going to do a subset of the work.  Can't wait until they open up those github repos.
&gt;After Apple unveiled the Swift programming language, it quickly became one of the fastest growing languages in history.

Citation needed.
Does this mean that the lifespan of swift can be independent of the apple ecosystem? That is my big concern and the reason I have not even tried to learn swift.

If apple drops all support for swift and tells everyone to go fuck themselves, will swift still be useful **OUTSIDE** of the apple ecosystem.
A Linux compiler too, pretty excited to test it out with some weekend projects
Website down.
Someone forgot to make their private GitHub repositories public. Oops.
Does anyone know if Swift supports compile time metaprogramming like modern C++? 
I was happy to see they even included [a file for emacs-settings when editing the code](https://github.com/apple/swift/blob/master/.dir-locals.el) and it also refers to an emacs swift-mode so I guess that is a thing as well. Nice that they are not forcing everyone into their IDE.
&gt; Once built, the best place to start is with the swift whitepaper, which gives a tour of the language (in swift/docs/_build/html/whitepaper/index.html).

Is this already out of date? The instructions, not the paper. There is no `whitepaper` in my `_build/html`.
PSA: /r/swift would love anyone to join the discussion
Congratulations!

Swift is perhaps made even bigger due to its strong ties to the LLVM toolset and because Swift has a different kind of garbage collection around automatic reference counting.

For people who are bored by big runtimes, Swift could help to show a leaner way to runtimes.

Inquiring minds want to know: will Microsoft come up with Swift++ or not? :-)
I find it interesting how similar Swift and Kotlin are (JVM lang. http://kotlinlang.org). They have very similar syntax and general look. I believe Kotlin came first, but don't quote me on that.
Let the [pull requests](https://github.com/apple/swift/pull/17) commence!
Damn, now I've got to find another high horse to criticize Apple from :) 

Not open sourcing Swift has been my main argument for months ! 
Someone can do an ELI5 on how important it is?
So for someone not as informed, what exactly does this mean for programming in swift on other OS's?

Will I be able to finally work on iOS apps using Swift while still on windows?
Are there any benefits of swift going open source for programmers who do not develop for ios?
Can we name the inevitable web framework Schwifty?
Hell yes! I've been waiting for this. And it compiles to binary as well, so no more runtime environments. Can't wait to see more libraries come out for swift . This could be big.
Now repositories are available on Github

https://github.com/apple/
What other programming language is swift most similar to?
Does this mean it's possible to develop iOS apps using Linux now?
Could someone give some insight onto how to navigate the repo? I can't find the lexer
[It's time to get schwifty](https://www.youtube.com/watch?v=4ctK1aoWuqY)
Does it mean I can use it on Windows?
I don't understand why people want to edit the small typos so bad. It's been only one day. 100+ merges for editing small typo, like missing a letter and stuffs. seem like  it's making a mess of the commit history.
My reaction to this: https://www.youtube.com/watch?v=usfiAsWR4qU
I do not follow Apple tech very closely. I get that people loved Swift because it was not Objective C but that's about it.

So serious question, outside of the Apple ecosystem, what will make Swift useful?
OK, Android, your turn! Please release a non-Java non-Go applications language for Android. =D
"OK"
https://github.com/apple/swift/blob/master/lib/Parse/Lexer.cpp#L1581

Interesting
This is good news.

Swift is surprisingly elegant, especially when you consider how ugly Objective C is;
I can't wait for the next few months and what people do with this.
So.. why Swift instead of Python, Rust, Go or even good old C++? Why should I, as a systems architect, care about swift?
As a person who has a basic knowledge of programming, and wants to learn more, should I learn Swift or continue on with Python?
Does this mean we can have a SDK for Windows? That would be pretty big for the language.
That's cool. Last I checked, Swift was only used to create Apple applications. I know it "came to Linux," but it was basically a way to develop Apple applications while being on Linux. Has that changed?
The only way to make this joke.
Can someone explain to me how programming languages can be "open source"? So what exactly can we change? 


And what does this news mean for us?

Thanks!
Speaking of swift,  but in a tangential manner, while I have a fair amount of Android dev experience, I haven't touched iOS dev in a few years. When I did play with it, it was very,  very basic work (basically just wrapping a website). 

With a project I am working on now,  I need to take a step into modern, more serious iOS development. I was wondering which of the two available languages (Swift vs. Obj-C) people would suggest I roll with and why? 
This code seems to be fairly immaculate C++11. Nice job Apple!
Gee, reading about Swift makes it sound like the best thing since C++!!  Not since PL/I has somebody managed to throw everything into a shoebox and make it fit like a hat.
Will we be able to use swift to design apps that are not for the ios
For success on the server, Swift really needs a web framework- something like what Java has. 


What Apple really needs to work on next is development tools for Swift that will work outside of OSX.  It wouldn't hurt if they funded a dependency manager and web framework for it as well.

There are lots of languages out there.  But first class editors and communities are what really get the juices going.
Everyone seems to be looking for a C++ replacement, and Swift has a lot of the things I'm looking for in my ideal 'new language':

- Compiled to native code.
- Generics.
- No garbage collection.
- Easy to write safe code.
- More consistent and cleaner than C++.

But there are some problems:

- No proper exceptions.  Lack of exceptions is unforgivable these days.
- No proper reflection.
- Having to put the type name after the variable name (yes this is petty - but what on earth is wrong with the C syntax that every modern language decides to do it the other way?).

I'm honestly tempted to go write my own language.  It would probably look a lot like C#, but without the big runtime and garbage collector.
I am not sure if it's real or a practical joke. The website is registered using GoDaddy by someone "private" and Github repositories are not public.
Wow... I just kind of assumed it was open source in the first place.
OK.
Finally somebody will fix all those embarrassing compiler segfaults, which Apple's engineers have proven to be incompetent to do.
I thought it always was.
There's something wrong with Apple these days when they license on Apache and not their own APSL.
Swift is just as open source as c# is. 
Exciting news! With the small runtime requirements I think Swift can also serve as a credible C and C++ replacement. You could write libraries in swift exposing a C interface which could be used by any other language.

Should be no need to write things like OpenSSL in C, which is a notorious source of security bugs.

Rust will probably fit in this space as well, but right now I think Swift will have a clear advantage in momentum. People will know that Swift is not going to go away. There is a huge community of developers depending on this and there is backing by the worlds largest company.
Look at the roadmap for [2.2 and 3.0](https://github.com/apple/swift-evolution/blob/master/README.md), breaking change everywhere. What the hell. The language is barely two years old and it's already breaking compatibility promise.
Relevant in one platform. 
YEAH! This is really cool. Can't wait for a Linux version!
More importantly, will my anti-virus let me into the site? Find out next time on Reddit!
[well there it is](http://i.imgur.com/RG0BS1U.gif)
So, why do I need this, again?

See, I can simply use F# or Kotlin and take advantage or years and years of platform and ecosystem, while this offers absolutely nothing?

Where's the server side web framework for Swift? Where's the full-stack application framework for Swift, similar to F#'s WebSharper?

Where's the ORMs, Auth libraries, CMSes, business rules frameworks, REST frameworks, etc. etc. etc. I could go on forever for Swift?

This sounds pretty much useless, other than to create iOS apps, which I'm not interested in, and if I were, I could simply use F# and Xamarin, and back to the 2nd paragraph.
is this as open as Facetime?
Maybe i misunderstand something, but how is it relevant for the rest of the world who doesn't develop for Macs?
Cool.  Apple have traditionally been atrocious open source contributors/citizens, nice to see this happen.  I'm still betting at some point they will hold back something critical to hamper adoption on other platforms or otherwise alienate any community that forms, as is their tradition.  

Does this also mean we have a language implemented on LLVM by The LLVM Guy?  That seems like it could be a very good resource.
I was going to learn Swift, but honestly, i hate the syntax.
Honestly, do we *really* need another new language?
I still cant get my mind around the unwrapping.

! or ? 


So now you can develop iOS on Linux?
No RPM release package makes me sad :(
Okay, so they've ported Swift to Linux and not Windows? That's a bit strange...
C++ or nothing.
My question is how can one of the lead developers have such an ugly personal website?

http://nondot.org/sabre/

edit: just because I think its ugly does not mean its not a functional or acceptable website. 
&gt; I don’t think a growing, $10M company should be **a place where people work from nine to five and then go home.** What do you advise?

For you to think about your life priorities. Not everyone is working to make you rich. They are working to make a decent wage, go out for a nice meal once in awhile and enjoy the time they have with their family. 
It get's more fun ... where I work the CEO asked us to give our 105% or whatever .... and they use stack ranking.  Stack ranking is the anti-thesis of team work because you literally have to throw all of your co-workers under the bus to avoid the dreaded bottom group.

Dear CEOs of the world, if you want your employees to work together and put in a good effort you can't pit them against each other.
**Working Hard**, an old zen tale:

&gt;A martial arts student went to his teacher and said earnestly, I am devoted to studying your martial system. How long will it take me to master it. The teachers reply was casual, Ten years. Impatiently, the student answered, But I want to master it faster than that. I will work very hard. I will practice everyday, ten or more hours a day if I have to. How long will it take then? The teacher thought for a moment, 20 years.
I don' know about other people but when I put in a productive 8-hour day, I'm exhausted in the evening. I can go in overdrive mode for a couple of days but when I cross the limits my body punishes me for it. Making me stay two more hours a day would pretty much guarantee a near-constant headache and associated productivity downfall.
There's a key part of the sucker culture here:

&gt;Stop humoring Victoria.  Victoria doesn’t work for free — every hour she puts in increases the worth of an asset of hers — so why should you?

The answer is partly because you're sold the belief that if you work like a boss, you're somehow a boss too. You're "working for the job you want, not the job you have", even though being the owner of the business isn't something you'll be no matter how hard you work as an employee. As the author says:

&gt;We wear our unpaid, uncompensated overtime as a badge of honor.  We sleep less, brag about our caffeine intake, and are available for calls and emails 18 hours per day.  We measure our importance by how many half hour slots during the day are double or triple booked, and we perversely consider it honorable to do this for free.

Because if we *were* business owners, all that *would* be good. Everyone wants to be an entrepreneur, because the media sells it as the sexy thing to be - and as with nearly everything to do with sex and status, many times more people are faking it than making it. While faking it has its costs, it does have its benefits in how we feel about ourselves - we may not all be entrepreneurs, working long hours to increase the value of our business, but we can all feel like one just by working long hours. There's a reason we're suckers.
Here's how I learned this lesson:

I had been working 60+ hour weeks for a couple years.  I wasn't working these hours to do my job, I was getting my job done during the normal hours then going and doing extra work like helping others get their work done or maintaining a build process because no one else would do it.  I was coming and taking over projects from people two levels above me (when that was their only project) because they couldn't get it done.

After about 3 years of this I got tired of it.  I was _still_ paid half what others around me were.  I was still not getting promoted. I was getting frustrated and confrontational and it was all building up.  I would go to my boss and he would shrug and basically say it's not his problem he doesn't set salaries or promote.  Finally I went to my bosses boss and basically listed out all the extra time I had put in to do work that wasn't even part of my job.  His response? "we don't expect that therefore we don't reward it".

They were happy to have me do all that work, and they would keep handing it to me.  I stopped when he said that and finally just said "ok, I will no longer be working any overtime" and walked out.  He looked...surprised.  I then went to my boss and told him the same thing.  He tried to tell me that it wouldn't be good for my career not to...and I asked him how that adds up with the statement made to me.  he tried to backtrack on what my boss said and I just made it clear that they hadn't fixed my pay and hadn't promoted me so near as I can tell there was no reason for me to work the OT.

about 6 months later I went and told them that I couldn't justify working at the company any longer if my pay rate was not fixed.  Over the course of a year I was promoted twice and my salary nearly doubled to the point where I am one of the highest paid employees.  To this day I will not work OT, if anything I'm known for guarding my time...and I don't regret that.

The company will take what they can get.  They will only value so much as you force them to.  If you don't stand up for yourself, you won't move up.  It's not about how much time you put in, it's about them believing you will leave for another company if they don't show you the basic respect of paying you what you are worth.
It's funny how working hard is equated with putting in bunch loads of hours - especially when removal of the product of thousands of malinvested hours could be the most productive thing to do in many software projects.
When I applied for the place I work I was told we didn't have overtime culture, that we were a software shop with a focus on quality. I believed that until one day, after a client complained that they couldn't see me at my desk as much as the other employees, that actually we were "expected" to work an extra 2.5-5 hours a week on average. 

This job has slightly lower than average working hours (7.5 per day), hence why I took it, since I am more productive when I don't over work, and yet, here I am being told that I am actually expected to do a typical 8 hours a day.

I am planning to quit over this and will tell them why. I have a life, I am not going to burn out my mind or passion for some stupid expectation by managers who can't accept that it is their fault for under estimating project sizes, not mine.

I don't get why CEO's think like this. Like come on most people when they work 40 hours a week actually only do about 30 actual working hours. The rest of the time is spent making it look like they are working.

Why do they do this? Well they simply cannot work more than that.


Seriously, I had all these exact thoughts back in the 1980s while working for an avionics company in the Midwest. We were all white-collar salaried programmers, and we got paid overtime. Then word came down that any time clocked between 40-44 hours a week was now called "casual overtime", and would be unpaid. As a result, I saw people coming in on Saturday morning to read the paper for an hour, just to get their time up to 45 that week.

Either you pay people what they are worth to your company (fair price for a commodity they are selling and you are consuming), or you risk them selling you shoddy goods (or services). The third option, being such an inspirational (or scary) leader (customer) that you can bully them into working for free, well, that's just evil and wrong. Maybe they'll put up a statue of you someday, but more likely you'll lose all your workforce just as soon as the economy tips in their favor.
"Why can't we just hang out as friends for a few hours after 5pm everyday? You could probably get some extra work done while we chit-chat."
The CEO can work longer hours because they are wealthy enough to have everything taken care of and can afford amazing downtime.  Drycleaners, maid services, nannies, restaurants, UberBlack vs bus, living in expensive area close to the office, standing appointment with the massage therapist every-week, high-end gym with laundry service, skiing every weekend, etc.

If the CEO and employee work the same hours, the employee puts in more hours per week once you add up everything.   


You're only required to work for what's outlined in your contract. Unless you signed on for 60 hour weeks, don't do it. It's just a way to squeeze free labor out of you and an extra 8-16 hours a week from you (two days!). Add that up, and you're missing out that pay, time with family, etc. 
"I don’t think a growing, $10M company should be a place where people work from nine to five and then go home. What do you advise?"

Wow, fuck this, fuck this ceo.
I'm assuming Victoria is a standin for [Cerner](http://www.nytimes.com/2001/04/05/business/stinging-office-memo-boomerangs-chief-executive-criticized-after-upbraiding.html):

&gt; ''NEVER in my career have I allowed a team which worked for me to think they had a 40-hour job. I have allowed YOU to create a culture which is permitting this. NO LONGER.'' 
&gt; ''You have two weeks,'' he said. ''Tick, tock.''

The consequences were not friendly to options holders:

&gt; On March 22, the day after the memo was posted on the Cerner message board on Yahoo, trading in Cerner's shares, which typically runs at about 650,000 a day, shot up to 1.2 million shares. The following day, volume surged to four million. In three days, the stock price fell to $34 from almost $44. It closed at $30.94 yesterday.
The best comment on that article:

&gt; More than 2,000 words and not one of them is "union". That's the real problem.

I've worked at jobs like this. I'm currently working at a job like this. I actually had a manager tell me that 8 hours wasn't good enough because the other departments were getting upset that the techs were leaving at 5 (you know, when we'd hit 8 hours).

Thankfully that manager isn't here anymore but that kind of thinking is still present in the upper management. This is one of those "culture" jobs where they think if the culture is good enough and the office is nice enough you'll want to spend all your time here.

Sorry. No. I have a 9 month old at home and I'd like to see him grow up, thanks.
The [actual Linked response](https://www.linkedin.com/pulse/why-dont-my-employees-work-harder-liz-ryan) is pretty good. Not sure the article added a whole lot to Liz's resopnse. 
As a non-US reader and someone who doesn't actually work in software (or at all, still student), it's so weird just seeing how overtime apparently doesn't have to be payed out everywhere. Like, wtf? Why is that a thing? That's really fucked up, you know that? And of course it leads to a culture where the uppers want you to work as much overtime as possible. Like, duh. Where I live I don't know a single field where that is normal, except when you're an entrepreneur/the company owner yourself and not even then. My brother is a mechanic and despite trying not to stay too long and make breaks the correct length (as in, not too short) etc., he still amasses a hundred hours of overtime every year which then get payed out.
I worked with a guy who would come in before anyone else, leave after everyone else, and accomplish absolutely nothing. His resume was impressive and he commanded a very high salary.

Appearances are everything to MBAs.
I had a boss (owner of a small company) encouraging a "bust our butts" mentality where employees would work about 45 hours a week without extra compensation.  This quickly devolved into a micromanaging situation where he was overly concerned about getting those 45 hours over anything else.  A colleague and I would leave work early one day a week during golf season to play 18 and we were chastised for it despite the fact were were already working 45 hours (0830-1800 4 days a week and 0830 to 1530 1 day).  It went so far that he tried to get us to request PTO for the perceived lost time.

The code was, by-and-large, pretty bad largely because you'd have to cram a day's worth of development in after 16:00 when everybody using the system left for the day.  Supporting it was a nightmare and was all you really ever did while people were working.  The work you did after supporting it for several hours was obviously bad because you were so burned out from supporting the shit you wrote last night (shit begets shit...).

After I left, there was a disgruntled employee that actually did his research into labor laws and notified the Dept of Labor.  I heard a bunch of employees were "demoted" from salaried positions and told to punch a clock, which was in their best interest.  There was some rumor of a fine to the owner but I never heard any actual details of it.
Either you are rewarding your employees with a significant percentage of the extra value they are creating by spreading it around, or you're trying to make suckers of them.  If you only reward and retain suckers, you'll have a company full of them.  This is unwise.
Its part culture and part terrible work-place laws. In a previous gig management fucked us over with impossible deadlines and tried to persuade us to stay late to get it done... I left at about 8 (so maybe 2h overtime), the non-aussies on the team stayed till past midnight IIRC (on more than a few occasions) :/

my current gig is 38h week on the timecard, no more, no less, go the hell home on time! - I like that culture.
Victoria even says that all the work is getting done. What does she want them doing those extra hours? Wasting her electricity and surfing reddit?
I was actually told unambiguously that it's better to be a poor performer with a great personality than a great performer with a mediocre personality.  Not only am I expected to put in more than 40 hours a week for a salary that hasn't even kept up with inflation, but I have to make believe that I'm a politician while I'm at it.
I think there are some bigger problems at play here, actually. The first is that everyone working, working together, and getting the job done is somehow a management problem to be solved. If you're meeting expectations and no one's working overtime, that's a management victory and you should be patting your team (and yourself) on the back. People putting in overtime shouldn't be seen as a sign of anything other than that you need more people but don't have time to hire any.

The second problem is anyone giving a shit about "working hard" in the first place. They've done studies, and it turns out that countries where more people think hard work is the secret to success actually have *lower* economic mobility. The more value we place on how hard people work, the more we lose track of what actually makes people (and the businesses they work for) successful. Getting things done correctly and in a timely fashion should be what matters, no how many hours one actually spends in the cubicle.
I've always seen the overtime-race as an example of the prisoner's dilemma. Sure, if everyone colluded to leave work on time, promotions and bonuses would have to be results/performance-based. Unfortunately you just need that one person to defect and stick around for a couple more hours, and suddenly he's the most hard-working guy in the office... and so everyone stays late
Interesting, in France developers are generally paid by the day, not the hour.
No rational person will go the extra mile for nothing in return. If you want your employees to care personally about the success of your company, make them have their own stake in it in the form of equity.
People who want you to work more for the same pay or less should all go jump off a cliff. It is the best solution to your problems and a host of humanity's problems. Seriously, CEOs, you're paying me for 8 hours a day, if you wanted 9, then pay me for 9. I don't care about your product, it's not "world changing", and when you sell the company to a VC, I'm not the one making the millions. But I built all your software and your only contribution was that one time you talked to your buddy at a bar and thought 'hey, this is a cool idea' and then called daddy for his connections with the old boys network for funding.
For what it's worth,  being a CEO or small business owner IS a super stressful job with loads of extra hours thrown in (because the buck of responsibility stops with you).  

However - just like programmers who put in extra hours at work or on a side project - you shouldn't expect others to work extra just because you do
It's inappropriate for CEOs to expect this of everybody, but I get why they do.

It's just that they're wired this way.  They don't think of it as "working extra hours for no pay".  You have to understand that many of them are driven type-A personalities.  Many won't believe me, but they're not doing it for the money (although that helps).  They're doing it because they have a certain drive (or compulsion) in their personalities that they **cannot turn off**.

The root of the problem is that they don't understand why anyone else is any different.
My doctor once told me:

" You doing all this work, but for the company's benefit, not your own."

Changed my life forever.
It sounds like something that worked great when the economy is great, like the 30 years after WW2 in France or the 80's and 90's in Japan.

In that time, people were rewarded for putting in a lot of work by big fat raises, big fat bonus and they could really climb up the ladder of the company hierarchy.

Nowadays it's less true, unless you got the startup lottery and join early a company that ends up becoming Google or Facebook and not one more acqui-hire were only the founders can cash out. A lot of bosses delusion (at least in Japan I believe) come from the fact that they grew up in a climate where they were eventually rewarded for working many extra hours for free, but younger workers know they have a low chance to get that reward even if they work hard.

Reminds me of working at Fitbit, actually. My manager left every day at 4PM and blew his top over me taking a 45 minute bike ride to test a new automatic activity detecting feature that wasn't triggering for shorter rides on a day I worked at the office 8:30AM to 8PM otherwise. How dare an employee exercise on his dollar instead of sitting at their desk!

&gt;She looks at Amazon’s tough culture, movies and shows about startups, and her own, over-glorified past, and thinks that she’s no longer one of the cool kids where people live to work

This whole thing is so overblown with Amazon.  The employees are/were basically trapped there because of stock bonuses equating to hundreds of thousand in compensation (in addition to salary).  They also have relocation contracts people can't easily get out of without repaying Amazon.  

The biggest lie is that anyone at Amazon was trapped at their desk.  Sure, you can walk out, just forgo your $90,000 bonus that goes off in 2 months and pay back $25,000 in relocation money.  These people aren't playing the same game as regular developers but are quick to tell you of the real struggle.

If you're a regular guy working for a wage that doesn't start with a 2 at the beginning, then you have waaay more of reason to be upset (and more mobility) when there is unpaid overtime.   
This attitude will never really go away, because it requires a certain level of experience to realise that quality of hours &gt; quantity of hours. This means that at any one time, there will always be a bunch of people out there who haven't quite got there yet. Kind of like how you'll always be able to find people who believe in Santa. They're not foolish, just haven't had the right level of experience yet. They'll get there eventually, and by the time they do, new people will have acquired the myth in their place, and so on.

I think the only way to do it is simply defend your own hours, and not expect everyone to quite understand why. Accept that you'll be seen as "lazy" by some, but its worth what you get in return.
Flexibility is good, but it should work both ways

If the project sometimes demands long hours, the team should do whatever is necessary to get it done

If the project ALWAYS demands long hours, the manager has failed

If there is no work to do, the employees should be allowed to go home, not forced to sit at their desks and look busy

If the project requires close, personal coordination, all members of the team must be there at the same time and same place

If the project can be done flexibly, the team members should be able to chose their hours, and work from home

The final measure of success should be the results produced, NOT the hours spent producing them
Are there any unions for software engineers? I feel like we need more of them. 
i'm happy to work for a *frugal* company. i hate working for a *cheap* company. unfortunately, *cheap* is becoming the new normal.
You don't get what you deserve, you get what you negotiate.
I actually worked at a place where I was expected for 8 hours/day. At my first job, I always felt bad for being the first one leaving at hour 8, but my work was done and I came to work a little early.

If I hung around, I would just be on the Internet browsing around.
Just as a side note. Money is not all that motivates people. If you want your employees to be enthusiastic you have to be enthusiastic and it rubs off on them. Also tell them when they are good etc.
The best part is that, if you expanded her mentality to the rest of society, it would justify higher taxes and universal healthcare. I mean, why shouldn't you contribute more to the team for the betterment of all? Work and and contribute, not for compensation, but to make this country a better place. Except her attitude is a perversion of that and is basically the mentality of fuedalsim (work hard for your masters because it's morally good to please them.)
Just gonna leave this here: https://twitter.com/modulhaus3000/status/588807393762357248
We should lawyer up.
As in, we should become like lawyers.
Form an association, push laws in our favor, those kind of stuff.

Then we would get some respect.

Of course, this would mean that we would have to renounce the ability to do commercial programming for third parties without a degree. But if one is good, he can get a degree, and the advantages then are too much.

Aside from the money, think about the respect you would get from other people, about being able to people who tell you how you should do your job to go away, being protected if you screw up something but it isn't your fault, etc etc

Also if you get into an hypothetical "official programmer list" you would not be able to be fired. A lawyer is a lawyer even if he didn't work for a long time. Same for doctors and psychologist. The impact of something like this would be enormous, even in job searches and such stuff.

The situation is already unbearable now, and programming is quite requested as a skill. Imagine the kind of stuff the category will have to suffer if the skill will not be as requested as it is now.

Also I'm without a degree, so I would be one of those who would need to study and spend money in order to get back in the market. But I feel a market like that would be much better than this one.

Sorry if I used non-technical legal terms but english is not my first language.
Why is 'effort' measured only in extra hours?

There's often some significant effort people put in to mastering new skills and tasks which can't be measured in hours.  
the thing that gets me the most angry is the whole "the stuff you do that i can't do should somehow be easy for you to do"
This is a larger issue in the US than most even realize I think. Managers and CEOs legitimately believe that we should want to work for free and do things outside the scope of our jobs simply because "its for the good of the company".

Unfortunately, too many people reward that mindset by simply doing whatever it is they are asked to do and not complaining or quitting. That's what got us into this mess, in my opinion.
I worked at a place where people willingly put in whatever hours were needed, and watched it change into a typical work environment where people punched clocks. 

Funny thing is there was never any new rules made that changed the culture.  As far as the execs were concerned they had allowed us to retain our culture through the acquisition.  

Only, they didn't.  They loaded us with more work than could be reasonably done, and used our down time and other freedoms as reasons why we should have been able to get it all done. 

Didn't matter if you worked 80 hours that week, you played ping pong for a whole hour, and went to the local bar, therefore you had time.  So.. people started hiding their down time.  Not getting it in while the code compiled, or for an hour before heading home to work more.  Nope - now they just leave at 5 and it's no one's fucking business what they do after that.  Which not only increased stress (beyond that of the increased workload) but ruined the social structure that made us work so well.  Turns out that people not only still talk shop over drinks, but they get comfortable to say what needs to be done instead of what people want to hear, and don't waste time and resources backstabbing each other.  

TL;DR - don't force people into meeting a metric.  Let them handle things how they need to, and merely expect results.  It worked wonders.  
It's also kind of sad what the state of this industry's culture is that the writer has to write this disclaimer:

&gt; At this point, I want to be absolutely clear, because I’m just envisioning what the comments would look like if this post floated near the top of Hacker News and I got a deluge of non-regular readers.  So, here goes.
&gt;
&gt;First of all, I’m not, in any way, allergic to long hours for myself.  I’ve spent a career working 50 to 70 hours per week, at first for employers, later to earn a master’s while working full time, subsequently to moonlight, and finally to work completely for myself.  This post is not me complaining that work is hard and I want to do less.

Like working 50-70 hours a week was healthy or meritorious. Or not wanting to work extra hours was a sin that left you stigmatized forever within the community.

Sad indeed. 
I recently switched jobs to a company I thought I would really like, unfortunately it turned out to be quite different than I expected.  

One of the biggest issues I have is my workaholic micromanaging boss.  I'm a fairly senior person and I'm certainly not accustomed to constantly being asked if I did XYZ.  If I have a track record of screwing up fine, I deserve to be babied but you hired me as an experienced worker and sought me out for employment.

Anyway, the final straw was when she said she was just checking to make sure I entered my time properly in my timesheet every day an noticed that I had been entering 8 hours most days.  She said to please make sure and enter the actual time I worked.  I'm not sure if she assumed I was working extra hours and that I was just reporting 8 since that was the minimum the timesheet system cared about.  She went on to say that if I was working 8 hours then that's fine, "just wanted to confirm".  I definitely got the sense that "just working 8 hours" wasn't what she wanted.  

I've stepped it up throughout my career to make sure things get done, fires get put out or projects get finished but putting in extra time should be the exception not the norm, it's a great way to burn people out and make them hate their jobs.

Needless to say I'm shopping for a new one already.
It's absurd for a CEO to expect all his employees to be as career invested as he/she is. If they were, they wouldn't be low level employees, they'd be CEOs or business owners. It's great if you want to dedicate that much of your life to your career, but most people have an external life they they also care about. 

Maybe you burn the midnight oil, but you make a large amount of money or are building a business on your own equity. Why on earth would people who have much less to gain be equally invested? 
Thrway just in case. I work at a bay area start up. We have a $1B eval, and are growing like crazy. 

But the culture here is nothing like described in the article. On occasion I've even been told to slow down and not work more than the prescribed sprint. 

There's a lot of worry surround developer burn out here. So they do everything they can to keep you happy. I think the characterization that people in Silicon Valley work like they do in the *show* Silicon Valley is an unfair assessment. 

//$0.02
It really is lazy of those employees. He's giving them equity in the company every quarter in addition to the typical benefits, and they're treating it like it was just some wage-slave job.

Oh.

Wait. He's not giving them equity.
Hello. Potential employer here.I want to start a small business (app and videogames company maximum 10 employees).I am not going to give my employees any mandatory number of hours (nor days) I want to base my judgement on their quantity and quality of work.But I feel that half of my employees will do nothing while the other half will stress themeself to impress me.I feel okay if they come to work 3 days for 12 hours or 6 days for 6, or if they feel at all to do nothing for x days.I like this system as it was I was used to in research and Academy where everybody was basically its own employer and managed his time as he preferred and was later judged on his quality and quantity of work.You had people that worked little but were able to produce a lot than people working twice their hours. And people that were the opposite. Up to 70 hours a week while probably losing too much time.Any recommendation?
The older generations are always getting onto Millennials like me for being so lazy and undedicated.  I find it really amusing how one generation sees a terrible vice and another generation sees a great virtue.
Are there any articles that provide the opposite perspective from an employee?  
I agree with this blog post but I also feel I need to read something that disagrees with it. 
The CEOs always recall a time when the company was just ten people working long hours in a garage on a desk made out of a old door and lament, "Why isn't everyone doing that now?"

Because it's been six years and stock grants are 1000 times smaller! You can't expect workers to keep working like it's garage days but without all the benefit.
Pretty good article.  I've had that issue too, I thought I was doing a good thing staying till 10pm and often till 3 am working on projects till one day I realized I was the only one in the office putting forth that extra effort.  The small bonus didn't come close to the hours I should have gotten paid.  Salary sucks.  Now I'm a straight "you pay me for 8 hours, and that's what you get"
This is like office space all over again...
I have one hard and fast rule: I don't except counter offers. I try to be open about that up front so they're aware that when I give my two weeks, that's it.
I have 15 years of experience working as a developer / architect and have never seen anyone be a good manager when it comes to software development. I've seen some who were loyal to their people, but they never did anything about the useless ones (sidenote: I've never seen a software engineer fired due to incompetence). The others were even worse, they simply ignored ability and referred to the ass kissing quotient. 

I have some ideas about what I would do if I were a manager (I chose the freelance consultant path, so it's not likely to happen anytime soon), but those are just theories. 

The fact is, when I was feeling depressed I could get away with an hour's work a week for months and noone gave it a second thought. That's how hard managing software development is. 
I find this article/topic fascinating. But it's scary at the same time. I really scares me how work goes in most companies, and the thing is that you can do very little about it, it's like fighting a monster. 
Sometimes when wondering the "sucker culture" I am told that I don't like my job enough. People who like their jobs don't mind working extra hours because their are so into it. That blows my mind, and to some degree I actually believe it's true.
It is really hard for me to understand where the responsibility with a company/boss ends. In many interviews I had been asked for commitment of at least one year.... I am not very good at negotiating salaries and asking for raises, however I haven't been in trouble finding new jobs. When someone quits to a company really fast and is seen as having very few commitment with the company I don't understand the where does your responsibility with the company ends. I always like to think of job market as a perfect equilibrium where people can move any time they feel uncomfortable. I guess in that way "sucker culture" will diminish since long term promises automatically are vanished.
I understand Victoria's sentiment if her employees have some equity in the company.
Article describes the protestant work ethic. https://en.wikipedia.org/wiki/Protestant_work_ethic

Which says that working hard is good.
If it's not worth it, don't do it. Different people have different tolerances and goals. Some companies expect more, and pay more. Some companies expect more, and pay less. Guess which type of company keeps good employees?

Especially for developers, after you have a few years experience under your belt you can move up and out if you don't like your current job.

The CEO seems surprisingly naive for someone in her position. If you want more work done, demand it, it might be possible to get more or you might cause attrition and you have to balance whether or not it's worth it.
It is stuff like this that makes me very thankful to have a understanding MD. He takes time to sit with us and understand the problem. **He** wants to understand the problem, and understand why it takes us `x` amount of time instead of `x-1`.

Case in point we are on about a new statistics service that we think  will take at least a week to build a walking skeleton for. You could see that he was visibly exasperated at the rough estimation of a week.

But immediately after that he said "I don't dispute that it will take that long, but I want to understand why". That makes all the difference.
And stop considering your vacation time work time.  If my company wants me to work when I'm on vacation they can pay me extra for it.
I love my job. 
Unfortunately, this sort of mentality is aided by the waves of unskilled/in-training, cheap labor force that buy into the "you don't need college, you just need experience and certifying courses" idea of professional programming. This is very common in my country, in which almost none of the programmers have college degrees, even if they have +5 years of experience. Some even sacrifice their academic performance (or even drop out; fortunately this is a country with tuition-free universities) to spend extra hours at work because "they need to have commitment".

It's even more infuriating when you notice that several studies show most work after hour has a serious lack of code quality, and this same lack of quality ends up creating critical bugs that... force you to work after hours later on. It's a vicious circle in which no one wins: the incompetent manager loses because the project quality is dreadful and they will keep missing deadlines trying to patch it up, and the programmers lose because a) they are losing time of their life at work b) they work in a seriously uncomfortable, frustrating environment and project and c) they pick up toxic habits in software development by going with "fix-it-now-rethink-it-later" tactics.
As an employee, I'm a clueless optimist.  I start my day early (because of real life), then I get peer-pressured to work late (because of 'corporate image').

But as a manager, I think I'm pretty good.  I don't care what time my guys come or go.  I just need them to get a reasonable amount of shit done.
&gt; "When I started this company six years ago there was a lot more team spirit. Now I have to come up with incentives to get people to put in extra effort.
&gt; 
&gt; I haven’t threatened anyone or threatened to cut the bottom ten percent of the team or any of that but I did tell my managers that I want them to incorporate not only output but also effort into their performance review rankings."

Basically: Work for free or be fired. Does Victoria realize what she is thinking?
This is the best tl;dr I could make, [original](http://www.daedtech.com/the-beggar-ceo-and-sucker-culture) reduced by 96%. (I'm a bot)
*****
&gt; So Victoria hands out offer letters that say she&amp;#039;ll give them $25 per hour for a maximum of 8 hours per day, and then wonders what&amp;#039;s wrong with them for not putting in 9 or 10 with a total compensation of $0 for the incremental hour or two.

&gt; I&amp;#039;ve spent a career working 50 to 70 hours per week, at first for employers, later to earn a master&amp;#039;s while working full time, subsequently to moonlight, and finally to work completely for myself.

&gt; Stop feeling guilty for asking, &amp;quot;What&amp;#039;s in it for me,&amp;quot; when your company implies that your 8 hour days should balloon to 10 hour ones.


*****
[**Extended Summary**](http://np.reddit.com/r/autotldr/comments/3uzqsg/why_dont_my_employees_work_harder_the_beggar_ceo/) | [FAQ](http://np.reddit.com/r/autotldr/comments/31b9fm/faq_autotldr_bot/ "Version 1.6, ~18892 tl;drs so far.") | [Theory](http://np.reddit.com/r/autotldr/comments/31bfht/theory_autotldr_concept/) | [Feedback](http://np.reddit.com/message/compose?to=%23autotldr "PMs and comment replies are read by the bot admin, constructive feedback is welcome.") | *Top* *five* *keywords*: **hour**^#1 **Work**^#2 **Victoria**^#3 **want**^#4 **people**^#5

Post found in [/r/business](http://np.reddit.com/r/business/comments/3uvfxe/why_dont_my_employees_work_harder_the_beggar_ceo/), [/r/programming](http://np.reddit.com/r/programming/comments/3uu0gh/the_beggar_ceo_and_sucker_culture/), [/r/LateStageCapitalism](http://np.reddit.com/r/LateStageCapitalism/comments/3uwis4/the_beggar_ceo_and_sucker_culture/), [/r/lostgeneration](http://np.reddit.com/r/lostgeneration/comments/3uv9uk/your_ceo_and_the_sucker_culture_60_years_in_the/) and [/r/occupywallstreet](http://np.reddit.com/r/occupywallstreet/comments/3uyi50/the_beggar_ceo_and_sucker_culture_xpost/).
My problem with this article is that Erik assumes that "Victoria" is some slave-driver boss that wants her employees to work 180 hours with no overtime and doesn't know what fun means. If I'm reading her letter correctly, *this couldn't be further from the truth.*

She might be a first-time CEO/founder that has little to no corporate experience and doesn't know how to motivate her people properly. Moreover, she might have come from places with shitty cultures and never learned what working at a "family-like" business is like. She might not even know how us developers or project managers or anyone that she doesn't spend all day with socializes or even works with each other. *This is much more common than you'd think.*

I think that Victoria took a **really good** first step by inciting the conversation and asking the question. All too often, managers and executives think that hanging the proverbial carrot over their employees' heads will make them want to work harder and/or faster. Instead, she acknowledged that she (a) doesn't know what the right answer to this is, (b) doesn't know why and when her employees stopped being motivated (employees will never say anything to their higher-ups in fear of losing their jobs, which makes obtaining this feedback even more difficult; most can't afford to) and (c) **doesn't want to use the carrot**:

&gt; I haven’t threatened anyone or threatened to cut the bottom ten percent of the team or any of that but I did tell my managers that I want them to incorporate not only output but also effort into their performance review rankings.

&gt; **I want to reward the people who work the hardest here and make it clear that anyone who wants a ‘dial-it-in’ type job is not a good fit.**

What more could you want from upper management?

Another issue that I have with Erik's diatribe is that all too often, most overperformers don't get compensated properly because the "overperformer" is either overperforming on the wrong things or is too mum about their contributions to the company. It doesn't seem like he addresses this in this post or [one of the other ones](http://www.daedtech.com/carnival-cash-the-cult-of-seniority) that he alludes to within it. 

If one developer spends 75 hours improving a backend component of an application that indirectly saves the company $x00k but is part of a 10+ person team **and says nothing of the sort** while another developer on that team spends 40 hours creating a small and dead simple Rails web service that makes the company $x00k and tells every person + manager they can and does talks on the subject, who do you think is going to get a promotion and/or sweet bonus come performance review time?
I'll play devil's advocate here because I don't think it's sucker culture at all.  I will say that at the end of the day, money does play an important role.  But I can't stand articles that make developers seem so one-dimensional saying that money is THE thing to value.  

There are programmers that got into it because they love tinkering and solving problems.  I started programming long before I started getting paid for it.  I still program without getting paid for it for myself and for my company.

There are programmers that are opportunists and have ridden the wave of high salaries due to the general demand for automation or innovation or what have you.  These are the guys that don't really care about solving problems or doing things right.  They care that they get paid for their time.  I tend to think that these are the type of programmers that this article is aiming for.

Back to my situation... I started as a professional developer roughly 15 years ago.  I've worked mostly in hedge funds where it's very competitive.  I remember seeing people work long hours.  And they did get paid.  But these guys were amazing.  I learned so much from reading their code.  And that's what I spent all of my extra time doing.  I put in the extra time to understand their patterns and why they did certain things.  And eventually, I became better at my job.  And eventually I saw the money.  But I also so mastery, responsibility and respect.  

(One other intangible, which to this day, is still more important than money is the people that I work with.  What's the price for offsetting shitty team-mates?)

Either way, I still put in extra time because I gain as well as the company.  I always felt that the more I was exposed to the better off I would be.  If I feel that the balance isn't right, not only money-wise, then I will certainly look elsewhere.  But I'll be more prepared.  It'll be much easier for me to go somewhere else because I'm constantly honing my skills.
As a resident old guy (40) I can try to put things in perspective: the 90's brought us some insanely complicated architectures and patterns which, on paper seemed like great ideas, but in practice became difficult to manage in the real world. Anecdotally, things like COM on Windows comes to mind and so do a lot of the tools and approaches companies like Rational (now part of IBM) were pushing to enterprise developers back then. People were embracing bat-shit crazy, needlessly complicated OO designs where everything was an object that inherited from something which inherited from something else ad nauseam simply for the sake of being the most OO. I've seen situations where things would come to a grinding halt when, what should just be a change of a record in the database, would all of the sudden, require the whole system to be recompiled due to the fact that you had all these layers of behavior and data strung together. Bad design? Sure. But people were learning, and ultimately those failures shaped the industry. I don't think it was really OO at fault, but rather engineers who were way too caught up in the new-new thing. It kind of reminds me of when I see people today using NoSQL for tiny transactional systems, because "relational sucks man" and then having a meltdown when they realize their design choices made it very difficult for their users to get reports they need out of the system in a reasonable way. 

In some respects Java was actually a reaction to all of that and, despite it being fully OO, it actually simplified things quite a bit compared to what people were doing in C++. Things like swift today, while still OO, have pared down the crazy and focused on a more practical approach. 

What does that mean? 

* Will he physically eat money? 
* Will be buy a meal worth one week's pay? 
* Will he donate a week's pay?
It's basically not even wrong, if you read it in context. Data tends to be more important than code. (I have some data in an archival system I wrote that has been generated about 100 years ago, and has survived multiple format conversions in the process.) Translating, I'd say that this guy argues against e.g. serializing Java objects into ObjectOutputStream and using that as a data storage format, which is perfectly sensible. After you do something stupid like that, you're stuck with interacting with the data through a particular language implementation.

Eating a week's pay is easy, just go to the right restaurant.
Functional programming has to be what's "in vogue". Sure OOP is still dominant in the same way that most people on the street aren't "fashionable". Maybe I'm reading too literally.

I know FP has always had a strong following but lately it seems like everyone who's done a JavaScript tutorial once is saying things like this: https://twitter.com/raganwald/status/671119192570134529
Oh, it's tablizer!  One must read the classic comp.lang.lisp trolls in context.  Enjoying a classic 2000s-era gavino requires a certain historical knowledge.
This entire "programming principle X is en vogue and Y is not... and Z is superior, let's use it on everything" is so stupid and limits the mind.  


http://web.archive.org/web/20010310074457/http://www.geocities.com/tablizer/oopbad.htm     
&gt;Why OOP Reminds Me of Communism     
    
   
XD
This entire page is uncannily like a conspiracy theory page written by a crazy person. 
I'm not sure that OOP is still "in vogue" in the way the writer probably meant. I mean, people are still using it, but the OOP craze that existed at that time seems largely to be regarded as a mistake. Nobody looks at enterprise Java apps anymore and says, "Wow, this is the best way to write a program."

OOP languages are still around, but the general movement of the programming landscape seems to have been away from the OO orthodoxy and simultaneously toward simpler structure and FP ideas.
Well, OOP is still in wide use everywhere, but is it "in vogue"?  It's not exactly trendy anymore in the way that Functional Programming is...
Well I mean he's kind of right. Certainly OOP is ubiquitous but I don't think anyone would describe it as vogue any longer.
You down with OOP?

Yeah, you know me.
To be fair, OOP has largely fallen out of favor these days, at least among experienced developers (who have had time to realize that OOP patterns tends to produce overly complicated houses of cards). That's not to say people don't use OOP languages, but going overboard actually doing things in an OOP way everywhere has started to become accepted as bad practice (favoring simplicity over theoretical elegance/genericity and pretty UML diagrams). 
I don't understand the point of this post. OOP is not "in vogue". In vogue does not mean popular, nor does it mean widely used.
[**@fernozzle**](https://twitter.com/fernozzle):
&gt;[2015-12-02 19:19:39 UTC](https://twitter.com/fernozzle/status/672133043037929472)

&gt;From 2001: "I will eat a week's pay if OOP is still in vogue in 2015."
&gt;
&gt;[*web.archive.org*](http://web.archive.org/web/20010310074457/http://www.geocities.com/tablizer/oopbad.htm) [*pic.twitter.com*](http://pbs.twimg.com/media/CVPlcLxWEAE2y6W.png) [^[Imgur]](http://i.imgur.com/iL3rx6g.png)

----

[^[Mistake?]](/message/compose/?to=TweetPoster&amp;subject=Error%20Report&amp;message=/3v77xm%0A%0APlease leave above link unaltered.)
[^[Suggestion]](/message/compose/?to=TweetPoster&amp;subject=Suggestion)
[^[FAQ]](/r/TweetPoster/comments/13relk/)
[^[Code]](https://github.com/buttscicles/TweetPoster)
[^[Issues]](https://github.com/buttscicles/TweetPoster/issues)

Oh, I remember that Tablizer guy! He was basically certain that Clipper, dBASE, &amp;c., were the paradigm of the future: http://www.oocities.org/tablizer/top.htm
You mean we should store data in a database?

One of the dumbest things I've seen posted here.
Idk if OOP is considered 'vogue' or not.  Java still dominates the landscape, so does C++.  JavaScript seems to have adopted at least a more OO like syntax.  However, languages are also moving into a more functional direction, and functional languages are certainly considered "cool".  perhaps OOP is sticky for the reasons mentioned, its "xenophobia" but for now I think we have to say its still quite popular.
I remember this guy. He was all over one of the programming newsgroups (note to kids: a newsgroup is like a subreddit, except you didn’t need an Internet connection to read it), pronouncing anathema against object-oriented programming and stumping for his own One Programming Paradigm To Rule Them All, which involved stashing anything and everything in relational databases.

(google google)

Ah, he has [a page](http://c2.com/cgi/wiki?TopMind) (last updated a year ago) at the old c2 wiki.
Well, it depends on your definitions. If "completely taken for granted" doesn't still count as "in vogue", then sure, he's fine. :)
[This is too perfect](http://i.imgur.com/lOdvAbH.png)
Wasn't all that bold a prediction - XML was first released in the late 1990's.

Streaming objects with their methods and crap was never going to work well, didn't take a genius to see that in 2001.

Not in vogue as "architects and other kids aren't pissing their pants over it".

But  very much dominant, which is probably what this person meant won't happen.

It's a tool in the toolbox, he who has a better one will blow the competition away and the tool won't be used then, right?
He does present a valid point about data exchange. 

It's funny to me that his description of OOP limitations in 2001 make almost no sense in the context of 2015 OOP best-practices. I feel that at least part of his rant is trollish ("Xenophobic"? really?), but I also have to recontextualize myself with the old ideas of *truly* object-relational datastores and language-native RPC protocols that were considered to be the next big things at that time, because they promised to eliminate the "impedance mismatch" of serialization and deserialization, mapping and unmapping, etc. 

To be honest, the wholistic definition of "OOP" that we recognize today is so much narrower than its definition in 2001 that his prediction is essentially correct. The only inaccurate thing in the meat of his rant is the implication that "building OO wrappers and mappers" takes "a lot of time". Once we all agreed that standardized serialization formats were a good idea, we spent 5-10 years building those wrappers and mappers and now we are essentially done. Now we laugh at anyone who proposes a new language-specific serialization format, because why not JSON? Also, no standard serialization format in wide use today includes object methods in the payload, so he gets another cookie there.

When I do end up using Java Object Serialization, usually in the memory-caching space, or when I use java remoting between JVM cluster nodes, I do sometimes run into the issues he complains about. If you don't run the exact same JVM version when reading and writing, the system fails. This used to be the kind of infrastructure businesses were investing in, and it is definitely a bad thing. I'm glad OOP does not imply this level of integration across an enterprise anymore.
Show me a AAA game engine, or even a AAA game that doesn't use OOP.

There are reasons it exists, and nobody has shown a better solution for those cases than OOP. Certain programmers may be in fields or projects where OOP doesn't make sense or isn't optimal, but that doesn't mean OOP is bad for everyone. The argument that it's bad or going away makes no sense.

And every time I hear about someone say why it's bad I just get examples of people that used OOP in bad ways, like crazy multiple inheritance trees. OOP can be misused, just like any other language feature, that doesn't make it bad.

Also, just because you're using some OOP doesn't mean *everything* in your code is OO (unless you're using a language that requires it). I've seen complaints about "not everything has to be an object!", or "just because it can be an object doesn't mean it should be!", and both of those are correct, but sometimes OOP criticism seems to imply that all OOP is used that way, and it's not.

EDIT: Just to clarify my position, I consider the Entity/component model to be OOP. Unity3D and UDK use entity/component models, I've not used CryEngine but I am guessing it does as well.

EDIT: I didn't realize I was getting into a religious debate, sorry if I've offended some of you.
In vogue is a vague notion and kinda depends who you talk to. Is anybody still using OOP? Sure. Too many people. Is anybody genuinely getting excited about OOP? I think not, and I sincerely hope that I am right.

That is not to say that objects aren't useful, but OO as defined in 2001 is not cool at all!
You know who's NOT in vogue in 2015?

This guy.
Is there one for Java?
Or the problem they need to know anything about the car and you don't run the exact same JVM version when reading and writing, the system fails.
Or the problem they need to know what's en vogue I'll ask some big government contractor.
I read this as "will eat POOP"

am disappointed
This is hilarious because in one of my classes we have been using C as a function language mainly and in 2 of my others we've been using C (not ++)  as somewhat object oriented 
Hope he gets paid in peanuts.
But OOP is not "in vogue", in that, while it's certainly still common in everyday programming, it's not "fashionable". These days "functional programming" has become the fashion.
OOP isn't really "in vogue" anymore ie popular, fashionable.  That would be FP.  I guess he can eat for a week.
That's the thing. It's not "en vogue" because it isn't the current fad and is instead a pretty staple style. Functional programming is what is "en vogue" right now because everybody thinks they are the shit if they write some shitty useless program in Haskell alongside anybody that writes some elegant and invaluable one in Haskell.
OOP is not in vogue any more because OOP was nothing more than a way to organize the code in such a way that various parts of it communicate only through messages and nothing else.

After 30 years of OOP practice, we came full circle, realizing first hand what Alan Kay realized when he made Smalltalk: that in order to have a modular system, we need to model the communication of modules with messages.

And that thing, i.e. messaging, is prevalent in computers on all levels and domains: from submitting urls, to public web apis, to message protocols, to remote procedure call systems, to virtual method calls, in all modular systems communication is done via messages.

Message passing leads to encapsulation, because if we have messages then we can hide the implementation details.

Message passing leads to subtyping, when we want to make messages statically verified by the compiler.

Which leads to inheritance ('extends"), because code reuse can be enchanced by  sharing implementation between subtypes.

Message passing is used, beyond the traditional languages, in Erlang, in fp languages implementations (in closures), in the operating system kernels, etc. It is ubiquitus.

One thing that objects won't do though is to increase reuse of algorithms. In that domain, OOP does nothng, it has asbolutely nothing to say.

And that is the reason there is currenly focus on FP.

FP does a great job to allow for reuse of algorirthms; it can greatly simplify and reduce the effort required to solve a problem.

In reality, both OOP and FP are required for writing good programs.

A program is composed of Data and Code.

OOP helps with organizing Data and Code into logical modules.

FP helps with reasoning about Data and Code and reusing algorithms.

OOP without FP is actually Procedural Programming with an extra organization tool. Programming inside methods is actually straight up Procedural programming, because methods are procedures, i.e. list of instructions to execute with a very specific order (doing things in order = follow a procedure).

Thus, OOP and FP are almost orthogonal to each other and one completes the other - and that is reflected to all the recent developments in programming languages that are a mixture of the two.

In the future, after the FP vogueness goes away, we will rediscover data-driven programming.

And finally, we will rediscover event-driven programming, to complete the circle (OOP, FP, DD, ED).

Object oriented programming is a great way for humans to organize and think about code.  You'll see it used heavily in any large application, library, or framework.

I've personally seen it used in many web frameworks.  I've also programmed iOS where literally everything are objects.

So, no, I can't imagine object oriented programming going away any time soon.
I think a lot of the anti-oop rhetoric we see is just elitism.

A lot of programmers want to feel as though they are *special snowflakes*, so if regular Joe programmer is using OOP, you use functional programming to set yourself apart.

The reality is if you are building something good, it doesn't matter. If you are still debating whether to use OOP or functional programming, then you clearly haven't actually started writing your product yet.
OOP is not in vogue.

In the 90s it was supposed to be a silver bullet. Many teach yourself in 21 days books were sold.

In 2015 object orientedness is one of the tools in the toolbox. It is not a silver bullet.

Sometimes I let an interview candidate pick their language for a white board problem. Often they pick a language they don't know well; this is a mistake. Then I ask, why did you choose language X? If they say something about polymorphism, they fail and must go back to their 21-day world.

It's hardly 'in vogue,' it's either legacy or people don't know anything else.
What exactly is it that OOP gives us over having libraries and structures?
ITT: people dissing OOP without knowing how to use OOP well, thinking OOP is only about shit inheritance hierarchies. 
Sounds like this guy wants us to use XML!
I thought that said OPP I thought I was in hip hop heads
It seems there's a difference between scripting business logic and engineering.
To be fair he is right. Now OOP is kind of old hat while functional and components are the new hotness.
Questions What do these modern frameworks equate to a specific implementation, but it becomes super cheap in the 2000s people were doing this sort of thing being done in the Big Enterprise Stuff.
The hardest part of the industry, in my opinion, is that things come and go, or come and stay very quickly.  People always want to be on the forefront, learning something day 1 and trying to become a master of it, and sometimes it completely flops then they may have a bank of useless, very specific, knowledge.  In a way it feels almost like gambling to me, but I guess the takeaway is that you learned to learn and in the end you did net additional knowledge which is never negative.
Boy, he sure didn't see [JSON](https://en.wikipedia.org/wiki/JSON#History) coming. :)
A Geocities page? Any older, and it may have been written on papyrus.
What a dumb bet. OOP has been around since what? The 70s? It was popular in academia, which means it's inevitably popular within the commercial world. What would cause OOP to fully go away? The function revolution has happened, and we saw that both paradigms can exist together.
It is not a bad point, so what is the guys go to language?
Are you down with OOP? Yeah you know me! 
I've never been a fan of OOP, at least not how most people think of it. 

My favorite methodology is Functional Programming, which is based around functions along with their inputs and outputs, instead of mutable objects. For any function f(x), the outputs will always be exactly the same if you pass in the same inputs.

OOP is typically paired with Imperative Programming, where you instead have mutable objects and variables that can change state. This means all of your functions must deal with structures that evolve over time. This in turn means you must be very careful to delineate private and public structures. Which in turns means you need to add more code to allow outside classes to modify your structures (the typical get/set). This means you end up with something that's much more complex to understand and more prone to bugs.

You can use some OOP concepts just fine in Functional Programming, such as having classes and instances. But unfortunately too many developers go too far.
I have been told that my code/design is "not very oo" and this was clearly unacceptable.

This needs to be seen as a neutral statement.
So basically he wants us to use naked C structs instead of coupling code with data because it is more portable? Well, the whole reason people moved from C is because naked structs are annoying to work with and if you really want then you can convert all Java classes to C structs with a single method invalidating his claim that it is hard to port.
I do see a lot of talk about functional programming, so he could be onto something. At this point I am a hobbyist but I do kinda prefer functional, and OOP makes things more complicated than they should be. But that's just me, I don't work for the big four or anything like that.
It's not "in vogue" anymore.


OOP is older than the hills http://www.exforsys.com/tutorials/oops-concepts/the-history-of-object-oriented-programming.html , and is still very relevant today.
I look forwarding seeing how the ingestion will take place.

I work in web, so worked almost exclusively with languages that are not functional (php, python, ruby, js).

It's just a matter of "The right tool for the right job". I'm working with users, products, orders, invoices, payments and plenty of other types of interconnected data that is stored in a relational database with plenty of foreign keys (think of databases with over 500 tables, and if denormalized, the products table would have over 5000 columns). I can't imagine how this would work in a FP language, maybe it's my lack of experience here, and would like to be enlightened.

If I need massive data transformation (generally for reports) - some ETL processes can be used, alongside FP concepts like monads.

Neither is dead, both have their uses, and both are often misused. But isn't this programming since day one ?
Yea, Xerox PARC was so dumb. Inventing OOP, GUI/mouse, Hypertext, and Ethernet. Nobody uses that stuff anymore.
What the F**K is this dude talking about? Melding data with methods? 

I was writing Java in 2001 and no, I was not melding data with methods. I was also using RMI for networked video games, and 'dhtml' before that was even a thing - because page reloads suck.

2 out of 3 ain't bad... 
Do people here think that OOP is still in vogue? Surely OOP has been discredited. Genuinely unsure of what the groupthink is on this.
This guy reminds me of every dope that jumps on the dick of every new paradigm
The title is slightly misleading. The Open Source editor mentioned here is Visual Studio Code, which is basically a bundled webapp and has nothing in common with the actual text editor in the commercial Visual Studio versions (at least for now).
I'm not sure if I should applaud or check my meds.
can I develop C# in Visual Studio on Linux and then use a Linux CI/CD pipeline to deploy to Linux containers?

if so: go to hell Java.
Really great to see VS Code go open source. I've been using it for the past 2 weeks, and it's one of the best editors I've ever used (at least for TypeScript/JavaScript). It made switching to TypeScript much easier with all the hints it provides.

With extensions, it should hopefully get even better.
Oooh. I'll be trying that GDB stuff out. I've got a Linux app I need to support and debug, but I never quite got comfortable with gdb and the IDEs I've come across on linux are just so damn clunky...
cmake support?
Just installed it. Everything is working. Delve debugger integrates correctly. This is pretty cool. Now, how do I get vim mode?

Does anyone else find it slightly humorous that the first application Microsoft ships for Linux is... a text editor?

([somewhat relevant](http://www.catb.org/~esr/writings/unix-koans/editor-wars.html))
This is the best tl;dr I could make, [original](http://arstechnica.com/information-technology/2015/11/visual-studio-now-supports-debugging-linux-apps-code-editor-now-open-source/) reduced by 88%. (I'm a bot)
*****
&gt; Visual Studio 2015 already made big strides in this direction, and Microsoft is pushing ahead to try to make Visual Studio the best development environment around.

&gt; This will replace the old Visual Studio Gallery and will be a single place for finding extensions for all versions of Visual Studio, Visual Studio Team Services, and Visual Studio Code.

&gt; To make all-device development easier to access, Microsoft is releasing a new Visual Studio bundle called Dev Essentials, which includes Visual Studio Community Edition, the free tier of Visual Studio Team Services, and from early 2016, monthly Azure credits.


*****
[**Extended Summary**](http://np.reddit.com/r/autotldr/comments/3tb5wm/visual_studio_now_supports_debugging_linux_apps/) | [FAQ](http://np.reddit.com/r/autotldr/comments/31b9fm/faq_autotldr_bot/ "Version 1.6, ~15439 tl;drs so far.") | [Theory](http://np.reddit.com/r/autotldr/comments/31bfht/theory_autotldr_concept/) | [Feedback](http://np.reddit.com/message/compose?to=%23autotldr "PMs and comment replies are read by the bot admin, constructive feedback is welcome.") | *Top* *five* *keywords*: **Studio**^#1 **Visual**^#2 **Microsoft**^#3 **NEW**^#4 **extension**^#5

Post found in [/r/programming](http://np.reddit.com/r/programming/comments/3tb05i/visual_studio_now_supports_debugging_linux_apps/), [/r/microsoft](http://np.reddit.com/r/microsoft/comments/3tb08x/visual_studio_now_supports_debugging_linux_apps/), [/r/opensource](http://np.reddit.com/r/opensource/comments/3tb083/visual_studio_now_supports_debugging_linux_apps/), [/r/linux](http://np.reddit.com/r/linux/comments/3tb06l/visual_studio_now_supports_debugging_linux_apps/) and [/r/Technology_](http://np.reddit.com/r/Technology_/comments/3taz7h/visual_studio_now_supports_debugging_linux_apps/).
This is the first time I'm reading of the Nov. 30th release date for VS2015 Update 1; good to know. Looking forward to that and especially the C++ Core Guideline Checker that should be released at around the same time.
Wait, what I go from this article was...Microsoft has an android emulator? I've been using Xamarin's for a while, but Microsoft has one too? And it looks awesome?

Holy shit. 
Yay! I love VS Code. It's a minimalist programmer's dream. I ditched it because it wasn't open-source and had no plugin support. I'm a vimhead so the only way I'd adopt it is if I can tweak the hell out of it. Me being a web developer, I'm dying for a web-based IDE that's actually good and I can just straight hack on &lt;3
Note that this only supports CoreCLR under Mono... So if you're actually trying to use .NET Core you're still sitting in shit.
So now programs are called "apps" on Linux too I guess
Linux apps written in what language(s)?
It's certainly an exciting time to be a VS user / .NET developer.
I have to say, on the one-year anniversary of the announcement that .NET is open-source, I'm really disappointed with the progress of crossplatform .NET/ASP.NET. At the rate they're going it'll be years before it's production-ready on Linux. MS is taking their sweet time. Their Core/ASP.NET teams seem bare-bones, why not add more developers? What's a couple more million a year for 10 top of the line developers? (yes, I know about the mythical man-month...but since we're talking years here, extra manpower would definitely accelerate things).

That said, I'm glad Code is now FOSS. The SublimeText dev must be shitting his pants.
Now this I'm actually pretty excited for. Ever since the Windows IoT project dropped support for my Galileo I haven't really had a way to work with it that didn't involve installing Linux and having to learn a new IDE or vim or whatever and gdb. 
Awesome! It's really nice to see Microsoft open-source Code. Tried it out during its initial release, and it was pretty fast compared to Atom. I'll probably switch back to it as my main editor now that it's OSS.
Good to Microsoft stepping it up but I think I'll just continue to use Linux + Vim. 
Well holy damn.

I never thought I'd actually see a Microsoft brand fighting for headspace with the big boys' toys outside of their magical .NET bubble, but they might just be trying.

I'm seeing some props for it with JS and TS in a few places in the thread; I might just have to give it a shot myself.
Meh. After many years away, for the last year I've been doing windows development. While VS has a lot of great bells-and-whistles features, as a programmer's editor it really sucks compared to almost anything out there (how it handles tabs &amp; whitespace when moving or selecting by word, how it handles different text encodings, etc). Everything is clunky, and while you can fix a lot of it with endless googling and installing plugins, you shouldn't have to do all that out the gate just to get to parity with saner editors.

Want support for Ruby and/or Rails in Visual Studio? Go cast your vote here! http://visualstudio.uservoice.com/forums/121579-visual-studio-2015?query=ruby
This kills the sublime text. 
Pretty exciting news about the editor going open source, liked that one much more than other open source alternatives.
any linux users have any luck in getting this thing up?
I currently use VisualGDB, which works well

I am hoping that VS with GDB support will be even better
Hopefully this will lure some windows developers to switch to developing for Linux. 
Go vote here to request Ruby / Rails support: https://visualstudio.uservoice.com/forums/293070-visual-studio-code?query=Ruby
[deleted]
And recently Microsoft embraces its new Brother company Red Hat.

What a coincidence!

Don't get me wrong - it's good that Microsoft leaves the 1980 era but, it took them like what, 25 years or so.
Dreams truly do come true...
... blah-blah-balh ..... it's getting open sourced .... blach-blah-blah....

good!
Give it a  few years and see how Microsoft pulls the rug out from under you as they always do, always have, always will.
[Visual GDB](http://visualgdb.com/), is that you? :-)
I call bullshit. This story is just an embellished version of something that Ken Thompson, the designer of Unix, famously did. Obviously Thompson did it without the white supremacist messages. Read all about it here: http://c2.com/cgi/wiki?TheKenThompsonHack

This is not easy to pull off. I obviously have no proof, but I find it hard to believe that a random grad student working for a psychology department would do exactly what is narrated in what is probably the world's most famous story of a subversive hack. It seems more likely that the author took the famous story, adapted it a bit and posted it on Quora for the +1s (or whatever it is that you get on Quora).
If you asked me my worst nightmare, it would be maintaining another person's code so a legacy system can continue to run well past its intended life span, and gain new features.
These three words - usually by the coder themselves:

*"Huh. That's interesting."*
Definitely fake but a nice read.
Sound like an urban myth. The moment it was clear the problem is in the compromised computer system you back up the data and reinstall everything from scratch, starting with the OS.
I love how he the guy agrees to a reasonable solution (the goal was to get stuff working, after all), but then goes "fuck this, it's personal now". Which was good, since they wouldn't find the security breach on /sbin/login otherwise.
Other people's code.
1. Codes spreading thousands of lines, copy pasted, to another 20-30 files, There are 3 templates
2. database tables and columns named like a001, a002 ... a0030, columns named like f001, f002 on each tables,
3. each file works on 2/3 tables like this (actual code), EACH AND EVERY queries are done this way:     

        $sql = "SELECT ac0038.f0001 AS noTrans, DATE_FORMAT(ac0037.f0002, '%d-%m-%Y') AS tgl, ac0038.f0004 AS keterangan, ac0038.f0005 AS nilai, ac0038.f0002 AS noUrut FROM ac0037 INNER JOIN ac0038 ON ac0037.f0001 = ac0038.f0001 AND ac0037.f0009 = ac0038.f0008 WHERE ac0037.f0009 = \"" . $param["cabang"] . "\" AND ac0037.f0003 = \"" . $param["curr"] . "\" AND ac0037.f0004 = \"" . $param["acc"] . "\" AND DATE_FORMAT(ac0037.f0002, '%Y%m%d') = " . $tgl . " AND ac0038.f0099 = " . $param["status"] . " ORDER BY tgl, noTrans";

4. Also contains HTML like this, again each and every view specific code are done this way:
    
        echo "&lt;div style=\"text-align: center; font-weight: bold;\"&gt;\n" .
                    "&lt;h1 id=\"12\" name=\"12\"&gt;Bank Reconciliation&lt;/h1&gt;\n" .
                    "&lt;span id=\"1124\" name=\"1124\"&gt;Cabang&lt;/span&gt;&lt;span&gt; : " . $this-&gt;spesific-&gt;getNamaCabang($this-&gt;param["cabang"]) . "&lt;/span&gt;&lt;br /&gt;\n" .
                    "&lt;span id=\"1\" name=\"1\"&gt;Akun&lt;/span&gt;&lt;span&gt; : " . $this-&gt;spesific-&gt;getNamaAccount($this-&gt;param["acc"]) . (($this-&gt;param["acc"] != "")?" (" . $this-&gt;param["acc"] . ")":"") . "&lt;/span&gt;&lt;br /&gt;\n" .
    
5. And written in non-English, and not my native language.
6. Without ANY documentation 
7. Is scheduled to be deployed in multiple offices in multiple countries.

All in a single project ... and I'm having to work on this and have to add features even


Have a good nightmares.


EDIT: Oh and URL parameters are like this: ?452844884648468840884048=1116301310&amp;44484488=BCBAR1509001&amp;400839283968392844484168=BAR
Its NOT the worst nightmare.  As others have pointed out, there were easier solutions involving a different machine.  The writer chose to fight a nerd war because he ENJOYED the challenge.
Being trapped in a maze of spaghetti code.
The way the story is told, it's not *immediately* obvious whether:

* (A) Were the subliminal messages were related to the psychology-research, e.g. deliberate priming to see how it changed their answers?
* (B) Was ex-grad student who made the program was also (probably) the person who hacked up the system?

Accidentally coding something like this:  https://en.m.wikipedia.org/wiki/Therac-25
Stack overflow is down!!
Not gonna lie, halfway through day 1 I would have been like "nope, time to scrap it and start over!" The code itself wasn't complicated enough to warrant more than that.
No a coder one, but [debugging a hardware](http://nighthacks.com/roller/jag/entry/at_the_mercy_of_suppliers) using a Geiger counter would come close.
So I clicked the link and a Java notification popped up telling me to update.  That really is a worst nightmare.  
If this story is true, it's pretty incredible. Are there any other real world "trusting trust" stories like these out there?
I was going to say cancer. 
If that were a true story, it sounds like fun to me. Not sure why the compiler wasn't his first thought though.

My worst nightmare is debugging an issue that happens 1% of the time, has no details provided from QA/BA/Users, requires data that only exists in production, and takes half an hour to intricately setup each test run. Eventually it ends up being an environmental issue and you have to go to war with operations to get them to take ownership.
Mine is having to maintain a 15 year old million line code base that I'm never allowed to refactor because we constantly have some critical release in the near future.
My worst nightmare is when something is chasing you, and you look back and its literally right behind you the whole time... So hard to run with jelly legs. 
Quora is garbage 
Man, I was like "Oh, it's the compiler" after the first time it did it. Was it not obvious? There's no way it would change the source code otherwise if it was just an include or something. 2 weeks?
Clearly this guy has never worked with a sales team that sells a product and books the business before its designed. 
Programmers worst nightmare? Turning 40.
Reads like a fucking creepypasta story
TIL Mel was a white supremacist. 
I won't lie, I've been in situations like this, where on Day 3 I discover there is a setting sending my compiler output files to a different directory and I've been running the same 10-month old program every time.

I call it the ruby slippers situation - you spend hours or days researching an issue and find out the solution was a config file in the root of your project the whole time. It's always a config file in the root of your project.
Coders worst nightmare? Users.
Site doesn't work without allowing foreign code to run on my computer
http://superversivesf.com/wp-content/uploads/2015/11/ripley.jpg
ellipses
"each day your keyboard will take on a new, random layout"
RSIs
Sometimes you just gotta grep the whole machine.
What I find interesting here is that I would never have been able to come up with 3/4 of the things he tried, and yet as soon as I heard that it not only re-added the prints but also modified the source, I knew to check the compiler. I mean, it's literally modifying and moving the source code...that's not coming from an included library. That *has* to be the compiler. He wasted like 2 weeks on stuff that seems to me like it should have been discounted as impossible right away, or at least until checking the compiler to find that it was clean. 
Coder's worst nightmare is product management.
Bosses that code part time still ...
If this guy didn't go straight for the compiler when the magic started happening, he doesn't deserve to be called a programmer. This is a famous hack: rewrite the compiler to insert code if it isn't there, compile the compiler, then remove the code. Voila, the compiled compiler will now add the code even though it never shows up in the compiler source. If you see malicious magic, get a new compiler. This guy wasted a ton of time tracking down a known vulnerability.
These three words - usually by the coder themselves:

*"Huh. That's interesting."*
I started reading this thinking it was an actual dream...
awesome?
I want to believe. 
Undocumented code, or poorly documented code.
Clowns.
Refactoring "self-documenting"  code that performs unexplained actions on the data. Why does it swap hyphens with underscores? Only the guy who wrote it five years ago knows. 
I would guess writing it from scratch would have been the better approach. All this did was waste hours and cost money. It became an ego job instead of getting back up and running. 
its five am. why did i read this.

either way, imma sleep now. bye guys.
This: 
Someone: hey remember that bug you fixed a while back, it just re-appeared
Me: There's no way, it was fixed and tested....wait a minute....something is not right here....Nooooooooo......I launched the wrong version of the build! Fuuuuuuuuuuuuuuuuuck
I think the real worst nightmare is when you are up all night coding and wake up the next morning to realize it was all a dream.
If this had been me, due to this being a relatively simple project.

Day 1-3: Figure out how fucked this is.

Day 4-5: Reverse engineer
Untested code
That grad student?

Albert Einstein.
Code.
It might be a little late to post this, but this reminded me a lot of a story called [Coding Machines](http://www.teamten.com/lawrence/writings/coding-machines/) by a guy named Lawrence Kesteloot. 
Somebody could use #define to change certain keyword to other. 
Fake or not, am I the only one thinking: take the unobfuscated business logic, nuke everything else from orbit?
A coder's worst nightmare is reading the code they wrote last month.
For me it is a legacy system with years and years of "improvements". 
writing an entire project in a format that is grossly inefficient, and realizing that you could have saved hours of coding and debugging time by doing it differently, but you're now too far into the inefficient format to back out. 
No documentation.
A coders worst nightmare is having little time to make a little change to some complicated system, and finding out no one has the original source code.
I came here to say zero commenting in 200 lines of code but I didn't notice it was an article. 
The first thing he did was modify and compile the production code on the production server without having any sort of copy? :-o
Total fabricated bullshit
Most annoying project I've had to work on was being asked to port a program that was written by a medical professional. Turns out it was written in one giant spaghetti file of code. I found it easier to reverse engineer the desired behavior by using it and re-implement it from scratch.
Some people are close, but it's really quite obvious because it's the thing coders do the least and it has fostered huge methodologies to remedy it: Communicating with others.
I was really, really, really hoping that he turned out to be the _subject_ of an elaborate study of personality types.
Bugs in your code... No, not your "bugs" in your code. Literally, bugs: https://en.wikipedia.org/wiki/Software_bug#/media/File:H96566k.jpg

Wikipedia article: https://en.wikipedia.org/wiki/Software_bug
Oh, I don't know, probably being called a "coder."
Probably broken glass in the urethra is up there.
I own a healthcare software company and HIPAA fines can reach into the millions for lost data. So I fear that code I wrote, or that we publish, will have holes in it and expose someone's data and have them harmed by it. I would probably lose everything I've worked for. That's really my worst fear. Human error is our biggest risk.
grad student owned this guy, he's not a programmer he's a desk jockey who "does some code"


&gt;  This guy isn't beating me. We are compiling it from his stinking code or not at all! "You don't have to pay me anymore, Dr. Phelps, I just want lab time." 

well ok.   he's a programmer.


&gt;  It took an AT&amp;T tech to find this.

nevermind.
I've once had to deal with a cobol program with gotos all over the place, back and forth with no apparent structure. 
That wasn't a pleasant week. 
The answer should be javascript ;-)
Ha, we need a new genre of fiction on programming... was highly entertained by the writing. Nerd war, lol.
Fake and gay.
Large Python 2 codebases. 
My worst coder nightmare is being stuck with a system that only has vim or emacs and can't use an IDE on it.
&gt; What is a coder's worst nightmare?

Global Thermonuclear War.
Aaaaaand that's why debuggers exist... (Shudders on the thought of the debugger lying about the disassembly) :-)
People who won't use version control correctly (not can't. Won't.)are my worst nightmare. One of my colleagues regards the git flow process as a formality that should be only done "if you have time" and commits to any old branch and opens feature branches all over the place from other feature branches and refuses to do QA. Fortunately he's not on my project team but if you ever pull this shit in any project team i'm on, so help me God I will fucking end you.
For me Visual basic pre 2008. In fact just VB in general.
shitting on the unix admin because he didnt immediately know how to do something instantly told me this was rubbish. 
And that ex-grad student's name ... Terry Davis
Definitely a bullshit story, but my question would have been having with fucking awful is the professor to have someone do this to him. If I found something that awful done to someone I would assume they are like a pedophile or something. 
if you compile your source code and it compiles to bullshit, wouldn't the first thing be to copy it on your own machine and compile it there instead of wasting 14 days tracking down an issue, that isn't even yours?
This isn't really a nightmare at all. All the program does is ask questions and do a calculation. He could have written the program from scratch in a few hours. Then again, in that case he wouldn't have been able to bill all those hours of coding time, so maybe this was actually what he wanted. 
"Not everyone can become a great artist, but a great artist can come from anywhere"?

^Edit: ^obligatory ^"Thanks ^for ^the ^gold" 
It's a two way street. If you don't give programming jobs to mediocre programmers how do they get better?

Experience is king in most every trade I can think of.
Yes, not everyone is going to become a paid software engineer. But, I do think it would be useful if a lot of people started getting at least a taste of what it is to program so that people can become more technically literate. Even the concept of if statements can help people who are using computers figure out what is going on.

At my university, and I know at several others, everyone has to take at least one programming class. It's not going to make everyone a professional software engineer, but I do think it helps with general technical literacy across campus. People have a better grasp of basic logic and they can translate things like iteration to solving equations iteratively in several physical engineering problems. Moreover, it helps people who had never been exposed, like myself, to see a whole branch of science that they hadn't really thought about before.
I would say step 1 learn to code, step 2 learn to code well.I can't tell you how many self taught twenty something's who thought they were utterly infallible coders I had to knock down a few dozen pegs before they started to take learning to code well seriously. Of those people I'd say maybe one in ten actually did, most just did a half assed job til their work became a living nightmare then found a new employer... You watch their LinkedIn and they're in a new role nearly annually... :/Edit: My first reddit gold :D thanks anonymous person!
What I found really shocking is the awful employment prospects for programmers in England. Programming used to be somewhat looked down on here as well, but the dot-com boom and the last decade of tech explosion has made people realize that software developers are the new Masters of the Universe, with low unemployment rates, high salaries, and strong job markets. 

It sounds like this is not true in England? That would explain the bad perception that people have of programmers there. If that is indeed the case, people should really be focused on growing the demand for programmers in England. Encouraging startups to be created locally &amp; wooing big companies to set up branches locally, would go a long way.
Sigh. The article refers to a Coding Horror post which refers to this small "study" that was never published and later retracted by the author because it was widely used to make the wrong conclusion about programmer ability:

[The camel doesn’t have two humps: Programming “aptitude test” canned for overzealous conclusion](http://retractionwatch.com/2014/07/18/the-camel-doesnt-have-two-humps-programming-aptitude-test-canned-for-overzealous-conclusion/)

I repeat the mantra over and over: "Becoming a software engineer is hard. Learning to program is easy."

And really, becoming a software engineer isn't a hard task so much as a thousand moderately-difficult tasks. And very few employed software engineers have all thousand under their belt anyway.

I can't believe articles like this still get popular attention. The "us programmers are just smart and able to do things others could never be taught to do" is a self-serving zombie idea that refuses to die.
The bigger meta question here isn't whether anyone can learn to code. The question is whether it's possible to teach anyone to think abstractly and express abstract concepts concretely. This base skill is what determines your aptitude. Whether you become a doctor, lawyer, or programmer. The base determinant is your skill in abstract thinking and expressing abstract concepts concretely.

I do believe that skill in this is to some extent built in. What I don't know for sure is to what extent hard work and practice combined with age affects it. I'm not sure anyone knows for sure. But the level of skill in these two things have the largest bearing on your aptitude as a programmer.
I think this relies on the fact that CS = programming, but it doesn't.
Kinda like music or sports, some have talent, some don't

Learning a bit about programming is probably a good thing for overall technological literacy

But, believing that a person with no talent can take a short course and rise to the top is silly

Imagine if someone offered the same thing in sports, promising that a person with no athletic talent could take a short course and then play in the NFL
There were some studies how to identify good programmers with a huge set of questions to try to correlate with personality type and other preferences. It turned out that the biggest correlation was for the question "do you like to program".

I personally would not mind all people leaving the profession who don't enjoy it but force themselves for the money. I would apply the same thinking to doctors, lawyers, etc.
I'd be happier if programming became a trade. Do your basic trade school, learn the basics. Then apprentice under a master until your get enough hours of programming to do your journeyman. Then work your way up to master and start taking apprentices. There is a lot to learn from coding, and like carpentry, electrical work, and so many other trades a lot of that isn't necessarily learnt from reading it, but actually implementing and doing it. 
The fact that X is hard, does not imply that X can only be done by a select few. The only real aptitude programming requires is the same for mastering any discipline; the dedication and opportunity to devote a lot of time producing code, and, this part is key, reflecting on your mistakes and learning from them. This goes for creative writers, athletes, architects, actors, musicians, you name it. 
This is pretty spot-on, in my experience, but it seems to be an unpopular opinion, depending on the crowd.  

It seems that much like during the dotcom boom, there exists a group of people who manage to *somehow* survive in the industry despite their relative ineptitude. These are the folk who can't even make it through a simple FizzBuzz filter, yet decry the value of a formal education in Computer Science and will vociferously fight the idea of standards bodies or other means of separating the chaff from the wheat. 

My guess is that a large number of those most vocally opposed to being measured are those who know they won't pass muster. Whether they're self-taught or attended some bootcamp or another, they're banking on the fact that they won't be recognized as lacking. They subsist only because they can be lost in the crowd.

While I absolutely understand the difficulty in formally evaluating developer skill, I do believe that most seasoned developers capable of making valuable software contributions can recognize that skill and aptitude in others. It's a hard characteristic to fake. 

I believe the software industry would be well-served in establishing some form of self-regulating certification body, akin to a bar association, in order to not only elevate the profession, but protect it from the worst aspects of the business world, which seeks merely to minimize costs and will gladly play us against each other and send the jobs where they're cheapest.
&gt; [Other research](http://blog.codinghorror.com/separating-programming-sheep-from-non-programming-goats/) points to similar results. There seems to be a ‘double hump’ in the outcome of any programming course between those who can code and those who can’t.

Sorry, but the fact that the OP links to and quotes the now widely-discredited Jeff Atwood post, ["Separating Programming Sheep from Non-Programming Goats"](http://blog.codinghorror.com/separating-programming-sheep-from-non-programming-goats/) doesn't give me much confidence that he's really done any indepth reading on the topic. 

When I say Atwood's post has been discredited, I mean that the paper has been retracted by its author:

http://retractionwatch.com/2014/07/18/the-camel-doesnt-have-two-humps-programming-aptitude-test-canned-for-overzealous-conclusion/


FWIW, I agree that learning to code is difficult, and I currently teach programming at Stanford. But it is not inextricably more difficult than reading, writing, and math, which we are all forced to do for about 12 years in the United States, and yet college students still have trouble formulating basic sentences or calculating a 20% tip without using an iPhone app.
http://norvig.com/21-days.html
&gt; If we accept that programming requires a high level of aptitude, it’s fun to compare some of the hype around the ‘learn to code’ movement with more established high-aptitude professions. Just replace ‘coder’ or ‘coding’ with ‘doctor’,  ‘engineer’,  ‘architect’ or ‘mathematician’.

Most programming is a low-stakes affair. There are no low-stakes architectural projects or surgeries. 

Programming is uniquely welcoming because anyone *can* code. There are no barriers to entry and few consequences for fucking about. If you've got a browser to read this comment then you can be doing FizzBuzz in Javascript within minutes. Do you know how many books you have to read before medical schools will even let you dissect a frog? 

Obviously these little learn-to-code tutorials don't constitute professional training, but nor are they meant to. They're a taste of the mindset - an invitation to explore further. Surgeons and architects have these toy programs too, but when build a virtual bridge, you can't drive your real car across it. 
Substitute "programming" with any other non-trivial field (say, economics, or dentistry).  Does it still sound true when you're on the outside?  Or do you think "Of course I could learn that!"

Whatever your conclusion, self-evaluation of this sort is difficult for humans, and it's worth developing little mental tricks to see if you're fooling yourself.
One big issue raised by this article, that no-one else has mentioned yet.  Is the implicit slight on the entire profession of software that comes along with it.

Not only is it full of terms like "coder", which are rarely (never) used in the wider industry.  It's also the claims that somehow it's a closed world despite having only slightly higher-than-average salaries.

Meanwhile in the real world, there are dozens of professions that are not only harder to get in to, but also directly discriminatory.

&gt; it’s fun to compare some of the hype around the ‘learn to code’ movement with more established high-aptitude professions. Just replace ‘coder’ or ‘coding’ with ‘doctor’,  ‘engineer’,  ‘architect’ or ‘mathematician’.
&gt;
&gt; “You can pick up Maths in a day.”
&gt; Start surgery this year, it’s easier than you think!
&gt; skyscraper.org aims to help demystify that architecture is difficult.
&gt; “The sons and daughters of miners should all be learning to be lawyers.”

For some reason people accept these traditional professions as being elitist.  "Well, to join our graduate training programme you need a first from one of these five specific universities."  What about Bob from the 10th ranked university, who top of his class?  "Well, you see, that university is in an economically deprived city, err, no, what I meant to say was: in our line of business we need the absolute brightest and best and only the top five universities in Southern England can provide them."

This is not only annoying, as well as letting a lot of inherited power remain in the hands of a very small group.  But it's also counter productive if a larger software industry is your goal.  "Well, I went into programming when I learned it was the modern equivalent of coal mining, that literally anyone can do it; I didn't want one of those highly paid professions..." said no-one, ever.
&gt;Or, how about this career: go away, learn some stuff by yourself, we’re not sure exactly what; try and get a junior, low status job, and just learn more stuff – which you can work out somehow – and work your way up. No guarantees that there’s a well paying job at the end of it.   
  
I would add that you should be careful what you learn because, if you find down the road you don't like working in that technology/platform, sorry, but you're gonna take a huge pay cut to do anything else.  

Want to look professional but don't like being well dressed? Do like doctors and scientist: put a lab coat on.
I don't agree with this article. At least not fully. The author assumes that if someone can't hack it at a programming job - or indeed even get a CS degree - that they've failed at learning to code and are incapable.

There seems to be a veil of mysticism around programming that I don't think is entirely justified.

Anyone can _learn_ programming, but only a few can _master_ it, just like any other skill. And just like any other skill, there are those who it comes naturally to, and those who struggle endlessly to reach the top.

There's a large difference between learning a bit of Python to write simple utilities or automate things, and becoming an expert in C++ or Java and getting a $175k/yr job at a MegaTech^® Corp.

It's also unfair and a little disingenuous to compare software development to becoming a physician or a surgeon. They're entirely different beasts. You can pick up _C# For Dummies_ at any Barnes &amp; Noble then go home and practice writing real code that compiles and runs, but you can't practice surgery after reading a medical text. If you cut out somebody's appendix after reading a book about the procedure you're going to get arrested even if you don't accidentally kill them in the process. That's why you never hear of any self-taught neurosurgeons.
I've programmed professionally for over 25 years and one thing I can say is that we love to pat ourselves on the back.

Truth is there are many types of software solutions that can be created by middle-tier programmers.  As experienced as I am I spend a lot of time doing glue-code between Amazon and Google webservices, which has only gotten easier over time.

Most people can learn to read and edit script code, which is often all you need to accomplish certain tasks.

My opinion is that every student should be exposed to programming somewhere along the line because it incorporates other skills as well.  (My kids had a great time learning how to make games on Scratch but I wouldn't say either have picked up the programming bug long term.)

Still the article has a point about setting realistic expectations in the availability of highly-skilled developers.  Most of the really important stuff high-level developers bring to the table doesn't involve coding as much as knowing architecture, framework choices, testing, QA, building, delivery concerns, etc.
We don't need this to be the next "math is hard"...
The funny thing about the expensive bootcamp schools, not the free online stuff, is they often tout that anyone can learn to code, but are actually very selective about their students.  The people I have met who attended them already had degrees in math, sciences, economics, etc., or were already programmers with very out of date skill-sets, and they were all given aptitude tests on the way in.  Yes they do well after attending them and get hired, but that bar is a lot higher then they say it is at those free events and workshops.
One thing I haven't seen mentioned: not everyone on a team should be an all star, having less experienced devs is actually a huge organizational need. 

I'm a fairly senior dev, normally tasked with thing most of our team are not capable of doing properly yet (like designing the **best** architecture for a problem and documenting why its the best out of these 6 alternatives that would also work), or tasked with things where there is a problem and no one has any damn idea what's happening (heisenbugs, for example). Doing these hard things takes intense focus on one and only one problem for sometimes days. Without my team doing the other necessary work while I'm in hiding, I could not address these big concerns and keep reporting progress. Sure, i could I do bits of their work faster, but there is always a limit for one person, and most companies are well beyond that limit. I can still participate--doing a code review on someone else's work typically takes a very short amount of work, and most of our devs rapidly correct common mistakes after a few reviews, so it gets even faster. In short, I don't care if some people will never get to "rockstar", I still want a team with at least basic skills that is willing to show up daily and accept software is a cooperative 
A lot of things bother me about this post, but the canary in the proverbial coal mine is definitely his abuse of statistics:

&gt; Only a third have a computer science or related degree and nearly 42%, the largest group, are self taught. 

Yet this is his (filtered) stats:

Percent | Method
---|---
41.8 | Self taught
37.7 | Bachelors
18.4 | Masters
16.7 | Some University courses
2.2 | PhD

1) "Only a third" 37.7 &gt; 33.3....  minor quibble, but it's a signal his bias.

2) According to the survey itself 52% of respondents have degrees in CS, and 67% have at least a university CS class under their belt:

&gt; There are many ways to learn how to code. 48% of respondents never received a degree in computer science. 33% of respondents never took a computer science university course.

3) The numbers in the graph add up to well over 100%, so it was a multiple selection. Meaning that a lot of people who would answer that they are self-taught may also have checked boxes for university education of some sort. You'd need to do a much more thorough analysis of the underlying data, taking into account what boxes were checked as sets, etc, before drawing any sort of conclusion about it.

No one is going to read this...  too late to the ballgame.  Oh well.
Certainly an interesting read, however, as a non-professional programmer this isn't a view I've come across before -

"Take Object Oriented programming for example. In the 2000’s, it seemed to be establishing itself as the default technique for enterprise programming, but now many people, including myself, see it as a twenty year diversion and largely a mistake"

What's the problem with OOP and what is considered the alternative? 
High aptitude?  nah.

The requirement to being good at programming is the desire to learn programming and the hard-headedness to keep trying in the face of failure.  Most of our job falls in three categories learning more programming, figuring out why its broken and learning how the customer's business works.

You can tell a mediocre programmer from a good programmer almost entirely just by looking at their curiosity and desire to learn.  Do they just copy paste what works or do they take something that worked and then poke it till they know why it works?

It doesn't make us elite hackers born with an ability others don't have.  2 centuries ago half of us would already be dead from our curiosity because we tried to create a wingsuit and fly off a cliffside or something of that sort.  Thankfully now we have computers to keep our curiosity in check.
"TL;DR: All the evidence shows that programming requires a high level of aptitude that only a small percentage of the population possess." Oh God... I don't like how this starts. Sounds like an elitist asshole neckbeard.
Learning to code is relatively easy. Although I'm certain some people just inherently grok it (cause their brains are wired that way) and many others struggle with it. Even after years of experience they have just learned patterns, rules of thumb, etc. They still don't grok code.

[Unless you are one of the grokers you can't even understand what I mean by grok. If you have ever looked at code and thought that is elegant, a thing of sublime beauty, you're probably in the grok group.]

Learning to "code" is a small part of software development. The rest; data structures, algorithms, efficiency, performance, design, planning, deployment, figuring out what not to build, testing, debugging, more testing, organization (of people and tasks), documentation (library/programmers, not user manuals), research, prototyping, estimation, architecting for maintenance... All the things that aren't specific to any language. There really is a fuck ton lot of knowledge someone needs to be a high-level general software developer. It changes, and it largely not taught in school. A reason self-taught is the norm/the requirement.

I wager the people who don't get coding easily, and naturally just don't have the time to learn all that other stuff. They specialize (to reduce the scope of what they need), go into management, hide, or remain mediocre.
Whether a particular person believes coding or software development in general is easy or very hard is mostly irrelevant. What really matters to people who are looking for a job and those folks who are already earning a living in this broad industry is what do the purse string holders believe and want. The trend for the last 10-15 years in countries such as the UK and USA has been to send as much software work as possible to lower cost countries such as India, Philippines, Eastern Europe, etc. When that is not possible, you can often bring the cheaper/more compliant labor from overseas into your country on a work visa.
Too true. I (like other commenters) agree that everyone learning a little "coding" is a good thing. Yet to really be great there is a mindset.

I taught C/C++, VB, SQL, at the collegiate level for six years. There were some students who just barely got through who were brilliant students in other areas of study, including mathematics. Others weren't that great in other areas of study but just nailed programming.

Many universities really push only math majors / minors as the best programmers. While math is important there is just an innate analytical thinking in those that are super successful outside basic "factory" development. Anyone can be trained to crank out little snippets of a bigger project but to truly envision something, think it through, and delivery solid, working software is as much art as science... IMHO.
This is great. I haven't been doing as much programming as I would like to have recently, and the thought that so many people struggle motivates me to thrive.
My opinion is if salaries go up to the level of Doctors, Accountants  and Lawyers with similar job security then the smart people who become doctors and lawyers will become coders. In reality programming can be brutal - people end out of a job at 50 and no one will hire them as people have either moved to offshore or the coolest coding language.

Those of us who should have picked a more highly paying career do it mostly because we like it.
I'm a computer science student. On one hand I absolutely agree with this. On another hand, I don't agree with it at all.

I agree that computer science degrees do not prepare students for a programming job if they are not passionate enough to teach themselves a lot of skills on the side. I also agree that it's misleading to market code academies as though they will make someone a great programmer when realistically they're only providing people with training wheels to get started. However, I object to the idea that the purpose of a computer science degree is to land someone a job as a programmer.

For the sake of brevity, I'm going to put some stuff down in point form (in no particular order).

**Things I don't think a CS degree will explicitly teach you**

* How to be a software developer
* How to take initiative to learn new things
* How to program *well*
* How to survive in industry, land a great job, or make lots of money
* How to have good networking or social skills

**Things that I think make a computer science degree worth it (at least for me)**

* An introduction to computers/computing from theoretical, academic and scientific perspectives
* The chance to get involved in research (including interdisciplinary research)
* The opportunity to take electives/minors and broaden your intellectual sphere to include other fields
* How to stumble your way though the command line and write some small programs (i.e., you will learn how to get started with programming, but it's up to you to do everything from there)
* Algorithms, data structures, and a broad understanding of various programming paradigms and languages (e.g., functional vs. imperative vs. object oriented, type systems, etc)
* Experience managing the stress and complexity of an undergraduate degree
* The opportunity to meet (and work with) people studying very different topics (e.g., philosophy, gender studies, biology, marketing, etc)
* Upward mobility in academia, in case you ever decide to go back to university for a professional degree or graduate studies
* The college experience, if that's something you want
* Exposure to different fields and the opportunity to make more informed choices about your future (e.g., I started in neuroscience, but after becoming exposed to both neuro and cs I discovered I really enjoyed cs more)
* The opportunity to network with people from all different walks of life, who will wind up having careers all over the world
* The opportunity to work your way into specific areas of computer science that often require an academic background (e.g., if you want to do research in machine learning at Google, having an academic background in applied machine learning can be an asset)
* *Some people* learn how to be good programmers during their degrees as evidenced by upper year courses where they implement their own compilers, programming languages, or complete other challenging programming feats

None of this is to say you *can't* achieve many of these things on your own (or in industry). Only to say that it's reductive to pretend that a computer science degree is simply a "programming degree". It really depends on what you want and how you learn. I would discourage people from getting a CS degree if they want to be a programmer and don't care about many of the things I mentioned above, but I think there are plenty of people who go to school for reasons other than landing a job. With a computer science degree and some motivation to work on side-projects, you can have good career prospects and still benefit from the many other positive aspects of getting a degree.

A computer science degree is not a professional degree. It is not an engineering degree. It's a science degree. You don't see people complaining that psychology degrees don't prepare people well for their future careers as therapists or that anatomy degrees do not prepare people well for their future careers as doctors (or at least, you shouldn't). There is a reason why we have distinctions between academia and industry and, while I'm often for cross-pollination, there needs to be some level of respect for differences in philosophy between the two branches. They both have different aims, measures of success, and strengths.
There is no programmer shortage.
Lots of people can learn to code, but how many people actually enjoy it?  Passion and drive are a far better indicator of performance in the real world.  I've seen mediocre people either get really good, or just stay that way, and the difference is usually how badly do you want to be there.  Being a bad dev isn't the end of the world, but it takes work to get out of that stage of your career. 

Sadly in this day and age, software engineer === fat paycheck and everyone wants that salary without becoming a dedicated engineer or investing the time.  10k hours is a real thing.    
&gt; TL;DR: The current fad for short learn-to-code courses is selling people a lie and will do nothing to help

**TOTALLY FALSE**

I just took a job where I am undoing the work of 1 of these coders. It's going to take me much more than a year to undo because I'm now operating on a live system that can't be taken down.

These coders:

1. produce more work for me -- job security!

2. when I fix it, they make me look brilliant!

I'll say this as a 3rd level comp sci student.

A lot of people just memorize code and what that code does without understanding what's going on, and they get away with it quite a lot within our current education system. (well at least within my locale's education system)

A frequent stumbling block I find with people who I help (and I help a good amount of people) is that they don't understand what order the compiler will parse their commands.

This can often wreak havoc because you'll tell them what they need, they'll understand what they need, but they have no idea how to put it into the existing code.

I'm amazed that modern exams and testing don't have questions like; "Complete this code sample". Asking people what data they need to put into a variable to get a result should be a thing, because it cuts out their ability to just memorize a block of code and reproduce it.

We really need to shift the focus away from just forcing people to mindlessly memorize syntax, to knowing the mechanism to produce a result in my opinion.
It makes me chuckle that some comments here compare programming to art. Makes me wonder if the profession is not over-hyped and with a lot of inflated-ego people.
If I compare writing software to let's say civil engineering, I'd say writing software is harder some ways (constant learning, churn etc) but easier in lots of ways as well (sitting all day in the aircon, you can work from anywhere etc). 
Some variables I think the author is overlooking:

The average programming job is getting easier, not harder.

Thanks to frameworks, adding value to a software product requires less brilliant programming than it ever has (and again, getting easier).

Most universities are honest about the supposed gap between skills of graduates and the skills employers want. A good portion of a CompSci degree is designed to teach you theory that applies across languages/decades. That does make it less practical to your first job, but not necessarily a bad idea.

A large part of programming expertise is problem solving expertise. This suggests a) that general problem solving acumen will likely translate from other walks of life b) that making resources available to help people learn languages could allow them to apply problem solving skills to software development and c) that programming can get better (and expertise developed) with practice.

I wish more employers were interested in closing the gap themselves, on the job; but teaching people to code (and starting those initiatives early in life) seem like a good approach as well. I guess time will tell.
This myth (programmers are in short supply) is based on an economic misunderstanding.  

High profit typically indicates a shortage, therefore high profit in software *must* indicate a shortage of software being created, right?   

Wrong.  That's not how the economics of software works.


&gt; Even programmers with CS degrees insist that they are largely self taught

I keep seeing this tired platitude repeated. Computer Science is **not fucking software engineering**. Where did these poor fools get the idea that a CS degree will make them a software engineer? 

CS is an offshoot of pure mathematics that developed in large part due to the entscheidungsproblem and related work by people like Godel and Turing.
"What are you talking about? You literally just type out what you want. HOW HARD COULD THAT BE!?"
This article is saddening. It's purpose? To discourage young people from discovering and curios perusers from breaking out of their boxes.

Let's not over complicate this. ANYONE can program; it's not rocket science and it's not art. Stop over convoluting something that a monkey can do. I hope you guys feel special.

This is coming from a person that has been programming his entire life.
coding is difficult not only because it requires an understanding of technical jargon, but because there are many, many parts to it...

coding alone is a useless skill...

in order to build any kind of web app you need to:

1. Know how domain names work (dns, mx, a records, cname etc..).
2. Know how servers work (apache, linux, varnish, nginx, cpanel, CDNs etc..)
3. Know a programming language (ruby, php, java etc)
4. Know html, css, javascript &amp; jQuery (and all the new javascript stuff like angular / react)
5. Know how databases work (sql, tables, schemas, primary keys, foreign keys).
6. Know about SEO.
7. Know about basic security (encryption, sql injection, ddos, brute force logins etc...)
8. Know how all of that shit above fits together and makes a real / life / visible website or app or something useful.

Then there's all the soft skills needed - you need to be able to present, persuade, articulate the reasons why a business should or shouldn't use certain technology, what problems may arise, what information is needed before we can start to even thinking about designing and planning, let alone writing code etc...

I've yet to meet any developer who knows everything about technology and coding / languages / insert technology skill here. One of the biggest personality traits i like to see is someone who openly admits they didn't / don't have a fucking clue what they're doing but hacked their way to building something useful / impressive. That tells me that when their motivation is great enough, they'll find ways to learn and make things work. That trait coupled with a natural passion for technology and a working knowledge of all of the above (and ideally expertise in few areas) is what i'd consider to be a decent person to work with / hire.
I think pixar was onto something with Ratatouille. 

&gt;Not everyone can become a great artist; but a great artist *can* come from *anywhere*.

Not everyone who tries programming is going to take to it, but that doesn't mean we should stop encouraging everyone to give it a go. No one should ever feel like they can't learn something simply because they couldn't get into the right school, or because they came from the wrong background. 

Is there any other profession being pushed on everybody as hard?  Besides being told to join the army this is like the second most common thing I have seen instructing people on what they should do careerwise.  Nobody saying learn to sell crap or learn to operate on patients or learn to write a legal brief.  
So how does one become a 'good' coder?
I think just exposing people to what programming can do would have huge benefits for every single industry on the planet. Software engineers typically do not see problems faced in countless other industries that could be solved by software, and people in those industries typically do not immediately jump to the conclusion that things could be better if they had X software tool. Imagine if your marketer could make himself more efficient by building custom scripts instead of copy pasting between excel sheets. Or if your physics teacher could illustrate concepts by building simple simulations in a game engine. In every industry there exists highly specialized people that could build amazing software because they have domain knowledge that others do not. I really think its about time we started enabling these people. You really do not need to be great to build something useful
I'm learning more on my own than through my CS degree that I'm trying to get now.  Furthermore, that degree is taking a considerable amount of my time, on top of my work and family.  Sometimes I wonder if it's even worth it, considering I can learn more in that same amount of time I invest into my degree.  I just keep on pushing... so we'll see.
The only reason I am a software engineer (and decent at it) is because I wanted it more than anything else and I was willing to do whatever it took to become one. I taught myself. I went to school. I practiced. I worked professionally. I never cared about money or careers, though it seems that I lucked out.

The thing is... software development is HARD and it takes LOTS OF HARD WORK to get good at it. I suspect that is what really turns most people away from it. But, people will give various excuses and blame as to why they tried to be a programmer and why it couldn't possible work out for them.
I just wish more people would be exposed to programming so that when I told them about what I do their eyes wouldn't gloss over.
Not sure if coding ability is innate as the author suggests, but it definitely is harder for some to pick up than others. Persistence pays. A person who really wants to learn to code well can *if* they have a strong desire to master the skills. Some will have to work at it much harder than others. And some of them may decide that they don't really want to learn to code all that much after all.

I had been trying to convince my wife that she'd be good at programming for several years. She's detail oriented. She's a good logical thinker. Finally after several years of my suggestions she decided to take a Java class at a local community college (not my suggestion, I think she would have been much better off starting with something like Python). But I don't think she understood any of the concepts. She would ask questions that were kind of baffling for a programmer (we're fish in the water so we don't see a lot of these things) "How can you say x is equal to 5 but then say it's equal to something else later on?" type questions. And then she had some good critiques like "why do I have to type a semicolon at the end of every line? This is just ridiculous!". She stuck it out for the quarter, but in the end she decided that she found programming just really frustrating. I think part of this was the instructor and part was the choice of Java. I can't help wondering if she had taken something like Python first if things might have gone differently, but at this point I don't bring the idea of learning programming up to her anymore because she has no desire to learn to program after that experience. 
I mean, it depends what we all mean by "code," right?  Sure, most people don't have a large-scale application in them and it's not reasonable to expect everyone to learn any more than it's reasonable to expect everyone to learn any other trade.  On the other hand, even the most basic programming skills (I don't know, VBA or something) could make many people's lives better if they knew them.

I also kind of object to the part saying that a CS education is only a small part of what you need to know.  All of my CS education is informal, but once you understand CS concepts they're applicable over and over again in a way the framework du jour is not.
&gt; all these courses saying ‘learn to code, it’s easy’ just reinforce the perception that software development is not a serious career.

They don't "reinforce" that perception; they are a manifestation of that perception.

Many business people with "ideas" think they can just hire a bunch of junior developers to "code" their idea.

&gt; Oh, and, by the way, the whole world will think you are a bit of a social pariah while you are about it. Which would you choose?

Yea well, that's partially true. At least it certainly applies to me.
I did a CS degree (with a SENG emphasis, before you could get a SENG degree), and I knew coming out of it that I couldn't really code that we'll because I didn't take the time outside of school to develop those practical skills.  
However, this allowed me to step into IT with better qualifications (in 2009) than many other candidates.  (And do work I much prefer to do)
The general overview you get from a CS degree has been great for helping me understand how things work under the hood, and having some experience putting software together has put me in a great position to capitalize on the growing DevOps trend. 
All of this has made me sure that it takes a particular kind of person to enjoy the deep, narrow, long term focus that software development requires. I don't think it's something you can teach. 
I think there will be a lot of CS graduates coming over to the IT side, provided they can put aside any preconceptions/opinions about being in "IT"

It really bothers me that there is a sentiment in my industry to minimize my profession and the many many years of experience that have shaped me as a coder. Anyone can be a coder... sure. Should everyone be a coder? No. That's like saying anyone can be a laywer or a doctor or a physicist or a pilot etc. Maybe, but it's hard work and not everyone is cut out for it. This concept of having a shortage of programmers is bs. What they mean to say is, we don't want to pay you what you are worth. By increasing the supply of labor, that can then put pressure on lowering salaries. Just let the invisible hand of supply and demand sort it out... like with all other industries.
I wish people would stop referencing the draft of a paper put out by a professor at MiddleSex University about "double humps." The guy who wrote it said he was not able to reproduce his findings.
I understand the frustration with this type of government promotion from people in the industry.  But from the government perspective it works REALLY well.  It is the equivalent of a very well done blast email marketing campaign.

The government has recognized that demand is higher than supply and the deficit is increasing. By saying that anyone can do this, everyone needs to try it for an hour - what they are really saying is that we recognize the need for x new qualified programmers in the next 10 years.  If we expose x^10 to the idea, we can fill the need for x. 

A similar promotion happened a decade ago across the pacific northwest for dental hygienists.  There was a massive shortage and colleges were producing very few graduates. Young females were targeted and told of high pay, flexible hours, could live anywhere...   Today my state has an excess of dental hygienists and will only average 51 new jobs per year for the next 8 years all of which are due to hygienists leaving the industry.   Pay has been stabilized and a community need has been met. 

In the case of programming, the government's marketing campaign is offensive to those that have spent 10,000 hours learning the craft.  But if you strip back the marketing and look at the core mission of the program, the government has developed a workable solution to a problem they have identified.  Even if a very high percentage of people that are exposed to programming are never able to enter the industry. 
Currently in the CS program at my uni. I didn't start programming till this Spring, and it's still not something that's easy to grasp. And I've been struggling in my classes. 

Because the biggest thing in programming is that it's not just something you read in a book and say "Oh, I got it."
It takes a lot of outside learning to completely understand. 

I'm sure what the people in the "CS major, but self taught" mean is that there was a lot of outside work to apply to the major. The major (at least for me) doesn't teach you so much as give you some basics to look and find the info you need to complete the assignments. You can't just look at lecture notes and say "I know how to program." 
No. You have to practice. Constantly. Like art, it's not something you just read in a book and know how to do.

And I think that's where some people fall off if it's not because they can't pick up the mindset for programming (thinking of things algorithmically(?)). They fall off when they don't take the time to apply what they learned and make things to practice their skills. 

Everyone wants to learn how to draw, but not everyone wants to put in the work to practice the skill of drawing and realize that mistakes happen, they're supposed to happen, and it's ok to make mistakes, it doesn't mean you're bad. 

It's something I'm still coming to terms with, but I'm in the previous paragraph of understanding that it's ok to make mistakes, that doesn't mean you should stop practicing. 

It takes a lot of work and dedication, but I think it's definitely a skill to have if it's something that interests you. 
Trust me: &gt; 95% of us Indian developers are just programming to get paid. They don't have the aptitude for it, they just force themselves into it, learn it somehow, cram it and jam it, and then hammer out super spaghetti code. Maybe even good code sometimes. I've heard examples of every kind. 

Girlfriend of a friend of mine took up ASP .NET, when she couldn't find a job from her major after graduating. So she "took to programming", seeing how my friend was making good money in a startup, from his .NET job. She completely hated it, but after about 4-5 months and 2-3 small but good projects to show off, she got a job. She fucking got a job.

Same shit with most of mates from college. Fuckers didn't know how to compile a java program from the command line, even by final year (and they were in B.Tech of CSE program). Now they're in Wipro, Capgemini, Virtusa, CGI, HCL, Infosys, Deloitte, Dell, etc. Annnnd they're devving. They're devving hard, in C++ to Java to one of the fuckers coding some crazy shit in Erlang even!! Heck, I was the class' "programmer", and I tried out Erlang a couple of weeks ago and couldn't get a hold of the event driven programming model to solve problems that I usually solve with OOPs in Python. But he writes Erlang code, the same fucking guy who used to tell me, that it was only for his parents that he was doing B.Tech in CSE. He absolutely hated programming and all subjects in the 8 sems we had to go through. 

Now he knows more than me about compilers and networking and redis and event driven programming.

The author is right in that it's not something you pick up in a week or a month even. You gotta have an aptitude. 


Maybe my friends have an aptitude, which was hidden all along. Or maybe just the pressure of getting *some* job, to have *some pay* pushed them hard enough to *develop that aptitude*, if that's even possible.
Very well written. Thank you. 
Reading charts also seems to be a hard skill. People with degrees: bachelor 37.7%, master 18.4%, PhD 2.2%. That makes 58.3%. Definitely not "Only a third have a computer science or related degree".
I think that anybody can learn to do anything and anybody can improve to almost any standard at whatever they want to. However that in no way means that the difficulty level of these things is the same for everybody. Some people are talented in mathematics for example and can go through High school at least with little effort and get an A. Anybody else can get an A though it's just a case of more effort for some.
Can anyone ELI5 me what he means with this?

&gt;  Take Object Oriented programming for example. In the 2000’s, it seemed to be establishing itself as the default technique for enterprise programming, but now many people, including myself, see it as a twenty year diversion and largely a mistake. How quickly would programming standards and qualifications stay up to date with current practice? Not quickly enough I suspect.
I think it would be useful to make the distinction between developing a complete software project as part of a team vs hacking together a useful script for your own use. I would imagine that most people could learn how to write a simple script (eg, not using functions or anything like that, just some basic flow control), but the professional knowledge and discipline required for large projects (using source control, testing your code, spending time thinking about design / architecture, keeping track of coding tasks using some sort of issue tracker, etc) is something else. There's learning the how, but also the why for all those things. I agree with others that it's more of a practice thing than an aptitude thing, which to me comes down to interest - you have to enjoy programming from the start to have the patience to keep practicing, and then you have to have the ability to overcome the ego that programming can give you to realize you need the disciplines of modularity, tests, documentation, etc.
&gt; one that finds programming a relatively painless and indeed enjoyable thing to learn and another that can’t learn no matter how good the teaching

I'm not sure those two things are mutually exclusive... I've been paid to program computers now for nearly 25 years, but I can't say that learning it was "painless" (relatively or otherwise).  For me, it's more of an obsession... I'll bang my head against a brick wall for a few days trying to understand something, give up in frustration, stew about it for a little while, jump back into it with a vengeance, wallow in misery and then finally realize what I was missing in the first place.  Painless?  No, not painless, any more than climbing Mt. Everest is supposed to be painless.  Worth the pain.
Using R in college got me started. Writing little scripts and seeing my logic work got me kind of excited. Alums would say 'Learn python, just go and learn python!' so I went to codeacademy, took the python course - neat. After graduating I set out to look for a project and find out what I can do for businesses.

I watched a few videos on how to build simple web crawlers/scrapers, OK, I don't know anything about programming but at least it's interesting! So I started an account on oDesk, bid for a job, got it, worked for several months, made lots of mistakes, but I learned a ton. I worked for several more businesses, learned about web development, picked up more tools, made more money. Whenever I'm working with another dev I pick their brain.

It began as a part time gig, but it's slowly becoming what I do. I'm driven entirely by curiosity and a sense that what I'm doing is science fiction, and I'm living in it. I almost cannot believe that I can walk to cafe, write up some code on my laptop, run it from a server hundreds of miles away, and have it produce a CSV file containing everything business #1 needs to know for the day. I can make bots, AIs, games, websites, algorithms to identify most probable k-mers in a sequence of DNA. If having that power doesn't excite you, writing code probably isn't for you.

I appreciate the author's frustration with people who think programming is easy, but I generally disagree with the premise here.

It's true that innate ability is a real phenomenon, but the biggest factor in reasoning ability has been proven to be exposure to that type of thinking at an early age.

The good jobs of the future probably will rely on the sort of reasoning programmers use, so I think it is perfectly wise to teach these skills to people in general, in short bursts, self-guided lessons, and all manner of ways.

Certainly some people will excel, and others won't.  We still try to encourage kids to exercise, whether or not we think they have a shot at the NBA.

From the fact that many programmers are self taught, I draw a different conclusion from the author's.  It suggests to me, as my own experience shows, that there are interesting and potentially lucrative things a non-expert can do in programming.  If there weren't useful, enjoyable things to do with our developing skills, none of us would have stuck with it.

My path to expertise (which I'm still in the middle of) would have been much smoother if I had taken an occasional one-day or one-week course from somebody with experience and insight.  I'm glad those kinds of courses exist.

There are parts of my job that give me all I can handle, despite years of experience, but there are also tasks I'd happily trust to most any bright person with a week's instruction.
The author used results from the StackOverflow survey to support his point that many developers are self-taught or don't have a degree; only problem? They don't sum to 100%, so they don't show what he's claiming they say. Each category can only be compared to the total number of respondents, not to each other. I.e. some of the people who are self-taught also have a degree.
Like a lot of other commenters here, I believe that even a small amount of programming knowledge is useful. Hell just understanding that, from a very macro level obviously, programming pretty much just involves editing the right text files helps people understand what it is we're actually doing at the computer. 

Understanding that alone has gone a long way in helping my no-programmer team-mates recognize what I do and how I do it. Programming is not magic and more people are capable of doing it than this article suggets. 

I would say the real challenge is reaching those who could but never know it because of their environment or situation. 
These sites are not helpful for new programmer but I think they can be useful for someone who programs already and wants to pickup a new language. 
This makes me feel good, but I think there will be a huge shift in perception as new ways of programming evolve (more graphical editors, etc) and it gets taught more in grammar school.
I don't agree with any of this.  Perhaps there are people that can't learn to program, but they are almost certainly a small group.  I think the problem is that programming is just different from other stuff you learn in school.  You might get something wrong 20+ times before it does what you want it to do, there aren't many other subjects like that and I felt like my courses did a poor job explaining that.  Once students get the mindset that fucking up again and again is totally ok they can become competent coders.  Comparing it to surgery is really dishonest, it's not that complicated for most of us lol.

Also there are both abstract thinkers and concrete thinkers who do well in software development, a lot of instruction will be more appealing to one type vs the other.
IMHO skill diversification can give you a good leg up.  Too many people think that it's simply write some code, ???, and profit.  Not everyone can be a programming machine and genius at algorithm design.  For those people that is their selling point.  For the rest of us, it helps to have soft skills, business knowledge, or a specialty such as biology or design/graphics.  When you finally land that entry level job or internship it helps to learn the industry/product/skill/whatever outside of a development perspective.
As a recent CS grad I found it difficult to explain to my co-workers I'm not just the "IT Guy" as they all bother me with literally every hardware and software issue that comes about. I also do agree that coding isn't "natural" as I'm one of those CS grads that found programming extremely difficult but I was able to do the weird CS math i.e finite automata, algorithms, linear algebra, discrete etc. I'm still teaching myself practical programming as I was highly fearful of going into the workforce and being incompetent. 
&gt;  Take Object Oriented programming for example. In the 2000’s, it seemed to be establishing itself as the default technique for enterprise programming, but now many people, including myself, see it as a twenty year diversion and largely a mistake. 

What is proposed as the alternative to OOP? Functional programming? This is the first time I've ever seen it written off as a mistake. 
I would apply the same for mastering any discipline; the dedication and opportunity to devote a lot of knowledge someone needs to be a lawyer, but for fun I psuedo-coded a solution in notepad with just a few K a year by hiring a mediocre programmer trying to intuitively understand quantum mechanics, relativity, or variational calculus - that's what you were struggling most with was how to paint.
Rather than having student start learn about coding in high school, its starting as early as elementary school. If they develop and interest they can start typing out some code in middle school.  

Coding ten years back the learning curve was much steeper, there were far less tools, materials, and websites. I think I was handed a C++ book and told to copy code. That was it. Now its games, videos and whole lot of fun stuff. 

But K-12 education isnt about specialization but preparing children for a whole range of potential problems they might face. Code.org is never going to create any software developers on its own, however, if a student does want to get into coding its the first breadcrumb on a very long line of breadcrumbs that leads to that CS degree. 

He shows the "surgery" example well yeah you aren't going to teach surgery to kids however they do teach anatomy, biology, and health in primary schools as well. You go into college you are going to be taking more in depth biology courses and anatomy courses as a prerequisite to becoming a doctor.  

Personally, I think coding, programming and software development is just a ton of things that are meant to be easy and logical. The problem is there's a metric fuckton of things that you need to learn (as opposed to an imperial fuckton). 
In my opinion:

It's easy to learn to code. Learn the syntax of the programming or markup language you chose and you are good, you can code.

The hard part is building efficient algorithms for the problems you need to solve in the process of building software.

No one can teach you how to think.
I completely agree with his conclusion about natural aptitude. I took an intro programming course as a *required course* while completing my degree in Biochemstry. This is usually a pre-med major and many of my peers were 3.8-4.0 GPA students destined for medical school, yet most of them couldn't program if their life depended on it and had to retake the intro programming course. Keep in mind this was a Python course that only covered basic loops + variables. 

This experience has lead me to believe that programming requires a certain type of brain that only some people posses.
I think it's a little different if you're self employed. I make apps and I know I'm not a great coder in terms of knowledge but I get by using resourcefulness and learning what I need when I need it. I know little bits of C# and use Basic4Android for apps so I'm pretty much unemployable but I've had some hits nevertheless and am doing okay :-)

Coding involves all sorts of different skills depending on the nature of the work. Someone who could never be a full stack developer or work in a big team might still be great at solving coding problems creatively, at reverse engineering, at algorithms... Those people might fail course but not be 'bad programmers' in every respect.
Is coding really an aptitude thing? In my experience it was more of a "are you willing to put in the effort and attention into learning it" thing. To me it was a lot like learning to play guitar; it was very difficult at first even though it wasn't really complicated per se and it wasn't particularly fun or easy to see your progress, but after the initial hurdle you get good at it well enough that you can slowly and incrementally improve your ability over the years until you wonder why you had any difficulty with it in the first place.

Of course you can't really learn to code quickly and new learners should know about the steep learning curve at the beginning. But I think part of the reason why those marketing gimmicks work is because people want to be able to get running as quickly as possible so they can feel like they're accomplishing something in the first month or two. I think a little delusion is actually helpful at first and one of the best teaching aids I got early on was a very simple graphics library given to me before they went too in depth in doing text and IO. This gave me the immediate satisfaction and feedback that I was actually making a computer program that kept me going. I also read a book that said it would teach how to make games, which it totally didn't, but I had to read the entire book to really figure that out and by then I had already been tricked into learning how to program.
Yes, yes, only a small percentage of the population has the intelligence to be really great programmers, but  I would argue that programming is a little like literacy.   Yes, most of us are never going to write shakespeare-level masterpieces.   That doesn't mean we won't get benefit from being able to scrawl out short notes.      

I mean, yes, training everyone with the expectation that they will become Shakespeare is obviously a losing proposition.  But trying to get most people up to the level where they can write a semi-coherent cover-letter, or at least scrawl out a grocery list, and letting people who have the aptitude go on from there?  That's a reasonable goal.  

Same goes for programming.   Yes, most people don't need a lot more than how to make a spreadsheet and do simple procedural math and text processing... but even simple spreadsheet programming is kind of like having a superpower.      Further, I would argue that being able to handle a basic programming language, something like BASIC, or more recently, Python, is a good grounding in procedural thinking, which is useful when planning other tasks.  


So yes, most people aren't going to be able to contribute to the Linux kernel, but most people *can* come up with a basic spreadsheet, or do basic text processing tasks.   
I honestly found coding to be a very strait forward and easy thing to learn. I didn't become good over night, but it wasn't the very challenging to become good enough to pass as competed. What I find hard, is writing large pieces or software, and theory. 

One of my mentors put it best

&gt; Programming is easy, writing software is hard, coming up with theory requires a PhD, and even they aren't that great at it.
One swallow does not a summer make, nor one academic paper an established truth.
CS degree-holder here, though its vintage is bit long-in-the-tooth, perhaps. I graduated with a degree in CS from a Big Ten school (solid middle-of-the-pack, to those of you outside the US) in the early nineties.

I learned very little in university that was useful for coding on the job. Practically nothing, in fact. My first job was with Andersen Consulting, now Accenture (with a little doohickey floating above it, I guess). They had a very thorough six-week training course to teach you how to write 3270 COBOL, which was actually the most practical training I had received.

I consider myself a reasonably skilled programmer now, some 20-odd years on. 99% of that was self-taught and on-the-job.

I don't think it's difficult to learn to hack together some statements in the right order to accomplish a thing. I agree that pretty much anybody could learn to do that. I think what's very difficult is to be able to build an enterprise-scale application with 100k's LOC (lines of code) that is multithreaded and scalable. That skillset is very rare, and I have known many programmers, professionals, even, that have no clue as to how to go about fixing performance problems in an application or managing multiple threads properly.

So, yes and no: anybody can learn how to code like anybody can learn how to drive. But not all drivers are equal.
I agree more people should be exposed to it from a young age, but there will always be a skill gap. Even people who love coding and have all the required training can fail miserably when exposed to professional environments. I see it happen all the time. The fact of the matter is that being a professional developer requires such a specific set of skills AND certain personality traits (masochism, for one).  
  
I think a lot of this may have to do with it being a "new" profession and our tools and methodologies are still immature. That makes it particularly hard to learn. Maybe one day it'll become a more normal field
&gt; Take Object Oriented programming for example. In the 2000’s, it seemed to be establishing itself as the default technique for enterprise programming, but now many people, including myself, see it as a twenty year diversion and largely a mistake.

Is he talking about Functional programming here? Looks like I have something new to learn...
I think the problem with the perception of coding being easy is how it's taught outside of universities. They teach you languages, but they don't teach you how to use it or how to code; it's akin to the difference between teaching someone how to use various power tools and expecting someone to use those power tools to build a house.

Before I started studying CS at my university I *hated* coding. I felt like I had no idea what to do with the syntax I knew. When you do things like codeacademy or whatnot for python, you don't learn things like recursion or iteration or tail calls, you learn how to define variables and write if statements. I think if we taught people things like SQL, languages that are simple but have a lot of utility, it'd be more useful than a few lines of python that won't get you anywhere.
&gt; However, my main point in this post has been to establish that programming is a high-aptitude task, one than only some people are capable of doing with any degree of success.

I couldn't disagree more. On the whole, people are much smarter than given credit for and I'm pretty surprised someone could be so arrogant. I would, on the other hand, argue it requires dedication and willingness to spend lots of time on a computer in thought. That does not appeal to everyone.

Source: I'm (hopefully) of average intelligence and have worked as a software engineer at reputable tech companies and co-founded my own.
Yes, it is.
Hey if you lack ideas and can't make money as an agency, you may as well take money from bootcamp applicants.

Those that can't do?

&gt; This from the Guardian is typical: “It’s the must-have skill-set of the 21st century, yet unless you’re rich enough to afford the training, or fortunate enough to be attending the right school, the barriers to learning can be high.”

I didn't realise documentation was so inaccessible.....
Whats the maths aspect of coding?
Learning to code is only hard if you don't have the mindset for it and have to train your brain to think in different ways.
Reminds me of all the self defense/ju jitsu classes that promise results in a weekend. Sorry, no. 
&gt; The current fad for short learn-to-code courses is selling people a lie and will do nothing to help the skills shortage for professional programmers.

Professional programmers, as in a guy who spends most of his time in IDE - probably not. (As a side note, universities are producing bunches of the sort "oh, I know Ctrl+C - Ctrl+V, and can google the rest. Sure I don't really see what's the point, but I would be able to buy a house and marry X). But there's also multitudes of others who would be able to dramatically improve their workflows by learning how to write a most basic macro and not being afraid to do it.

I think before we can tell people to learn to code. I think we need to define what learnt meant. I know of several people who claim they can code and have learnt. Yet cannot solve any basic problems in the languages they have learnt....


Especially with ES6. I disagree with a lot of the syntax they're introducing, but I guess it's been finalized, unfortunately.
I see programing kinda like a language, sure you can learn it but unless you apply it daily, you aren't going to get good at it. 
To me (with 18 years of coding under my belt) the best thing you learn from programming is not to fear failure. It comes hard, fast, and frequent. Getting over being shitty at something several thousand times is incredibly helpful for recognizing patterns and mapping your experience onto other areas of study.
For those few who can really code ... we struggle with inadequate and constantly changing requirements.  You need to have the right frame of mind to succeed.

A 2AM Data Warehouse ETL failure is not something that is covered is academic settings.
"The implication is that there is a large population of people for whom programming would be a suitable career if only they could access the education and training that is currently closed to them." Its ironic that he says this while speaking about the ingenuity of the world wide web. If someone wants to learn to code, the resources are there.
I studied philosophy and economics in college, but took a couple computer science courses.

For the past six months, I've been writing code for a living in San Francisco, after spending six years in an unrelated finance career.

I got here because I got my feet wet in college. Later, when I wanted to make something happen automatically, I programmed it (using Visual Basic for Excel and eventually Python and Javascript).

It's hard to predict the impact of a course or two.
The idea exposed in the article is not completely wrong but it is a little biased. Several other variables emerge from the article:

- the possible fear of existing programmers for their profession to become more common and therefore less paid/appreciated. I get this feeling when the writer speaks about the fact that only a certain percentage of people display the (innate?) aptitude for programming: fine but do we know this percentage already? Is it not better to try anyway to expose as many people as possible to programming and see what comes out of it? Are we really so terrified of hurting them with "failure"?

- the quality of the education system might be the real reason why many who try to learn ultimately fail. Even more so given that the article is mainly based on observations relating to one country only, the US. Programming is a complex task, it would be very easy for a bad teacher to demotivate even someone born with the required talent.

- a little unconfessed category pride: a small non-written rule of the programming ecosystem is that programmers are expected to be self-taught... if you need someone to teach you, you are not a real programmer. This is of course not true and counterproductive, like almost anything that includes too much pride and ego.
It's a good idea to teach everybody how to specify a process just like we teach everybody how to solve an equation. It'll make our society smarter, and I see no reason why any user can't be a power user creating shell programs, cron jobs, user styles, JavaScript bits and so on. It's a terrible idea to hire anyone as  a professional programmers because it's cheap, though.

However, I can't help but dislike the fact some people want to be as protective as to strive for making formal education a requirement, or claim that you can't be good if you're self taught, or claim that we shouldn't be teaching people programming for free, and so on. Listen: if you're worried that I may teach somebody programming for fun and this person will take your job, then you must be a pretty shitty programmer yourself.

There's no way I can teach all I know and transfer a decade of experience and my skill as a software architect into my cousin for giggles and shit. If I get somebody into programming, and years later he's is so good I want to hire him for my team, I can guarantee it's not because somebody taught somebody else some imperative programming. And if I choose this person over you despite you having a university degree, tough shit; don't think I place much value on a degree. It's all about my desired level of knowledge, skill and experience, degree or not. A degree is just a way to guarantee **you**, not me, that you've got a very good *starting point*. It's not a green pass for a job — tough shit again; I'm so not sorry.

FWIW, I do have a degree in IT myself, it was a good starting point, and my knowledge and experience from those years represents roughly 1% of the value I can deliver to my company today. Out of the people I've had the pleasure to work with so far, I can easily point at the three I professionally respect the most. #1 has unfinished university education in electrical engineering, #2 has a degree, #3 is completely self taught.

I do agree that this sort of statement is ridiculous (from the link):

- “You can pick up Maths in a day.”
- Start surgery this year, it’s easier than you think!
- skyscraper.org aims to help demystify that architecture is difficult.
- “The sons and daughters of miners should all be learning to be lawyers.”
I am learning to program at the moment, but I feel like it actually is really hard to learn for me. I thought that I could just put more effort in it and still become a good programmer, but according to this Text I am just wasting my time at the moment since I obviously don't have the aptitude that others have. 
Should I just stop trying to learn it and spent my time with other things instead ? 

I mean like I understand the programming language and I like to think logically, but it is really tough for me to understand all the computer stuff around it and how computers and networks work.
I'm the assistant to the Headmaster at one of the best IT schools in France. Here's my two cents.

First, I think that no, not anyone can become a successful developer and make 60k+ a year. But here's the trick: the industry doesn't ONLY need 60k+ developers. Most of the time they need maintainers on stuff that doesn't require a high set of skills.

Also, talking about artists is pointless. True, singing an opera requires talent and if you don't have "the gift", you can't become an opera singer. You can't learn that. Singers train their voice but they have something to begin with: a (good) voice. But it's also true that anyone can learn the guitar and the piano, regardless of a "gift" they could have. It's just going to be easy for some and difficult for others, but unless you don't have arms ([oh wait...](https://www.youtube.com/watch?v=2MJ-NeXRcEk)) you can learn to become a very talented player.

I truly think coding or programming or whatever you call it can be learned. It's just a way of thinking and you can learn that too.

At our school, we don't have teachers. We don't have classes. Students work 100% of the time on projects. They're given a problem, a few hints, and they have to work together to find the solution on their own. We are here to follow them in the process, to answer their questions (without ever giving *the* answer) and it works.

Of course, some of them are smarter and go through the program with ease. But a lot of them just give in hours and hours of their life to get better. Our school is opened 24/7, students have a keycard to enter, and most of the time they are here from 8am till 2am. Since they don't have classes, they don't have hours. Sometimes they stay all week-end because they have a project due on Sunday. And they do get better.

They don't enter our school as geniuses nor with any sort of skills. 90% of them has never written a line of code before. Within 5 years, they are true technical experts. We teach them how to learn to code. Not how to code. And that makes the difference. 100% of our students are employed.

About the "social pariah" around developers, let me tell you a little anecdote that happened less than a week ago. We welcomed on our campus 2 girls from an HIV Prevention Organization. They came to talk about HIV, give away condoms, etc.

We have ~120 students, 4 girls total. You might think that our students are big nerds not capable of talking to two hot girls, especially when they are putting a condom on a dildo and talking about sex. But guess what? The 2 girls had "really great conversations" with the guys. They were surprised because this is an IT school and they expected either a huge crowd of virgin four-eyes or at least really socially awkward people. Most of our students have girlfriends, go to parties, smoke weed, take showers... 

Coding is for everyone.
&gt; It's Harder Than You Think

No it's not. You don't know how hard I think it is.
Now I'm even more confident that I'm an impostor.
I agree with the OP's idea that "Coding is Easy" is kinda of misleading and doesn't help that much.

That's why I'm writing a book to expose the hard parts of software development that's called "Coding is Awesome" -- https://codingisawesome.com

It covers important parts from CS education, and also things CS students do not learn while in college.
I have a CS degree from a top university and generally did well in my basic programming classes. However, I'm not sure if I really know how to program because I have lots of trouble solving problems to build anything that isn't very straight-forward. I have trouble understanding algorithms. I can learn them well enough to get through a class that isn't too in-depth, but I can't use them or remember them. I also have trouble learning new tools. Part of it could be lack of motivation because I find the process tedious. 
&gt;In particular, most people can't learn to program: between 30% and 60% of every university computer science department's intake fail the first programming course.

Holy fuck

How the fuck do you fail the first course

the first course is like here's a string here's a loop here's how to type on a computer

how do you fail that

how does that even happen

is my school just shit?

I mean, it's ranked pretty high so I don't think so.

What

the

fuck
The message of "anyone  can code" isn't a lie. I know multiple people who are making over $80,000 USD a year with as little as 6 months of programming experience.

Will people with 6 months experience be building the next version of Angular - probably not.

I think coders (like all the coders who hang out here on Reddit) who consider themselves "good" and "intelligent" are just angry that the barrier for entry has lowered. 10 years ago you had to have a degree to get a programming job. Now because of so many new technologies and opportunities people with low aptitude or experience can get paid to code.

I know very novice developers who have made a lot of money working as an iOS contractor. Someone has a need and they fulfill it.

So I think the message that all the angry long-term coders are really trying to say is this: "I am very smart. I worked very hard to get where I am and I don't think these coding schools are making people who are as smart as me and they will never be at my level".

In the end - if someone is paying a coder to code something, and if that person who paid that coder is satisfied in the end - can we really say that all of this is a lie? 
I am about to begin a major in programming and I know nothing about it. This article intimidates the shit out of me. 
As some one who works with inner-city youth to teach programming and tutored in collage I want to call bull shit on this. 

I have never met a person who could not learn to program. That said I have met many people who were not driven to learn it. Let's face it programming is allot of self study and that's the hard part. Programming is not a skill to  be learned in a school and I teach my students like its a trade which is how I feel it needs to be learned.
Our education system has failed us. It simply is not designed to find what children have an aptitude for. Talent is drowned in a sea of 'stock' learning that is largely forgotten.

Now we live in an age where the wrote learned knowledge of yesteryear is this year's wikipedia article. What use all those hard learned facts when they can be Googled instantly?

Yet the essential skills of mind and hands are sorely lacking. Those abilities to solve new problems and create new things - rare as hen's teeth.

Even now, when education has come along leaps and bounds, the focus is on learning to pass tests not flourish in a skill or talent.

I'm not saying throw away the basics. Yes maths, language and science are always essential. They are far more useful in the context of a skill, yet every attempt at vocational eduction gets shut down.
Great article, also puts to rest a lot of the retarded arguments claiming the lack of diversity in programming is an issue requiring an immediate solution in the form of quotas. Programming jobs should go to the people passionate about or good at computer science and coding
I can code now pretty easily, my major hurdle is modern visual design. 

Planning to conquer that asap..
I am a believer that peoples brains are fundamentally malleable and can be structured to be able to do hardcore logic if need be .

That said, my experiences are really in line with the article here:

* Had a tutoring job at uni. Nice black girl trying to get through some entry level cs courses for her unrelated degree. Honestly, I'm not a terrible teacher usually, but she simply could not even. We met every week for hours. I wrote her some practice problems "output an ASCII pyramid" type stuff, after just basic loops and whatever. We kept at it for a whole semester, and by the end, she still could not output a pyramid without my help. I felt pretty bad about the whole thing, but it showed to me that she was just not wired for this. I think she passed barely.
* Same deal with a linguist girlfriend. She spent about 6 hours with me trying to write basic Python. She could copy techniques I use, but really got confused somewhere between "print" and "for". She was super excited by it too, loved that she could instruct a device she was familiar with in such a way... In the end, she never did totally conceptualize looping -- it just didn't make sense to her. She had a second to top overall grade in our highschool's graduating class, and aced everything in college with style and grace -- except a stats course, where she would spend days rewriting and memorizing her notes; easily putting in 3x the work of other classes. She finished with a 62% grade and was devastated.

And I feel like I've taught a lot of people successfully too -- my brother was a natural, for example. I think the creative/logic divide is pretty real, but only because people often don't care to balance these two aspects of their ability.
I think the shortage is mostly an education/culture problem, and you don't need to look further than the rates of !(white males) in industry for evidence. Is OP arguing that women are underrepresented because they don't have the aptitude? Or maybe the pipeline has (gaping) room for improvement.

Also, just anecdotally, my experience teaching random non-programmers has been that ineptitude is mostly the result of acquired hangups about mathy things and not knowing strategies to break down problems. I think a large part of success/failure in the field comes down schools teaching the basic thought process implicitly (and probably without realizing they're doing so), and hoping the students get it. There is certainly a nonzero aptitude contribution involved, but OP is suffering from serious self-serving bias in overweighting it.
“Every year it's the same - no more than a third of them [CS students] are showing the sort of ability I would want in anyone doing a coding job. One-third of them are so poor at programming that one would be surprised to hear they had spent more than a couple of weeks supposedly learning about it, never mind half-way through a degree in it. If you really test them on decent programming skills, you get a huge failure rate. In this country it's thought bad to fail students, so mostly we find ways of getting them through even though they don't really have the skills.”

Or how about because CS does not teach programming. It teaches theory. How about this: HR needs to stop requiring or asking for a CS degree to be a programmer on every application there is. And there needs to be a new degree or program created that will actually teach good programming skills, ranging from security to clean coding practices, efficiency, and more. CS "fails" to teach practical programming because it is theory based; it's NOT for that. The majority of my programming is self taught by reading manuals and good books by authors who program as an art form, as well as from colleagues. CS has not really done that; it's only given me an idea of why code does what it does. Data Structures &amp; Algorithms and Assembly language are two classes I did well in; but they were also the least theory heavy and focused on programming linked lists / queues / stacks and writing assembly code in a vm. 

As i've gone higher up, it's been nothing but theory and heavy math that in no way will produce a good programmer fresh out of school. It has nothing to do with lower standards for passing; people have the wrong idea of what a CS degree is going to teach them and perhaps employers don't get it as well. I'm honestly glad to some level that I spent years working for myself learning programming before I could go to school otherwise I would feel so lost.
"Computer Science graduates can't code."  

Bullshit.  


[r/NoShitSherlock](https://www.reddit.com/r/NoShitSherlock/)
"The evidence points to a very obvious conclusion: there are two populations: one that finds programming a relatively painless and indeed enjoyable thing to learn and another that can’t learn no matter how good the teaching."

OR! This world could be populated by more hacks of their trade than anyone seems to suspect. And in certain professions, "fake it until you make it, and if you never make it, fake it forever" just doesn't fucking fly.
I've always been a firm believer in the phrase "anybody can learn to do anything with enough practice."

Anybody CAN learn to code, but to get good at it takes a ton of practice. People who enjoy it as a hobby will of course be better at it than those who view it simply as a tool to do work, because they are practicing it all the time. The more you do something, the better you get. This is true the world over for all sort of subjects, you don't think that great artists or musicians simply pick up a pencil or guitar and create fantastic works of art on their first try? They practice it for years before making anything of "value". Programming is the same way.
To derive from ratatouille; anyone can code, only the fierce can be engineers
Programming is a low social status job? Not that I've noticed...
The real problem isn't that we lack the required number of talented developers, it's that the developers have little to no say in how software is made. The top 10% of devs could probably write/design/fix all the software in the world by themselves if people would get out of their way. This could them be maintained by mediocre devs. The more underqualified people we add to the industry, the more the problem becomes a self fulfilling prophecy
I think the problem of not learning to "think" like a computer scientist is solvable. You need to introduce certain concepts early on. I don't buy the notion that some people just can't learn stuff, but I will accept that certain paradigms of thought are harder to wrap your head around when you've had a set way of looking at things for awhile.

Also, while I hate most "learn to code" events for intentionally teaching gimmicks to people who probably have minimal interest anyways, I would be happy if a formal education on CS basics were included in public school curriculum. Teaching people how to use Python to perform basic functions, and discussing how computers actually work, would help a lot of people simplify things they do often.
IT in general needs to get out of the university and into the tradeschool. Hiring kids to be network engineers is a fucking nightmare because their education levels are all over the place, and most can't even do a show run config. I don't give a fuck if you understand the underlying theories, I want you to do work, not go to training for 6 months before you become half assed useful.
Wait, I thought Al Gore and the US military invented the internet...
When did this become /r/programmingcirclejerk ?
When I read the original story, I didn't know there were coffee machines that you could telnet to. What are those?
Hey,



Thanks for your help with the database issue last time - would have been very bad with the upcoming long weekend. Saved me a lot of trouble!



Have a nice weekend,


Kumar
Just a note, you can use ranges in crontab specifications:

    */10 * * * * /bin/bash -l -c 'ruby kumar_asshole.rb'
    0 9-18 * * * /bin/bash -l -c 'ruby fucking_coffee.rb'
The "true" story was bash scripts. The fact that they were bash scripts made half the story. This is ruby :(
    #!/usr/bin/env ruby

    # Skip on weekends
    exit if Time.now.saturday? || Time.now.sunday?

    # Exit early if no sessions with my_username are found
    exit if `who`[/my_username/].nil?

    require 'net/telnet'

    coffee_machine_ip = '10.10.42.42'
    password = '1234'
    password_prompt = 'Password: '

    con = Net::Telnet.new('Host' =&gt; coffee_machine_ip)
    con.cmd('String' =&gt; password, 'Match' =&gt; /#{password_prompt}/)
    con.cmd('sys brew')
    sleep 64
    con.cmd('sys pour')
    con.close

Yeah, there's definitely some coffee machine out there that accepts Telnet connections and knows how to interpret `sys brew` and `sys pour` to perfectly brew and pour a cup of coffee.
http://reddit.com/r/programming/comments/3tnjpy/taking_bash_hacking_to_the_next_level/

But I up voted your post, since it actually has source code (even if it's a mock up based on the story). 
I dunno, this is kinda half assed and a lot of this stuff is just a terrible idea. I feel like the original story was entirely fabricated.

Edit. I accidentally pushed submit too soon. 

I originally commented on one of the original reddit posts, about the kumar-asshole script "what could go wrong?". Someone noted that this is actually a staging database and that was lost in translation, and that seemed to rectify people's concerns.

No. Sorry, that makes zero sense. I am an ETL admin and rolling back the staging database makes zero sense. First of all, 99% of the DBA/dev work would be done in the reporting database, not staging. The staging db is usually just a dumb storage area for incoming data so that you can get in and out of the source db quickly without waiting for your transform and load processes to finish. Very little dev/dba work would ever occur here. And I cannot think of a time when you'd ever be doing anything in staging manually. Maybe accidentally dropping a table instead of truncating it? But, why would you be doing anything manually to the staging db. I just don't get it. This just seems totally untrue and fake. And it made the whole article seem fake to me too.

Edit: as several people have pointed out, staging can mean something entirely different in other orgs. This sounds more feasible. In my org we have dev, test, and prod environments (or just dev and prod). Staging would be a separate database in each of those environments. Sorry for the rant guys.
I don't believe it. Too contrived.
Nice try, viral marketers from twilio.
[Relevant xkcd](https://xkcd.com/1205/)
sigh, not true.
This guy's boss is gonna wonder why his pipes keep breaking on the same few days each month...
Good guerrilla marketing twilio.
This is ruby. The story made it pretty clear it's written in bash.
Now show us the script that uses xdotool to let you copy and paste text into a java applet based kvm. (I used to need to do this every day o_O)
Genius, really
This story was posted on this sub like three days ago.  Although it's pretty cool to actually see the scripts this time.
I have been threatening to do that with my home coffee maker for literal months.
There is no way to SMS from your own phone number using Twilio without transferring it to them, right? 
Advert for Twilio?
this is fucking golden.
Oh shit you delivered.................................
I find a lot of people give Codecademy a bad rep because it doesn't go too in depth but I think it's great that they can even go into the basics just so that after you are done you can go on your own journey. I think it's great that Codecademy offers such a diverse amount of courses!
Udacity has offered a [great free one](https://www.udacity.com/course/viewer#!/c-ud775/l-2980038599/m-2960778925) for years
I never "got" git until I learned the rudimentary basics of how it worked technically. The tutorials were just the commands I already knew. Problem was going from a central VC like subversion to a decentralized VC. But just saying that out loud didn't make me understand it. 

I think understanding these points finally gave me the direction to learn about it, if tutorials don't work for you:

1. Git is a repository manager wrapped over a version control system. A lot of the commands are for editing/managing commits, not just files. As a developer you only need to understand version control, but to understand git, and why it does certain things, you have to understand that it manages and syncs multiple decentralized repos. So that is why there is a remote ref, why you have an "origin" prefix over incoming branches that point to remote refs (so that you can merge incoming changes instead of just dumping the commits directly into your custom tracking branch).
2. Understand and visualize everything as a tree. You should get something like "gitx" for mac or "kgit" for other systems. It's important to visually see the tree in your first month or 2. Understand where merges were made and how they were made. Also understand what happened when someone rebases.

Just understanding those 2 things first helped a lot. Was able to just use google to figure out the rest, since I wasn't just running seemingly random commands to fix/do stuff anymore.


If you don't want to deal with the internals/basics of how git works. This blog is really good at explaining things like pull vs merge in much more direct, non-git-techinical way (which the default git documentation is bad imo): http://gitready.com/intermediate/2009/01/31/intro-to-rebase.html
 http://imgs.xkcd.com/comics/git.png
It has for a while now.
But the sad truth is that all you will learn is some git commands, you will have self confidence in `git init` then `git add readme.txt` then `git commit -m "my first commit` and thats all. You will start your own project, try to use it, make several commits and.. that's all. 

Git is most useful when you work on the same code with different people which I think the course can't simulate.
I've always liked [this one](https://try.github.io) from github
Anyone use Codecademy pro? Is it worth it? 

It's really annoying sites that don't tell you how much they cost until you sign up. 
Atlassian have a great git tutorial. [Check it out](https://www.atlassian.com/git/tutorials/)
I've been waiting for this. I don't know jackshit about git but it just pops up in every job description and interview. I'm still a student, so I just need to get into it while I have the time
Treehouse also offers a [Git tutorial](https://teamtreehouse.com/library/git-basics) as well in case anyone is interested.
I gave this a try and found it really helpful - thanks for the link!
Ahh, the weekly git tutorial, at the top, as usual.
Praise the sun I was about to give up on learning this
I just signed up for Code Academy this morning.  I noticed this when I was there.  I am planning on going to Prime Academy here in Minnesota (twin cities) to learn the HTML stack.  Does anyone have any suggestions for me.  I took Java for 2 semesters in 2008 So I am not fresh off the boat. 
After trying like 5 tutorials in the past I have given up on git completely. Will just use it like SVN when I must.
Wonderful!
This isn't new.
Note: Not to be confused with "cloudacademy.com", who routinely spam Reddit.
Is this how learning looks like? typing some stuff that is commanded to write. I guest yes, it explain some stuff. 
This is essentially the funniest programmer joke I've seen in about a year.

Kumar-asshole. 17 seconds.
&gt;kumar-asshole.sh

This can't be real. The other ones I can believe, but this..?
http://i.imgur.com/0OVJICo.png
On a more serious note: anyone know a relatively inexpensive coffee maker that someone could hack like this?

I know there are makers with phone apps that automate everything, but that's no fun! I want to make my own and use it from the terminal.

edit: What would such a coffee maker need? I'm guessing a basic web server and a program written to handle incoming requests? Sounds like something a raspberry pi could do? I'd have to integrate it with the coffee maker somehow (or be lazy and make it push the coffee maker's buttons). Would this even be possible without a deep understanding of electronics
This is the original version, in Russian: http://bash.im/quote/436725
The guy's house must be like the starship Enterprise.
Damn I could use a fake texter :). Can anybody recommend a service thats reasonably priced (or better, free!) that allows programmatically sending texts? In the USA?
Sounds like type 1, the [technical thug](http://www.gnu.org/fun/jokes/know.your.sysadmin.html).
/r/thathappened 
What sort of crap is this? Where's the source code? 
Fake but funny. I wanna build a bash script that would create other bash scripts to do all my programming work
I did more of this kind of stuff back in my windows days with Auto IT, but I do love the random cron job to get stuff done... generate ctags at 1am FTW. But the coffee thing - that is genius.
Scripts or it didn't happen
This is obviously a fake.
Life goals.
how is this not in programmerhumour? :)

had a very good chuckle with every single one of them :)

Particularly the coffee machine one.
This guy...gives me hope for a better engineered, stream-lined future.
RemindMe! 1 hour
damn this is so dank my neckbeard fell off
Wow that's really impressive, if it had been 1995

Github link is slightly buried: https://github.com/tensorflow/tensorflow

This looks pretty awesome and powerful.. will need to find an excuse to try it out.
Interesting to see that Google has a new site to host their own source code now that Google Code is being shut down:

https://tensorflow.googlesource.com/tensorflow

Contributing requires [signing a license agreement](https://tensorflow.googlesource.com/tensorflow/+/master/CONTRIBUTING.md).
I studied basic neural net (feedforward w\ backprop and adadelta)
I wonder if I should keep study and code myself neural net or just jump on this library...
Perhaps I'm missing the point here, but why is this such a big deal? It basically provides the same functionality as Theano without the established community. It's possible it could have much higher performance, but with no benchmarks it's hard to tell.

It seems like new deep NN libraries crop up every week, and this isn't anything different but is getting attention just because it's Google.

If they added in their code for distributed GPU training, or developed a platform independent tensor library (OpenCL?) then I'd be all over it- but for the moment it looks like just another NN library.

EDIT: [siblbombs](https://www.reddit.com/r/MachineLearning/comments/3s4qpm/google_tensorflow_released/cwu2cv9) sums up the differences in /r/machinelearning. The recursion / loop stuff sounds superior to Theano's scan function, and the asynchronous execution would be super helpful for large disk resident datasets. Graph debugging also sounds good. However, again, I can't see anything massively groundbreaking as of yet, although it will be interesting to see where google take this. 
So, would this be a good place to ask if anyone has some good entry-level neural network resources? I think I get the gist of it, and want to apply it to a very specific problem, but I think I need a better understanding of the domain. Something with some basic exercises, preferably around taking real-time sensor data and providing outputs.

Also, maybe a dumb question, but is it possible to "train" a neural network in real time? From my brief experience fiddling with neural networks (mainly through playing [N.E.R.O.](http://nerogame.org/)), it seems like a lot of iterations of different network configurations are required for any useful behaviors to emerge. I kind of want to start with a baseline network that provides reasonable behaviors, but allow it to continue adjusting through experience, if you will.

Any resources along these lines would be much appreciated, and, in lieu of those, perhaps pointing me to an appropriate place to ask this question.
Now that this is released, does that mean Google has something better for internal-only uses?
Waiting for TensorFlow.js.
Why, hello, what LabVIEW could have become.

Dataflow is a great model for humans.  I hope this takes off.
Can anyone talk about the applications of this and an overview of how it works - I gather it's not neural networks - so how does it work?
Is there pip link/url for Windows as well?
They're genius and generous. They're generius
I wonder how hard it would be to port to Python 3.5?
Neural nets are way above my head but this is just too awesome not to tinker with. 
I have created a Vagrant virtual machine that allows you to easily run the TensorFlow library on Windows, Mac or Linux operating systems.

https://github.com/gavinln/tensorflow-ipy.git
Can I ask sone of the uses of this?
Where are the instructions for setting it up in Windows?
Does anyone know how this stacks up to DeepLearning4J? I just started playing with that library. 
CockFlow(tm)
Is this the same old "computers will act as intelligent brain" shit that we have been pursuing the last 50 years or so?

There was some recent article or blog post about the "decade of the brain" having produced a lot of ...

... fancy graphics.

And nothing else.

(It's an exaggeration but has an essential core.)

I remember having once read "On Intelligence" years ago. Actually that book is now 11 years old.

Things didn't proceed a single bit from the computer-side of things. Computers are still built in ways that are anti-thetical to biology. System biology is extremely limited in what it can achieve.

How many more decades of waste will go on into these projects?

I think the most interesting part about TensorFlow is that it is written in python. Guess that shows which languages are really prevalent.
&gt; If we don't look to the future, we will have to completely redesign the toaster in just a few years.

I love it when people say stuff like this, and the toaster is a 10-line script.
I can see why the software developer was beheaded. His class modelling was terrible and violated the Liskov Substitution Principle - in that his 'specialisations' were actually restrictions on their parent classes. For omelette to inherit from pork, dairy, and poultry it must also be substitutable for any of the above classes. Composition was clearly a better choice here.
A few relevant toasters from that era:


1999: [Y2K certified toaster](http://www.cnn.com/TECH/ptech/9903/08/toaster.y2k.idg/)                       
2001: [Toaster that runs Java](http://www.theregister.co.uk/2001/03/30/java_toaster_prints_weather_forecast/)

Laugh all you want, but that breakfast machine sounds amazing.
I want to see the DevOps toaster.
I worked on just such a project two years ago. The architect got fired and the project didn't go anywhere. I knew that was going to happen when I saw the first object diagram.
Even the microcontroller is massive overkill for most basic toasters. A simple 555 timer chip is more than good enough.
&gt; The king wisely had the software developer beheaded, and they all
lived happily ever after.

... until the king wanted a global communications network. 

Then the electrical engineers designed a circuit-switched, infrastructure-heavy network that relied on remembering arbitrary 7 to 10 digit numbers, went down if you looked at it funny, and couldn't transfer data at rates greater than 2400 baud. 

And the moral of this story is:

* Simple solutions for simple problems. 
* Sophisticated solutions for sophisticated problems. 
* Smug little parables for idiots. 
There's some other gooduns in that folder:
http://www.danielsen.com/jokes/

weirdly enough the root of the site redirects to a ~~spam~~ jobs page...
### [FizzBuzzEnterpriseEdition ](https://github.com/EnterpriseQualityCoding)

[Enterprise software marks a special high-grade class of software that makes careful use of relevant software architecture design principles to build particularly customizable and extensible solutions to real problems. This project is an example of how the popular FizzBuzz game might be built were it subject to the high quality standards of enterprise software.](https://github.com/EnterpriseQualityCoding)
But if it's not running Unix my fucking_toast.sh script won't work... How will I make toast in the office :(
As the Father of 7 children (and an MSEE/C# programmer ... among other things)... all toasters *suck*. Their duty-cycle is minuscule (e.g. "I only need to work for 5 minutes/day max) and the coils degrade alarmingly fast.  It doesn't even matter which brand/how much you pay - they all give out. Seriously, the only reason our current 6-month-old toaster (which has already been warranty repaired once) still works is due some angle-grinder work on my part (I love tools - especially ones that don't need "installing" and that involve some level of destruction). 

Seriously, though - build a toaster that actually toasts different bread types consistently, with a duty cycle of 2 hours/day.. and that lasts for 2 years and I would gladly pay $200 for it!

Go forth and innovate...

Then 30 years later the king decided he wanted an upgrade of this toaster HMI system, having vaguely remembered the concept put forward by the long dead software devs, the engineers decided to follow this "Object Oriented" route. 

They got to work, working away at this new "OO" system they had been sketched out on a dinner napkin, then part way through the development they heard about a thing called SQL and thought that this would be a great place to store all the variables in use by the system. But since they weren't too framiliar with relational data schemas, they got a drunk wizard to develop the table schemas and stored procedures. About a year in to this process, the drunk wizard heard about a thing called "BLOBs" and told the engineers about it. A greasy ogre who was walking by (having devoured the local priest moments earlier) interjected and told them 

&gt; "Static DLL's are for suckers, what you should do is compile the runtime libraries for your new toaster and then store them in the db as BLOB data"


The King, who had absolutely no concept of system architecture, HMI or SCADA systems was impressed and decided that this system would be fantastic for toasting his Wonder Bread and decided to name the platform 'Wonderware'.
I remember a similar toaster story where the requirements kept changing leading to a huge amount of engineering effort to build a toaster. Somehow I've never been able to find it again.
[I love my toaster](http://i.imgur.com/3T6FfKD.jpg)
Beheading is a bit much. I'd say that the software engineer can happily work on his project, but after a week he can only eat food that has been prepared by his own machine. And watch him slowly starve to death, muhahahha
Add an external PC programmed to reboot the toaster every 5 minutes, and a $750/hour consultant armed with an impressive portfolio of Powerpoint slides, and you will have the Enterprise version of the toaster, with a $10k price tag to match. Most likely named "Web Enterprise Pro 2015 SOA Toaster" by the marketing department. Quickly renamed "Cloud Enterprise Pro 2015 Micro-Services Toaster" to keep up with the latest fashion.
this was great
What if the subjects want to control it with their iphone 8s from their cars? It would need to be a thing of the internet.
:-( this was me today. I deserve to be beheaded :-(
Thanks to future-proofing, we have a dozen future-proof solutions to the old future-proof solution every few years.  Things are so complex, it's easier to invent a new solution from scratch every time, then add your own cruft at your leisure.

I remember when Perl was a nice text processing script language.  Now it's an online repository of algorithm reimplementations with what used to be a language buried somewhere.  Java was a portable, simplified C; now it's like C++.interface[0].getMethod().findFunctionByName("NextVersion").increment() or something.  Your favorite language?  Same story already.  You may not have noticed because you spend all your time with it and the bloat just makes it look larger and more powerful.

Complexity is weakness.
Galileo was also persecuted for being right. The world we live in...
I'm a relatively new programmer, I don't get it. The developer seems like he's doing the right thing, planning the project and creating a structure. Is this supposed to be about overthinking? He seems to making some pretty reasonable assumptions that would lead to the creation of the far superior, smart toaster oven. Is that bad?
Shouldn't this be /r/programmerhumor?
Reminds me of [this toaster](https://youtu.be/PN0rBhqJzSA)
OOP has grown on me. Has I grown older.  About 99% of writing software is obeying to the organizational rule of "a place for everything, everything in his place" or something similar.  OOP give this place to things.  It have other nice features, but the most important one is organizational.  Organization is less important if you are writing something simple, but the more ideas you have to work it, the more important organization become.  The more people that work on something, the more important is organization.  So people have this idea that OOP is good for big stuff.  But you can use OOP to write small snippets that are short, easy to read and powerfull.   Theres really no reason to start procedural and change to OOP later, since you gain advantages from the start. 

I use to be fan of OOP, and write a lot of procedural code. Now I avoid procedural code and I am less OOP fanatic. I am less enthusiast about things because everything is shit, but because everything is shit, you probably need OOP to protect yourself.  Like anything else, OOP can be used to build monstrosities, and some of the most horrible monstrosities use OOP.   With OOP the danger is complicator gloves guys overengineering everything, adding 8 layers to stuff that only need 1 or 0.   But this is to be expected, no good idea is idiotproff, many medicines will kill you if you use it the wrong time. 
This is a funny story, don't get me wrong. I also understand where it's coming from. However, no one ever wants a toaster. They always want a toaster at first, but then they want you to build a house around the toaster and supply everything else in the house for the price of a toaster. Scope creep is a very real thing. I've seen plenty of code that started out as a toaster and ended up being the entire house. The plumbing runs through the kitchen. Electrical wires are hanging out everywhere. The basement and the roof leak. The yard has a golf course. The garage doubles as a music studio. The entire house needs to only allow certain visitors certain access. No one can use the toaster unless they have two forms of ID. Finally the entire house is sitting on an active volcano that can blow at any time. 
I love it when people misuse OOP to argue against it. 
I've never owned a pop-up-toaster; I had a toaster oven that mounted under the upper cabinet and was superior in every conceivable respect except perhaps price...

This somewhat lessened the desired effect of the piece.
https://www.youtube.com/watch?v=x7KK7bXJV2c
Two days ago I purchased a humidifier. It consists of a tank of water and a fan. The first thing I did after plugging it in was upgrade the firmware. I have opened two tech support incidents so far and it won't stay connected to wifi. Last night I spent an hour trying to change a setting on a smoke detector via its app, but apparently there is high server latency in the cloud so I need to wait 24 hours. 

We didn't get the world we wanted, we got the world we deserved. 
We all (hopefully) know that. But occasionally it's so hard to resist and not fall into this pitfall...
Dammit, I remember when that came around the first time.

And, the irony was that the beheading machine was coded in IronPython.
guys it's just a joke; lighten up
And that is how mobile *phones* became what they are today :-P
That text took 16 seconds to load.
I'm a teapot. 
Hahahahaha...awesome !
Not funny then, not funny now.
sorry bro, not programming, check the sidebar

 * If there is no code in your link, it probably doesn't belong here.

 * Do you have something funny to share with fellow programmers? Please take it to /r/ProgrammerHumor/.


Sure... Let's all pretend like we live in a mythical, make-believe world where the requirements for a project are completely understood before anyone starts working on it. 

Yeah... Designing for flexibility is such a bad idea...
Usually the reason to not roll your own security is that you did everything right but one small mistake sinks you. These guys appear to have done everything wrong.
These standards are a joke, really they are. The company im working at, different field, although it falls roughly into security/installed communication equipment as well, works on these standards as well. The products barely comply to the standards. The standards are often 'open for interpretation' etc. Its a giant joke, with one purpose only. Creating market entry barriers. 

The only standards worth a dime are compatibility standards which actually serve a productive purpose and maybe governmental set standards. Mil specs etc. But even here big companies might try to use them for their own purposes.
As no-one else has commented on it, I love his cookie disclaimer :)
&gt; I'm a security researcher and reverse engineer. By visiting this site, you must realise that any or all files on this site may be jam packed full of the finest exploits, tricks and other gubbins. You might also get geo-located and port-scanned for fun and profit. This website uses cookies to improve your experience. We'll assume you're ok with this, but you can opt-out if you wish. If I really want to track you, by tricking you into visiting this site, then it's going to be a lot more subtle than a browser cookie.

I saw an overview of the issues with CSL the other day and knew it was bad, but reading the breakdown shows the overview did a poor job of putting across just *how* badly they've put these things together
&gt; No installers that I questioned own a programmer.

This took me a few tries.
The thing that killed me about this is the pig-headedness of your friends on whatever security installers forum that happens to be. As though making baseless accusations of pedophilia or computer misuse is going to silence a glaring security vulnerability. I'm saddened I share a nationality with these people.

I would hope you've taken the threats and such to the police, the work you're doing is important.
This reminds me a customer that stored credit card info in plain text but in reverse order, because who is gonna think of that
Oh man, I tried to roll my own encryption for fun once. Then I read the "don't roll your own" and 

Edit: decided to test my "cryptography" and...

made a spreadsheet that brute-forced the message. Ugh.
    If customers are concerned about the impact of these vulnerabilities CSL are
    releasing a new product in May which addresses all of the areas highlighted.

&gt;So on one hand, these vulnerabilities aren’t issues, but they are issues enough that you’ve developed a new product to fix them? Righty ho.

That company has some non-technical stage-4 top-down cultural issues to resolve before they make another product.
Talking about roll-your-own crypto. There's been a lot of talk against [Telegram Messenger](https://telegram.org) since it uses custom crypto. They pay good money for vulnerabilities and I, as a user, would like for the crypto experts to examine it.
Bwhaahahha Caesar Ciphers, really? In 2015? 
Does 'don't roll-your-own crypto' relates to algorithms only, or implementation of well-known algorithms also can be considered harmful?
&gt; It’s hard to explain (to someone outside of infosec) just how bad the encryption is. It is orders of magnitude less strong than encryption used by Netscape Navigator in 2001.

Would the phrase "medieval-era cryptography" help? Maybe "bloodletting cipher"?
I work in software security. It's rare to see people roll their own crypto, but I can't even tell you how many times I've found people messing it up in countless other ways

I'd say 1 app out of 100 gets crypto right 
That's worse than I expected.
The problem is that there is ZERO economic incentive for any device manufacturer to make a secure product. There is no incentive to hire people that know what they are doing. There is no liability for making an insecure product.

I'm been in the embedded software space for 17 years. Most of the developers older than me don't have a computer science degree, and I'd say only about half younger than me do. And even if a developer did have a computer science degree, except for the kids coming out of college right now, there's a low likelihood of them knowing anything about crypto.
Check out the [detailed report](http://cybergibbons.com/wp-content/uploads/2015/11/CSL-Dualcom-CS2300-Security-Analysis-2015-v4.pd) if you liked this blog post at all. It's 27 pages long and goes into great detail about everything involved in reaching these conclusions as well as methods used in reverse engineering the device(s).
Of course you should. That way hackers have no idea how the encryption is actually done and thus cannot decrypt it.
I wonder if such a crypto method was a cost cutting measure solved by hiring a graduate?

All too often we blame the programmer but often resourcing and management/sales dept decisions are to blame.

Methinks the conversation could have gone like this:

&gt; Manager: Oi - programmer monkey - we need an encryption thing written in a day

&gt; Programmer: Er - I can do a simple one I guess but it wont be secure of course

&gt; Manager: Good enough! Commit it and ship it Old Sport!
Holy hell.  I'm wrapping up a CS Crypto course right now, and their home-rolled implementation is abysmal.
Maybe they figured it was "good enough" because the kind of people likely to burglarize homes and stores are probably not well-educated in cryptography. 
This is incredible. I'm just a programmer who doesn't know much about crypto beyond the basics but I'm picking my jaw up from the floor. Especially the section where CSL responds is an amazing read.
The company that makes this should run a bug bounty with a **BIG** prize for a remote code execution exploit with the necessary run mode to re-flash the firmware, then build a firmware upgrade on that exploit.
Is this don't roll-your-own crypto Or use a crypto?
&gt; This is a remote control system protected by a short PIN (and it seems that PIN is often the same – 001984 – and installers don’t have the ability to change it.

How much do you guys want to bet their programmer was born in 1984?
The problem here is not to "not roll your own crypto", the problem is this is so bad, it's not even crypto...
That just hurts to read. Horrendous job by that alarm company. Worse than iZombie or Jessica Jones is to watch.
I hope this explodes and they lose a ton of business. Props for doing this research!
“There are two types of encryption: one that will prevent your sister from reading your diary and one that will prevent your government.”
―Bruce Schneier

They chose the first type.
Wait. AES-128 is recommended for hardware that could be used for a very long time? That's not smart.
Hey group of people more informed than me! I'd like to know what the hell is going on in this article. Does anyone have text book or online course material suggestions that might set me in the right direction to understand crypto a bit better?
What I don't understand is why would you roll your own? 

ATAES132A comes down to about 70 cents a piece from digikey in bulk.

For OTA updates it looks like someone has ported RSA to 8 bit micro controllers where the key size is only limited by the available ram.  https://sites.google.com/site/ortegaalfredo/pic18rsa
I think we've had enough examples to be honest. There's another story like this every few days.

Interesting article and borderline negligent security practices though.
If I rolled my own I at least know enough not to make it this obviously bad. I could have done a better job drunk and sleep deprived.
Wow, I learned how to implement the vigenere cipher in the 3rd or 4th homework assignment in the first CS class I've ever taken.
The age of the cipher isn't so much an issue as is re-use of a key in  what is essentially a one-time-pad type of encryption.   RSA-2048 is just as useless if you store the private key inside the device firmware.

"If you do use any Dualcom signalling devices, I would be asking CSL to provide evidence that their newer units are secure. This would be a pen-test carried out by an independent third-party, not a test house or CSL."

What is the difference between independent third party and a "test house"? Why wouldn't you trust a test house? I tried googling but test house doesn't seem to be a commonly used term.
When something is encrypted even if it's with a weak encryption method, isn't the first step determining HOW it's encrypted? I mean how do you figure that out to start with?  You've got a jumble of characters that you know are encrypted but you don't know how. Then what?
How long does it take on average to determine that a message is encrypted by the 7 billionth unique variation of the Caesar Cipher and how much do the analyst's get paid by the hour?

If the secret message turned out to be 'Drink Your Ovaltine!' and it cost the organization $10,000 to figure that out, wouldn't the encryption technically be a success if it were intended to drain resources as opposed to actually protecting information?   Specially considering the encrypted message only cost pennies to produce.
This is an example of why you can't always trust the crypto that someone else rolled.
[Does this count as rolling my own crypto?](http://security.stackexchange.com/questions/95004/can-client-side-hashing-improve-after-the-fact-security-in-response-to-password)

Nobody's using it in production, but I kind'a want them to :P.
Oh wow, capitalism at its finest.
"Don't Roll your own"

- An NSA initiative from the 90's that has encouraged every to avoid understanding crypto, and 6 people from writing bad crypto.
I rolled my own scheme once before. It rotated a through a block of keys and chose the key based on a hash of the previous unencrypted content.  Each character was essentially encoded by a randomly selected key seeded by the unencrypted content. I thought it was clever, I offered a reward to crack it and nobody ever claimed it, but that was like 20 years ago, don't think the offer stands.

It however was a symmetric encryption algorithm, and basically useless by modern standards.
good article
Why would you even think you need to (let alone should) roll your own encryption when there are libraries out there already doing things correctly that you just need to implement? I'm talking about stuff like bCrypt and the like. Personally, I mostly do .NET and just use [SecurityDriven.Inferno](https://github.com/sdrapkin/SecurityDriven.Inferno) - it's the only .NET Crypto library I've seen that comes with a lengthy PDF explaining why it was made the way it was and goes way in-depth. After reading something like that I was convinced it was a really bad idea to try my own crypto with how little I actually knew about all the details compared to what I had read. I guess it's a dangerous thing to not even know what you don't know.
This is not they own crypto, this is Cesar (as Julio Cesar) crypto.  Is simple and good for maybe kids toys.  But I guest you have to put "you own crypto" in the title to get more upvotes. 
can someone tl;dr this? If this is just about some guy not thinking and writing his own "cipher" akin to xor with a 32-bit key, I don't want to read it.

---

Yeah that's what I thought. Using a toy cipher is hardly an example of "Why you should not roll your own crypto".
I wonder if Joe will ever blog about the speech recognition effort that we had with them. 

After porting the browser, Midori was trying to show that the OS is ready for prime time, at least as a server OS. I think they already had successfully ported, or were in the process of porting, another project and had it running in Autopilot - that was the distributed storage and compute engine mentioned in the article. The speech recognition service would have been another win for them and porting it was a somewhat similar endeavor since that service had also recently started running in Autopilot, was of medium size in terms of traffic, scope, and people involved, and was a compute intensive server application (which fits well with their goal of showing that you can build high performance, high scale, high reliability server applications in a managed environment).

Long story short (I can expand later maybe), it was an enormous success for their team, for our team, and for Microsoft - we ended up reducing the latency and improving the scale of the speech service, they ended up taking all the legacy (pre-what would become-Cortana) traffic on just a couple of AP machines. What's probably more important, that was the first deployment of our deep learning models which we had developed but were more CPU intensive than the previous SR technology and so were reducing the scalability of the speech recognition engine. Eventually we didn't really need the performance aspect of the Midori service (because our team reduced the computational requirement of these models in a different cooler way), but because that service deployment was experimental in nature, we could try out these models there first without too much risk, which was great.

For me as an engineer that was the experience of a lifetime - meeting and working with all of these very smart and driven people (I read a book about framework design written by people on that team, which I got to meet), hearing their stories going back to the Commodore days (one of the principal engineers there had designed chips for the Amiga system), and even being able to teach them something (about speech recognition), was amazing.

*Edited for some grammar.
Direct link to the blog articles referenced in the ZDNet link for those interested: http://joeduffyblog.com/2015/11/03/blogging-about-midori/
Just so everyone knows. Singularity, the precursor to Midori, is available on codeplex. It's a "reseach development kit". It was open sourced by MS before they really "got" open source. That being said, I wonder if we could see some community participation now that .Net is open source? Singularity had a lot of the really cool features of Midori, like software isolated processes. 

http://singularity.codeplex.com/


The good news is that as the article points out, the people and technology developed are now being used toward shipping products. From what I know of it (which isn't a ton), this was sort-of a hybrid experiment that wasn't quite MSR but wasn't a retail product team.

Fortunately Microsoft can afford to fund these types of experiments.

I have one minor criticism, and that is that this project seems to have been overweighed when it comes to promotions and rewards. There were far too many people who zipped up the ladder to Principal and Partner levels working on something they knew would probably never ship*, while people solving real-world problems (that were just as hard) had to slowly plod along.

Edit: * not having to ship is incredibly freeing; it means you get to do all the fun stuff and not have to worry about the rest.
&gt; Midori was an operating system written entirely in C# that achieved performance comparable with production operating systems...

Wut... I like C# but I have a hard time understanding the concept of writing an OS in a language that has an intermediary between it and the hardware.  Maybe I have an old-fashioned idea of what an OS is?
So I guess they're going to make Midori the web browser change its name, now?
Am I the only one seeing the full page ads that don't go away on mobile?
&gt; The Microsoft party line is that the Operating Systems Group and other teams at the company are incorporating "learnings" from Midori into what Microsoft builds next.

That's where they haphazardly bolt random pieces of the Starship Enterprise onto their rickety wooden sailing ship so they can continue propagandizing about how the next version of windows will take humanity to the stars?
What's Microsoft's purpose in creating Midori?  Is it meant to evolve into a Windows successor?  Or just to be an alternative product?
Did I miss where they discussed the point of this whole endeavor?
Is the idea that Microsoft can't really advertise any non-Windows OS efforts, or at least believes they can't, because it might give the enormous corporate and home user base a reason to be concerned about long term commitment and support in Windows?
ELI5?
&gt;zero-copy I/O

Well there's your problem.
Is it an acronym for MIcrosoft DOing it RIght?

They just can't do it*.

*Not^entirely^true^but^they^suck^at^privacy
Is it called "Midori" because it'll tie you down and bind you?
Did anyone else read it as "Mordor operating system?"
[deleted]
The name seems to conflict with [Midori Browser](http://midori-browser.org/), which has been around for 8 years.
This is exciting news! It will be interesting to see how many large sites that will switch away from the established CA's. I'm looking forward to finally being able to HTTPS-enable my personal domain.
I feel like for normal SSL this will become the new norm. But for EV certs those will still have a cost associated for now.
I've been part of the limited beta, and I've already switched a bunch of sites over.  The client installer is a bit complex - it runs a script that pulls in a bunch of ubuntu packages, extra code from github, and automagically runs a python virtualenv, but I'm sure it'll get cleaner. Once this is packaged in Linux distros by default it'll be a very slick experience.
I didn't know about Let's Encrypt until now. I'm taking a Computer Security class this semester and for whatever reason it's never been brought up. Are there any legitimate concerns or risks from using Let's Encrypt as a CA?
I was disappointed to learn that they're only issuing 90 day certs.

I deal with hundreds of certificates at my organization, and I'd love to move to a free and automated issuing process.

Automating installation, however, is unfeasible. If you have one Apache server directly on the internet, it's easy enough to run a cron job once every 60 days that fetches your new cert. For a big organization, we have SSL terminating in Apache, IIS, Tomcat, load balancers, proxies. Even where it's possible to automate installation, many servers don't have outbound internet access, or they're clustered and service restarts have to happen in a particular staggered fashion, or they have to go through change control for what's essentially updating a webserver configuration and restarting the service.

With one year certs I could move renewals to Let's Encrypt with basically no impact. At 90 days, I can't reasonably expect to automate all installations or ask SA's to do manual installations 4-6 times more frequently than they do today.
I have posted the following comment to their mailing list, asking for clarification, during the period they asked for input, but never got a reply.

So, their certificate policy (https://letsencrypt.org/documents/ISRG-CP-September-9-2015.pdf) states in section 1.4.2 ("Prohibited certificate uses"), point 4: "*Software or hardware architectures* that provide facilities for interference with encrypted communications, including but not limited to [...]". (emphasis mine)

This seems to exclude use of their certificates on machines running commodity operating systems unless they have dedicated crypto hardware [1] because these OS-es expose debugging APIs which can be trivially used to observe and manipulate communications in plain text before they have even reached the crypto layer.

[1] Or SW running in a protected enclave  using Intel's SGX.

Could anybody shed light on whether my interpretation is overly broad and why?

So, I wonder if IIS will be supported in the public beta. They were going to go live at the end of November I thought. I guess it got pushed back. 
Awesome! That's before my current certificates expire. :)
Going old school, which CA provides widely accepted certs at the lowest cost? 
RemindMe! 3 Dec 2015 "Let's Encrypt beta"
Already using it for my projects on my own servers. Works as expected but documentation could be better. I see the problem  with shared web hosting. It will be pain to renew certificates every 90 days on them...
How will this be any different than using a self generated certificate?

I get that it may lead up the chain to a certificate that's recognized in some browsers, but does that really have any meaning if there is no vetting of identity going on?  
I am looking forward to this! 
I 'm only able to create certificates for example.com and www.example.com, will this constraint be resolved in the public beta?
Wasn't it supposed to start november 16.?
Does anyone know if there's an automated way to use LE on AWS?
I'm pretty excited about this as well! Can't wait to try this out.
The problem in India will be legal.  The indian laws require that SSL certificates be issued by a CA licensed in india.  This means that any application being used by indian government will not be able to use these certificates.  Other indian sites using these certificates will face problems in court because they used un-recognised certificates.
Is the typeface on their home page encrypted?  Virtually unreadable!
It's pretty impressive how bad that application is written. You'd think at least some of that would be done with a library which would implement some basic security. It's almost like they went out of their way to make it unsecured.
Chinese here. The truth is that almost no websites other than money related ones (e.g. banks) in China employ TLS. All of the data, even passwords, are flying bare in the air. In fact, of the three Internet giants in China, Baidu, Alibaba and Tencent (collectively called the BAT), only one has full on https on all of the sites (Alibaba) and even they start doing that only this year.
Well at least they're getting screwed by *something*. Thats more than most of the men on Tinder in my experience.
As someone who can't read Chinese, I came here hoping for a translation of that list of foul words.
This is great! Thanks for the read.
Didn't Tinder originally suffer from the same problem of being able to locate people by triangulation? As for as I know they solved that by now.
Very comprehensive assessment of how understandable but improper security protocol can lead to adverse unintended consequences.

Having said that, pretending like the Ashley Madison leak was the *cause* of relationships ending or suicides is both disingenuous and undermines the tone of the article.
And it's probably not the only app that does not use Encryption ( or where the data are poorly encrypted ) [especially on the Iphone](http://www.tomsguide.com/us/mobile-banking-app-flaws,news-18195.html) IMO it should be Apple's job to ensure that those app have a relevant level of protection before approving them on it's store.
The whole article could be summarized in one sentence: "The app doesn't use encryption". 

Now, who could ACTUALLY sniff on app's  users traffic? 1) The chinese government. But it would just demand the access to Tantan servers and get the data anyway, even if it was sent over SSL. 2) Some hacker in cafe with "free wifi". But what exactly would he get, profile information? As far as I understand, profile info is already publicly available for any user of the app.

Triangulation would still be possible to do even if traffic was encrypted. 

Comparing this to AM hack is far fetched. Data dump of million of users is not even remotely comparable to possible MITM attack, which en masse could be performed only by the government.

Looks like the author is trying to score PR points for himself and his business, surfing on the wave of latest privacy hype.
Reposting my reply that I sent over email and in the other reddit page:


Dear Larry,


First of all, thank you for taking the time to look through the technical side of our app and highlighting issues you found. The issues of turning on HTTPS/SSL and turning off debugging are definitely correct and we are working on releasing a version that fixes these two issues within the week. Thank you.


We also want to clarify a few points.
 

1. Due to the nature of the app, users fill in profile information on Tantan to make them available publicly for all other users. Profile information "exposed" through the API are available through the app anyway, especially since you need to be on the same network and thus close by to "listen in".
 
2. Before any contact list information is sent we encrypt it using a one way hash function. This means that it is not possible for anyone else to see your contact list information. In fact we do not ourselves have access to their contact list information in clear text to protect user privacy.

3. It would be impossible and pointless to sniff someone's location. Pointless because you would need to physically follow them around and impossible because as soon as someone “moves” they will stop using the wifi network you are both on and you would no longer be able to “sniff” them. Aside from this we have also taken special steps to prevent others from being able to pinpoint your location through triangulation of the data you get from the API.

4. Lack of SSL on our API makes us vulnerable to traffic listening (by people who are in the same network as you) but this is radically different from a full database breach like the one Ashley Madison had.

5. We have a list of sensitive words in the app which we use to remind users to behave in a civilized way. Unfortunately some guys do not know how to talk to girls and need some friendly reminders.


With all this said not having HTTPS/SSL really IS a bad idea in general and we are working on releasing it ASAP. Thank you for bringing this to our immediate attention.


In order to handle such issues with more diligence we have created the mailbox security@tantanapp.com If there are any other issues you have found or if you have any questions you are welcome to contact us there or to contact me directly on y@tantanapp.com


Yu Wang

CEO and Co founder

Tantan
Part of me wants to say "awesome hack, great breakdown" but then part of me thinks "holy crap that's just too easy"..

Scary that after 8 months there wasn't any feedback. Makes you wonder if it's actually a Chinese government ploy to gather information about people, although at that point they could at least pretend to make it secure by using HTTPS and just owning the server itself.
I don't understand the China-specific hate here. In my experience, most apps are just this bad. In their defense, you can't really secure mobile clients *too* much, since the code runs on the user's device, so they always have some form of access to it.
In Soviet China, Tinder App screws you!
why does it say destroyed relationships before multiple deaths? thats fucking weird
"I was interested to look deeper and see if, like so many of the people you meet on dating apps, Tantan’s initial beauty was only mirage."

*Slow clap*
Basically, if you want to host a server in China you need to get agreement from government. There is a part in agreement, that says all your data could be accessed by gvmnt.


 You won't see any apps in China using end-to-end encryption. 

EDIT: [see how painful is to get ICP License](http://webdesign.tutsplus.com/articles/chinese-icp-licensing-what-why-and-how-to-get-hosted-in-china--cms-23193)
Huge article with only one issue. There is no ssl used. Oh wow. Now everyone knows you like jingling. No wait, actually just your ISP and a man in the middle of present. Awesome find. 

Security expert!!1!11


The original post and some of the comments seem to focus on this app catering to the Chinese market, but in my personal experience, I've seen this sort of disregard for security in the American/European market all the same. Sadly, as developers, we are not always in control of the technical implementation of an app in the face of deadlines and other pressures from the business side. Sometimes, the person writing your paychecks tells you to get it out the door, security be damned.
Tinder doesn't work in China, unless you use mobile VPN.  

Chinese likes using Chinese software, because they don't trust the west. Though this is also due to China blocking certain apps and websites. Ironically China does this to let Chinese company innovate without the fierce competition, but many (not all) don't and make shitty clones because there's no competition.  

It is fine as long as it is more expensive to hack it than to bribe a Chinese official.
This level of insecurity is par for the course in a lot of places.

And I don't just mean other countries unfortunately.
&gt; The console log is a scrolling window of text - think of it as a Twitter feed for the apps running on your phone.

*wince*
Grindr still transmits coordinates that you can trilaterate. 

Their "fix" is that on start-up, an API response tells the phone to drop, if I remember correctly, about 120ft of accuracy from the geocoordinates. Or you can hide your location, which still puts you in an ordered list by distance, which still allows a less approximate locating.

Pretty easy to abuse, and frankly, I've seen use of it to locate a guy with 3+ fake accounts he used to catfish college students.
I checked because I was looking for the Tinder clone's account name..... 

I was disappointed. 
FYI this articles was originally posted to /r/china https://www.reddit.com/r/China/comments/3sxbev/how_chinese_tinder_clone_tantan_screws_you/
by the author. His username is /u/larrysalibra

The ceo of this company even replied to in the comments in the /r/china post
In communist China, tindr screws you!
In communist china, tinder fuck you!
    curl -H 'Authorization: Bearer f6080f7963b9319a6c08ba2a37f896120e748117f3269decdd9afabc369af4cf' 'http://tantan-core.p1.cn/v1/users?search=suggested&amp;with=questions'

using encryption on a chinese website or app requires a special license from the ministry of information in china. They dont want people to be able to communicate without being able to monitor it so very few apps/websites are given the license unless they build a backdoor for the government to listen in. An app this small would not bother with it. If they did it without getting the license the government would just block access to their app
[deleted]
&gt;Ashley Madison hack, information which resulted in destroyed relationships

To be fair, that was almost certainly for the best.
OK I didn't actually *read* all that, but I certainly appreciate the author taking the time to do the legwork to figure all that stuff out. That's the essence of what used to make blogs great -- this kind of intensive citizen journalism. 
In Soviet China, hookup app screws with people via you?
In Soviet China, Tinder screws you!
This comic extremely accurate. Somehow, every team that uses git will evolve exactly one person who understands git. They'll call them whenever they need to rebase -i, or to revert a merge commit. The rest of the team only knows commit -a and will continue to only know commit -a forever.
&gt; git gets easier once you get the basic idea that branches are homeomorphic endofunctors mapping submanifolds of a Hilbert space.

https://twitter.com/agnoster/status/44636629423497217
Is Git anything like Mercurial? I had no idea what a rebase is, or if there is need for a commit vs. push until I started (translation: was told) using Mercurial. I actually quite like it now
What I don't understand is why is Git so popular. Mercurial can do everything Git can, has a much more simple UI, much easier learning curve, incredibly useful GUI via TortoiseHG - but it seems that lots and lots of new projects stick to that cryptic thing called GIT. Why?
That xkcd has finally made me accept that I will never really "get" git's command-line interface. With git I always felt like I could never figure out on my own how to do something I had never done before; I would always have to dive into manpages or stackoverflow for nontrivial things.

I think the reason for this terrible usability is that git's CLI interface is very inconsistent and exposes too many low-level implementation details. So... does anyone know if there are any decent alternatives out there? When I searched for GUIs all the better ones seemed to be Windows or Mac only and I couldn't find any alternative command-line interfaces for Linux either.
A perfect description of me 2 years ago, haha!

Except instead of a phone number I had google.

It only changed when the lead of a public repo I was contributing to started teaching me some good practices and when he taught me `git rebase -i` I finally got interested and understood the power of it, and it's been command line only ever since.
I can't speak for the author, but I'll give my interpretation.

Git has a simple but difficult to understand underlying representation of data. It enables powerful features that give a developer enormous control. Once you get it, you get it.

However, until you understand what is going on, it's difficult to understand what the commands are actually doing. Most people have a concept of "save" and "load" and there are rough analogous commands in git. By memorizing the special commands that kind of save and kind of load, you can do okay.

If you understand git and get stuck, there is almost always a way to recover from the situation even in the extreme cases of a force push or similar. However, if you don't understand what's going on, then even a minor mishap may be unrecoverable for you. That's why the guy in the comic says to delete everything and download the remote repo fresh again.

As a new programmer, I would say to learn about what a commit actually is and what a branch actually is. Here's a few tidbits to get you started:

- a commit is an immutable entity containing code deletions and additions (changes, not a full code base)
- a commit points to the previous commit (or commits in the case of a merge) to know the previous state of the code that has been changed
- as a whole, commits form a graph, and the sum total of changes of all commits in the graph gives you the current state of the code, so each commit sees a different graph and thus represents a different state of the code
- a branch is a mutable pointer to a single commit
- checking out (similar to loading) a branch is the same as checking out the commit that branch points to
- when you create a new commit, usually you have a branch checked out, and that branch simply changes to point to the new commit, commits are never modified

Edit: To everyone telling me a commit is not a diff:

Sometimes you have to lie to beginners so they can gain some understanding and then reveal the lie later. It is useful to think of commits that way. It helps with understanding how to use commands like cherry-pick, rebase, and revert. Let's not teach calculus when the student is still learning arithmetic.
Kind of reminds me of setting up sendmail... As in "why is this so fucking complicated?!"
I understood git the moment I understood that it is literally just commits and tags. A commit has a parent, a tag is a human readable name. 

A branch is just a tag that moves around when a new commit is made from a commit that was tagged.  This explanation may or may not make sense but that's when I felt understood git.
Thank you OP, I genuinely did not know there was a web site for explaining XKCD comics.
Which is why we need Git Lite...
The 'Alternative' explanation is complete crap.
He assumes that 'git is a basic tool that should be understood by all developers'. He also assumes that 'git.txt' is in the repository. Neither of which are necessarily true.

It's pretty obvious that the first 'Explanation' is the intention of the joke.
Why has no-one written a wrapper for git that allows you to use a sane and consistent syntax?

You type a command, it parses it, and emits the correct git command.
I simply do not understand why git has a reputation of being complex, incomprehensible, difficult to learn, etcetera. I, for one, had no problems at all learning the basic usage of git and had never used any sort of revision control system before. Besides the plethora of excellent tutorials scattered across the internet, git has among the best documentation of any open source project. 

For example, you have the [git user-manual](http://git-scm.com/docs/user-manual.html), [gittutorial(7)](http://git-scm.com/docs/gittutorial), [gittutorial-2(7)](http://git-scm.com/docs/gittutorial-2), [giteveryday(7)](http://git-scm.com/docs/giteveryday), [gitcli(7)](http://git-scm.com/docs/gitcli), and [gitcore-tutorial(7)](http://git-scm.com/docs/gitcore-tutorial). I honestly can't think of another project with equally high-quality and comprehsive introductory documentation.

The only hypothesis I have for this belief that git is difficult is that people try to use it without learning how it works. I find this mindset incomprehesible, in general you should always have at least a cursory understanding of how something works before you try to use it, or you won't understand how the commands work and they will seem arbitrary and mysterious.

If you are "using" git and you don't understand, for example, how to add and remove things from the staging area, how to manipulate HEAD, the staging area and the working tree separately, and how to manipulate branches, what the hell are you doing? Go back and learn your tools, read the man pages for the most common commands; they're not difficult.

I really don't like this xkcd because it perpetuates this false idea that git is inherently difficult to learn and mysterious. It is not; at its core it is extremely simple and once you learn the core object model all its commands (well, at least the porcelain ones) are easily comprehensible. Nothing is intrinsically mysterious, if you think something is mysterious it is a problem with your understanding.

I won't say I like all the design decisions made with its interface, but it's nonetheless one of the best CLIs I have ever used. I don't like that there are often multiple ways to do a particular operation, but this is an understandable consequence of the porcelain being a "batteries included" layer on top of the core commands. I often use magit nowadays, but that's mainly because it is far more efficient for operations such as `git add --patch`, and that's not the fault of the CLI. For most any complex operation I will go back the the CLI.
It's as if someone was explaining an anecdote after speaking it. 'Oh its funny cause...'
I have no idea why git fetch ; merge broke a repo I hadn't modified the other day, but yes, a rm -rf fixed it.
[This](https://www.atlassian.com/git/tutorials) tutorial was very very helpful for me to understand git and gitflow. I haven't read the advanced stuff yet, but if it's as good as the collaborating part this should do the job. 
This is trivially not true.  Most teams do not have a single member that **fully** understands git.

On my team, I have no idea what "commit -a" does and I am sure there are similar holes in the other team members.  I am comfortable with git rebase, but I don't know git gui at all.
Git is a very powerful tool but it's a very complex one. It's grossly overcomplicated for many small projects and subversion would be a better choice.

But given git is widely used for both small and big jobs and for complex source management tasks it really is the best tool available it's well worth learning.
which is why i always like to use GUI stuff. Tortoisehg is particularly nice.
I can understand git just fine but I can't understand how all of you seem to be so bad at it. Then again I've used it every day for years and always had people to answer my questions so  ¯\\\_(ツ)_/¯
Why do people bash git for having complicated commands?

I never use git bash...

There are a bunch of GUI tools for git that mean you don't need to every worry about the commands behind.
Haven't used git since my senior project, and this comic is exactly what I found myself doing at least once a week.  Need to update my project after a merge?  Fuck it, I'll just re checkout the whole thing.  
I just started learning git today and I get the joke. I feel that's great process.
Oh I'm going to use that excuse from now for shitty UI: nah nah, it just has a *complex user experience*. [Please.](https://www.youtube.com/watch?v=npjOSLCR2hE)
I'm that one person! :D
It's funny because the meme says git is so damn complicated to use, and non basic things are indeed complicated. But just git-clone git-add git-commit git-pull git-branch git-checkout git-merge gets a team a long way before they need anything else... if you are working solo you need to use even less commands than that
Definitely check out [Easy Git](https://people.gnome.org/~newren/eg/).

&gt; EasyGit is designed to make git easier to use and learn, not to extend git's functionality. Because of this, you won't find new capabilities in EasyGit. However, you don't need retraining either; eg is mostly backward compatible with git, and any incompatibilities can be discovered naturally and innocuosly during normal workflow (well, except for the eg push default -- see below). All you need to do is replace 'git' with 'eg' in your commands. 
We use subversion at work and I use git at home. I often have to wipe out the project and download a new copy with svn but not ever with git
Out of interest can you explain why you specified young programmers? I know young programmers who are excellent at git, I know 'old' programmers who are awful at git. What's the point in specifying young?
I'm using SourceTree and it helps me understanding what i'm doing.
Git isn't difficult, untill you fuck the the repo up and have to sort out the merge conflicts
The most important feature of git (to me) is speed. Branching used to take forever and was space-intensive - you would have to check out a branch in a new directory, work on it, then merge it and delete it. Checking out a large repo took a long time so people wouldn't bother.

Speed is often the most important feature. Imagine you had voice commands on your phone but it took a minute to process each Command. They would be mostly useless.
Don't know if this is good reddit manners, but going to link to one of my old comments:
https://www.reddit.com/r/programming/comments/3qt28h/xkcd_1597_git/cwi4dr4?context=3
ITT: [Cargo cult](https://en.wikipedia.org/wiki/Cargo_cult_programming) engineering.

&gt; I don't understand this magic tool, and I'll make no attempt to do so, because it's probably too complicated anyway. Instead, I'll blindly go through these motions, cross my fingers, and pray.
I've seen some nice tutorials for git on YouTube. 

Concise tutorial: https://www.youtube.com/watch?v=r63f51ce84A&amp;ab_channel=DerekBanas

Slow ass tutorial: https://www.youtube.com/watch?v=cEGIFZDyszA&amp;ab_channel=thenewboston
This is interesting.
**Tons more comments about it:**

**https://www.reddit.com/submit?url=http%3A%2F%2Fxkcd.com%2F1597%2F**

This website exists.
You did it again, internets.
I use the git CLI, and while I'm no expert, I find it simple to branch, merge, commit, push, pull, revert, checkout particular commits, delete and add. I'm learning stash and rebase now.

To be fair, I work exclusively in linux. So command line tool conventions aren't that foreign to me.
"Don't make me think" is the most important feature of any auxiliary tool. Why should you waste your time on version control? Check-in, check-out - that's all you need to know.  
I prefer goatckd myself
If you're in a technical field and you don't understand git, maybe you should go back to the fastfood industry.
Why “for young programmers”? In my experience, familiarity
with Git as well as DVCS in general decreases with seniority.

FWIW, I've been struggling for years to make any sense of Subversion, either.  "Can be taught ... in 20 minutes..." -- well, maybe if someone would *teach* it, I suppose; but picking it up on one's own?  Try *five years* (and counting) ...  God forbid you make a change to a file's state/status/properties (whatever ya call it), then change your mind: there's *no way to put it back the way it was,* short of cloning the entire project *in advance* so you can start over from *that* again.  Oh, and there's a Windows-Desktop-integrated wrapper that makes it even worse, by actively interfering with the normal workings of Windows Desktop file manipulation.  
  
The only RCS that makes any sense to me whatsoever is SCCS - it's simple, it's clean, it exactly dovetails with the way I think and work -- and nothing else I've ever seen even comes *close*.
&gt; It is very easy for Git to become deadlocked

How?
Webstorm has a decent diff and conflict resolution interface and I use source tree if I have to do jenky shit like cherry pick or rebase. 

I love git and I honestly like that it's cryptic and hard. Understanding git flow is a good interview question topic.   It clearly shows the people that are actually using it from the people that saw a 5 minute tutorial before the interview. 

I recently just rejected a candidate because ( among other things ) he didn't know what git even was.   Granted it's not a standard. It's not even a requirement. But one would think that being a developer for the past 5 years one should have run into git at least once.  If he knew any modern VCS I would have accepted it.  

I'm an old man. The world before git was really poor. CVS and Visual Source Safe were  horrific piles of dung. Subversion was marginally better. 

But git is truly the best version control system with which I've worked. 
If you don't have the mental horsepower required to use git you have no business writing code in any professional capacity
That...is...*huge*...
This is amazing so fast too! They have got various standard libraries, and you [can vote here](https://trello.com/b/6BmTulfx/devdocs-documentation) for new libraries. I don't see wxWidgets in the list :(. But practically I'm always connected to the internet and the speed is pretty decent to just use google. The off chance the internet goes down I will probably forget this. It would be better as a standalone downloadable instead of caching inside webbrowser which could be deleted automatically which defeats the purpose of offline.
Where is java?
DevDocs is pretty awesome, but if you use OSX the app "Dash" is even more amazing. I would hate to go back to programming without Dash, it has raised both my productivity and the speed at which I can pick up new frameworks and languages.
The only complaint is that there is no C#. Other than that this is very *very* Helpful. Thanks OP!
I use devdocs.io exclusively for looking up PHP function stuff because PHP's actual website is such a slow and unsearchable piece of shit.
Where's stack overflow? /s
Make this work in vim with `K` and we got a deal.
Well. That's handy.
Okay, this. is. fucking awesome! BUT if I'm going to use this regularly  I would need to know a few things. How do the docs stay updated? And from which resources are they compiled?
What are the reasons for not including C# and Java?
You know what that used to be called? CHM files or / man pages
The CLHS is the only offline API I need.
No Java?  Really?


Very convenient! Wrapping it in a desktop shell like electron or node webkit would make it pretty awesome. I should have a look at this.
My solution for the "offline docs browsing problem" is a bit different. Most popular docs like `python`, `php`, `java`, etc. already provide offline html docs, so I just download them and add a bookmark to firefox.

For others where no docs are available, I have an app called `HTTrack` inistalled. It fetches the docs from the online html sources and creates a local copy that I can bookmark and read. From time to time, I keep updating them.
I'm interested in it, but wish it also has assembly in there somewhere too.
This is absolutely awesome!
Looks like Dash, but in browser.
Too bad the Symfony Cookbook is missing, that documentation/wiki is one of the best docs out there.
That's fucking great!! Besides Zeal I use devdocs! Awww yeah!
Thanks !
If only there were a button to open the documentation source as well...
Hmm, for some reason it can't find list.pop or list.append in the Python docs... Probably something about how the docs are written.
Awesome! 
The one who made this surely has a lot of time
Is there a way to download all these famous languages/framework then package it for latter use on a laptop with no internet connection?
very cool!
What a great thing. Thanks!
I want this to have c# and Android docs. Such a great idea.
How possible is it to get this running on an airgaped computer?
I know that it's not directly cryptography related, but I wish lists like these would include a stress on usability.  Encryption is totally useless if no one uses it because it's difficult or unintuitive.

Even as a programmer with access to excellent libraries, using crypto can be difficult.  There are no established design patterns, and very little in the area of helping us write cryptographically-sound apps for our users.
This is a good idea, but you really need to provide more information in your list. Some of these points only make sense if you already know enough about crypto to not make that mistake in the first place.

For instance, very few people would know what ECB is. Even if you have a link, you should probably give a quick rundown of what it means.

And "Using a human-readable password as an encryption key" is also very confusing.
If you're going to mention something as specific as random IVs for CBC you might as well also mention not reusing nonces for CTR.
I really appreciate all of the effort you've put into this but I have to say it's also important to spend time justifying this list of orders you're giving out. You are much more likely to effect change if people understand **why** the change is necessary. And on top of that, you're also more likely to stop people from finding creative ways of following these rules and still somehow breaking their security.

It's like telling your roommate to not make spare copies of his key, so he just takes the key he has and leaves it hanging on the front door. He's done exactly what you said but because he doesn't understand the core concepts you still can't stop him from letting in the bad guys.
What about *length-constant time comparing*? Very important and easy to miss.
/u/sarciszewski, I'm interested that you still recommend bcrypt instead of [Argon2](https://password-hashing.net/). What's your rationale?
The bcrypt article which it links to says that salts do not help if you use a fast hashing algorithm for passwords. But what if i use a few GB of salt? :P
I have a question, I am just starting a course on cryptography and IT security (we don't go full man-mode on all that cryptographic math... thank god). The lecturer said that it was conventional to HASH passwords stored in a database. But there have been successful attacks like the famous Sony outage from a few years ago, where the hackers gained access to Sony's database and were able to steal the users' passwords and bank details. The banking details were encrypted, but the password were only hashed, and the hackers were able to only find the value of the customer passwords.


My question is; why don't we encrypt passwords with AES password instead of hashing them? Or by using the users public key with RSA? If the answer is because of time constraints, I mean it's not like the user is going to be constantly loggin/loggout of his account...


Again, perhaps that my question is stupid and he's going to tell us next time why this is!


edit:used more precise terminology
Welp, it's officially been awhile since I've done any crypto.
As a web developer, how do I know if Java's Random library uses a user space implementation or urandom?

Or are you telling me to bypass any library and always go to urandom myself?

I'm sure your points are all valid but they are a little low level for me and the first rule is that I should be writing as little as possible.
The link puts me at the login page for github. I have an account, but this is not a good thing.
Wait, you mean I'm supposed to be *using* encryption?
[deleted]
I spent the morning removing SHA* from our codebase and replacing it with a BCrypt implementation. Thanks for posting this!
Forwarding this to the decision makers.
Can I encrypt a huge file, chunk by chunk in parallel using CBC mode?
Is it really not a good idea to use SHA-2? A lot of applications can't take the slowdown that comes with bcrypt. I understand that you can choose how slow bcrypt is but there doesn't seem a point in speeding up bcrypt.
&gt; Demanding all sorts of requirements for passwords - if necessary, just ask for a long one.

this comment is so true. i'm really fed up of people using the same password everywhere because some javascript considers their 54 character long password consisting of 7 misspelled words not safe enough (please, add some upper case characters and some numbers).
&gt;Writing your own home-grown cryptography primitives (For example: Mifare Classic)

stopped reading there

&gt;Using a fast hash function (e.g. MD5, SHA256) for storing passwords. Use bcrypt instead.

double stopped reading there
Yeah, that's all nice, except you're barking up the wrong tree. Lack of, or poor crypto is caused by management not wanting to spend any more resources on it. Period.
It missed the most important point: don't write your own crypto. 
Why bcrypt over PBKDF2? My understanding was that although bcrypt is theoretically slightly harder to crack, PBKDF2 is more battle-tested and has more implementations readily available.
heh, i was expecting the PHP link to be "don't use PHP".
&gt; I don't give a damn what your "legacy support" concerns are.

I wouldn't be in a job if I had that attitude.

Sometimes developers just have to do really stupid things to keep the business going.
This article should have been called " An Open Letter to Developers Everywhere (About Cryptography(About web development(Actually really, it's about me)))". 
Regarding random number generation:

In general, you can utilize [RdRand and RdSeed](https://en.wikipedia.org/wiki/RdRand) if your CPU supports it for platform independent, safe RNG.

If you use windows and thus lack /dev/urandom, [use this instead](https://msdn.microsoft.com/en-US/library/system.security.cryptography.rngcryptoserviceprovider\(v=vs.80\).aspx), which is a certified RNG
Where is the letter? Some how I can't stand this guy's self entitled rants.
seems valid
"Should array indices start at 0 or 1? My compromise of 0.5 was rejected without, I thought, proper consideration." - Stan Kelly-Bootle﻿
Years ago,  pre internet, these circulated as photocopies of handwritten originals. His handwriting was beautiful. Then someone showed you the one he wrote with his other hand for practice, which was just as perfect to look at.
So... can someone break down what the second paragraph means? The language is a bit arcane. I get that he's dialing in to option A, but how? What does 
&gt;Exclusion of the lower bound forces for a subsequence starting at the smallest natural number the lower bound as mentioned into the realm of the unnatural numbers.

or 
&gt;Consider now the subsequences starting at the smallest natural number: inclusion of the upper bound would then force the latter to be unnatural by the time the sequence has shrunk to the empty one.

mean?
Don't buildings in Europe start at floor 0?
Meanwhile, in Perl:

    my @array = ("alpha", "bravo", "charlie");

    $[ = -13;

    print $array[-13]; # "alpha"
    print $array[-12]; # "bravo"
    print $array[-11]; # "charlie"
Thought experiment: what if we numbered starting with 1 and used inclusive-on-both-ends ranges?

The range(n) function would still have nice properties. Range(0) would be an empty set, range(1) a set of size 1, and so on. It seems all the downsides have to do with specifying ranges with two numbers, but how often do we do that?

Edit: I guess in C we always do that. But this doesn't seem so bad:

`for (int i=1; i&lt;=N; i++)`
Counting discrete quantities starts at one because throughout human history that's how it's been done. Measuring continuous quantities starts at zero because throughout human history that's how it's been done. Trying to change how something has been done for thousands of years invites error.

Measuring a set that doesn't fit neatly into one of those two categories doesn't really have a deep historical precedent. Expecting all measurements to fit neatly into one of those two categories sets invites error.
Counting is different from using numbers to represent distance

Counting starts from one..1,2,3... I have three objects

Distances start from zero..If I am standing in front of my house, how far do I need to walk to stand in front of my house?

If my pointer points to the first element of the array, how far do I have to move it to point to the first element of the array, how far to the next element?

Indexing in software is distance, not counting
Sometimes students will say they understand how to code an algorithm but keep getting [off-by-one errors](http://www.smbc-comics.com/?id=2831). But if you are off-by-one, you don't really understand. If you don't know whether your bounds are inclusive or exclusive, or whether the length is your endpoint minus your start point or one more or less, then you're not thinking clearly enough. You're just guessing and hoping the machine will back you up.

Dijkstra was trained in an era where you sent your code off to run and waited days for the response. If you had a bug, you wasted a lot of time. He knew his code worked before he ran it because he thought about it thoroughly instead of incidentally.

His argument makes perfect sense, and it comes from a source we have reason to trust.
The problem with this essay is that it establishes a very nice line of reasoning as to why a certain representation of ranges is preferable, but that is not the title. The title is "Why numbering should start at zero", and IMO Dijkstra completely fails to establish his argument for that statement. Let's look at the key section:

&gt; Adhering to convention a) yields, when starting with subscript 1, the subscript range 1 ≤ i &lt; N+1; starting with 0, however, gives the nicer range 0 ≤  i &lt; N. So let us let our ordinals start at zero: an element's ordinal (subscript) equals the number of elements preceding it in the sequence.

The first sentence is obviously completely subjective. Why would one range be nicer than the other? Because it contains N+1? That's subjective. There is nothing objectively better about this range. He doesn't establish any objective argument in this sentence at all, other than his personal preference. I find the other range "nicer", how are you going to prove that I don't?

As for the second sentence, the argument is completely silly.

&gt; So let us let our ordinals start at one: an element's ordinal (subscript) equals its position in the sequence.

What now, Dijkstra?
Context is everything.
Counting starts at 1. Measuring starts at 0.
There will be no consensus on this issue. All math papers are written with 1-indexing. Math oriented languages will almost always do 1-index. Reference to the i-th element of a matrix m should never ever be m[i-1]. That will lead to awful off by one errors.
Used to write in C# for COM API created for visual basic scripts. This was driven me mad.
For anyone else wondering about his signature and that ij that looks like a ȳ, I found [this discussion](https://www.quora.com/What-is-the-deal-with-the-ij-digraph-in-Dijkstras-surname) helpful.
OP: "Because then a repost is actually the first post."
He makes a good point, but it is based on redefining a sequence without "the pernicious three dots". What I don't understand is why those dots are so bad. That notation is unambiguous to humans, so why don't we just include something like "3-dot format" in a computer language?
The difference between mathematicians and computer scientists (and I am speaking very generally) is that mathematicians care about the bounds on a variable and the number of things in a set, while computer scientists care about how many times they will do something and how far they are from where they started (e.g. list indexing). Hence the two different conventions on bounds and numbering.
Even "better" than fixed subscripts would be the system I've seen in some languages where you can do `array[0 .. 5]`. So you can choose at which number your array starts and there's no ambiguity of the last index of the array. Don't remember the exact syntax or which language it was, though.

In the end, I feel like it doesn't really matter. 0 indexing has a wider adoption, while 1 indexing is more in line with how people think (supposedly, though I don't know anyone who starts counting at 0).

The argument that `[0, n-1]` is nicer than `[1, n]` is silly, though. 
While I understand the rationale, people are used to counting from 1, not 0. It's takes extra thinking to loop from 0 to the end of the array. If you're an experienced programmer it's not a problem but if you're not, it's not intuitive.
I wish the creators of Julia had read this.
I may get this bit framed:

&gt; "In corporate religions as in others, the heretic must be cast out not because of the probability that he is wrong but because of the possibility that he is right." -- Antony Jay
[The Book of Numbers](http://www.amazon.com/Book-Numbers-John-H-Conway/dp/038797993X) has a chapter that made clear to me, as a kid, the difference between the *cardinal* numbers 0, 1, 2... and the *ordinal* numbers first, second, third....

When counting with cardinal numbers, you use the *first* (ha) cardinal you didn't use.

    (this)  (that)  (the other)
     "0,      1,      2, ..."

So there are 3 things here. This works for some other sets, like:

    "..."

So there are 0 things here!

Someone else here said that "counting is not the same as indexing", which is exactly this distinction between *how many things there are* and *putting things in order*.

(I just realized that Dijkstra uses the word ordinal in his paper, to refer to the numeric indices into arrays, which is what I would have called cardinal here. Words!)
One based indexing is why being forced to use Matlab is so horrifying
I disagree with Mr. Dijkstra here, and we can put the argument like so:  
When you are counting some items you *start* with 1, for the next you say 2, for the third you say 3; this gives a map where the number you used corresponds to the list's length when including that item. -- This also means that you don't have to throw away half your possible-indices (in 2's complement) just so that you can use &lt; to test for "not-found" while searching; this is to say reserving 0 for the "it's not here" value seems more prudent.
Of course EWD is right. However, it takes 2 days to fully and properly explain 0-based indexing to raw students (C). It takes 2 minutes to illustrate 1-based indexing (Lua).

The invention of zero is one of the great steps in history. However, teaching is not a vast knowledge dump onto a tabula rasa. Learning is a bootstrapping process.
For some function arguments, I prefer to index arrays starting at 1.

This allows me to use an unsigned integer to represent the index being passed to the function, and if I want to no-op the function or do some other special task with it, I can pass it 0 instead.  This becomes a reserved condition, and it doesn't break the brain of someone who might need to read my code later.
Because intuitively we all number lists starting at zero...

Sorry EWD is approaching this from CompSci instead of the human perspective.
So that I can accurately count the number of ways I actually care about &lt;whatever&gt; no matter what it is.
Talk about preaching to the choir with this title. 
Addition: Here is what Guido van Rossum (Python creator) had to say about [why Python uses 0-based indexing](http://python-history.blogspot.com.br/2013/10/why-python-uses-0-based-indexing.html).


This whole article is based on the presupposition that 0 is a natural number, and that that fact is actually important.
That is some unreadable "I'm a scholar, look at how complicated my sentence structure is" BS.

Also, if I have one of something, I will stick to using 1, not 0.  If I come in first, I am the first.  When I cross, there will be 1 person across the finish line.  1 works better than 0.
Why start with zero? I'm number one, baby.
Is there really people who think arrays should start at anything other than 0 who understand how pointers work?

I've heard a hundred people defending it but never heard anyone seriously defend it who wasn't a first year comp sci student.
How many integers are in this sequence?

    0 ≤ i &lt; N
    N
    N ≤ i &lt; N
    0
    N ≤ i &lt; N + 1
    1
    
    0 ≤ i ≤ N
    N+1
    1 ≤ i ≤ N
    N
    N ≤ i ≤ N - 1
    0
    N ≤ i ≤ N
    1
    N ≤ i ≤ N + 1
    2
My final assessment had an array of 15 different types of monster (it was a game) and my project partner created the array like:  

    cMonster monster[16];

"Because starting counting at 1 is easier". Like what the actual fuck, how is this even a discussion? Why is it not super obvious to people that not starting at 0 is bad?
Damn you Lua.
Zero and infinity are the most natural numbers.

Edit: One of you grab your nuts and actually tell me I'm wrong. Cheers cunts.

NextEdit: This seems apparent to me, but I could be wrong. Only time will tell. If I do prove this one day I will post it to /r/iamverysmart and then I'll let SRD suck my dick en masse. Cheers cunts.

LaterEdit: For all of you incoming haters...**If the proportionality of zero and infinity are equal then you can infer they are in fact a singular function of mathematics.** Special thanks to /u/theelk801 for pointing out I didn't actually say that anywhere. Now, I have to get tacos, but in the meantime if I could get some more super duper smart people in here to tell me how retarded I am and how smart they are that would be fucking fantastic. Remember, no logical assertions debating me. Just try and sound as condescending as possible. Cheers cunts.
One reason I'm not using lua or julia. If I have to keep going back and fixing the same mistake - I think your language is wrong. 

Life is too short to have to think about this sort of irrelevance.
It hasn't been officially released yet, just tagged in Github. The official release (including builds) is set for tomorrow afternoon (according to the mailing list)--although it should be equivalent to the tag.
For those wondering why there's a jump from 5 to 7, it's because the [php 6 development branch was dedicated to full unicode support](http://www.slideshare.net/andreizm/the-good-the-bad-and-the-ugly-what-happened-to-unicode-and-php-6), but the work involved overwhelmed them, so they jumped to 7 to release new features without the unicode component.
308 comments, and no one has mentioned yet how much faster php 7 is compared to php 5.6?
I never liked PHP and glad I don't work on it anymore. But I'm also glad I never turned as toxic as all the PHP haters in this thread.

It's just a language. Congrats to the PHP devs for getting another major release out.
I'm currently at the first first job I've had to do some PHP work.

It was quite painful at first, but it grew on me over time.  
Does the language have issues? Oh dear god, yes it does.  However, I was expecting it to be truly terrible given the amount of hate it receives, and it's really not as bad as all that.
Congrats to the PHP team! Here is to you: For developing a language that allows me to have a job and in turn give my family a good life. Few people ever get to impact the world in such a positive way. :)
- [Changelog](http://php.net/ChangeLog-7.php)
- [New features summary](http://php.net/manual/migration70.new-features.php)

7 looks to be a great improvement from 5, there is a lot of work in this release that should allow for cleaner &amp; safer code, easier.
PHP has to be the platform that has the hardest time propagating updates and new versions.... Props to PHP developers.  Despite all the hate, PHP is big.  That is an undeniable fact.
So amazing. 
&gt;Null coalesce operator

&gt;The null coalesce operator (??) has been added as syntactic sugar for the common case of needing to use a ternary in conjunction with isset(). It returns its first operand if it exists and is not NULL; otherwise it returns its second operand.


PHP 7 is x2 faster than php5.6, which is already faster than python and ruby, go LOL on that
Showed up to this thread and expected elitist assholes from other languages. Was not disappointed.
Some comments here are really salty. If you don't like it, don't use it.

Is there any good benchmaek out there (especially compared to hhvm)? Is it still usable as mod_php on apache?
I'm an old self-trained codger who started in FORTRAN. I've programmed in assembler, APL, C, C++, Perl, Java, XSLT, PHP, Javascript, Objective C, Lua, and a few others. I've dabbled in Python, Ruby, and this and that. Languages besides PHP are alright, but I don't get why they're so much better.

Not that I have a great opinion of PHP, I simply have no opinion of any language, they're just tools. Okay, if I had to choose a favorite, it would be XSLT.

The web app I'm working on now is in PHP, with Javascript and jQuery. It's a large, best-in-class product, not that hard to maintain, and it's making money. But really it's our process and discipline that makes it work. We could be doing the same with any language.

FYI RTM is not due till the 3rd (Tomorrow):

http://www.serverphorums.com/read.php?7,1352537
Ooh, only [137 compiler warnings](http://gcov.php.net/viewer.php?version=PHP_7_0&amp;func=compile_results) and [102 failed tests](http://gcov.php.net/viewer.php?version=PHP_7_0&amp;func=tests)! That's a huge improvement from 5.6's [638 warnings](http://gcov.php.net/viewer.php?version=PHP_5_6&amp;func=compile_results) and [114 failed tests](http://gcov.php.net/viewer.php?version=PHP_5_6&amp;func=tests)!
http://php.net/manual/en/migration70.incompatible.php

Seems like a painful upgrade for legacy spaghetti code.
It must be said:

/r/lolphp

And yet, PHP is huge.  Software developers should take note of this really important effect.  Elegance and good design matter to some of us, but at the end of the day it's about making shit work, and you don't have to be elegant or well designed to meet that bar.

"Making shit work" is 99% of the game.
PHP is completely changing their philosophy at least since version 5.
Before it, PHP was a duck-typed language for creating home pages. It was perfect for its purpose, because it was easy, quick and dirty. 

At some point, people started using it for full-blown applications. PHP maintainers decided to support these mad men and they've started transforming PHP to meet their needs. They're trying to make it look like Java, abandoning every original PHP design principle.
It took years and it still cannot be even compared to Java, feature-wise and ecosystem-wise. It's a sad, pointless struggle.
I'm reading a lot of negativity about PHP. I've used nearly all of the common programming languages since I was first paid to do programming in 1987. One notable exception is that I have no experience with Python, but I'm sure that will change in the future.

In any case, we as programmers need to stop thinking about languages. We need to start thinking about programming as a process. To the experienced programmer, programming is about solving a problem with the best tool for the job, and solving that problem in such a way that someone can come along 10 years from now and build on what you did.

Think about a well-engineered vehicle. It began as a design in Germany, Detroit, or Japan. Almost every design decision had good reasons. Mistakes were made. But in general the vehicle was made in such a way that a variety of people could easily work on it for the next 5-15 years. The mechanic who replaces your brake system doesn't hold a degree in automotive engineering, but he doesn't need one. He has the right tools and the process to fix the brakes makes sense.

Rather than being attached to any particular piece of technology, you as a programmer should focus on creating a fine piece of engineering work that is beautiful in its simplicity. A programmer who doesn't natively speak English, or someone fresh out of school should be able to see what you did many years ago and immediately be able to start building on it. He will use whatever tool is necessary. The tool is not the point. The underlying engineering work is the point.

Yes, newer languages and technologies are intellectually interesting, but don't lose sight of the forest for the trees and jump onto whatever fad you see. Focus on developing your skills as a programmer, rather than as a coder of a specific language and you will not only have longevity in the field but you will be a much happier person.
Scrolled down the comments.. seems to just be people saying "everybody in this thread is shitting on PHP". Yet to find someone really insulting the language unfairly.
Hurray, time to celebrate with some fun.

http://cube-drone.com/comics/c/ingeniuty-of-sorts

http://cube-drone.com/comics/c/ergonomically-sound
So, if some of you are more knowledgeable than I am, how long should it be until it is packaged for Ubuntu, and easily installable with aptitude, even for a noob like me? 

I'm afraid of compiling such a young version, even though my server only hosts hobby, non-critical projects.
Do not run `git branch --list --all` on their repository. They don't know the difference between tags and branches. Every minor release has it's own branch, instead of a tag...
This is fascinating. I'm not sure what's with all the hate this is receiving.
I've been out of the loop since forever. I didn't even know there was a PHP 7 on the way. Why is it called PHP 7 though? Shouldn't it be PHP 6 since the current version is 5? Can someone fill me in on this?
New version of crap
Seventh major piece of shit

I code PHP from time to time and hate it.
too little too late 
And so a new cycle of

 - 7.0.1: Fix severe security issue in 7.0
 - 7.0.2: Fix severe security issue in 7.1 introduced by fixing the security issue in 7.0
 - 7.0.3: Fix severe security issues in 7.2 introduced by still not knowing what the hell we are doing

begins ...

The developers of PHP are the prime reason why I think that all this "learn to code" thing is wrong. No, not everyone can and should code, like for instance the developers of PHP.
People are free to use what ever they feel like is working for them and be happy with, but please don't come here bragging about the legacy that you have to maintain and the **shit** you are making with it right now. Also admit it, most bragging is not because you actually develop something in PHP but for things like Wordpress and such which are PHP lifespan.
heeeeeeellllllllyyyeeeeeeeaaaahhhhhh
PHP is fine and everyone works with everyone works with Wordpress or Drupal and these are great software packages powering sites like whitehouse.gov it's time to stop the hate and accept this REALITY.
this should be on /r/ProgrammerHumor
Not interested. My dog's already had its breakfast.
Obligatory Obi [Wan](http://imgur.com/g01HYn8)
Congratulations to PHP team. Best programming language for the World Wide Web today. Father of RoR and Node.js SF Brooklyn hipsters. Good job.
&gt; List now has a sort method

List.sort is now the preferred way to sort lists. If still in doubt, `Collections.sort` in JDK is implemented as:

    public static void sort(List list, Comparator c) {
        list.sort(c);
    }

Let's forget about Collections.sort in context of modern Java ;)
Worth noting that streams etc. are not yet as performant as their oldschool methodology counterparts, just in case performance is an issue.
Just as a complement to your post, I suggest this series of articles which appeared here a few months ago: [Not Your Father's Java: An Opinionated Guide to Modern Java Development](http://blog.paralleluniverse.co/2014/05/01/modern-java/)
Thank you, I needed this! Well written and clear!
Can anyone give me an example or explain me why the 'default' feature on interfaces is smart? Isn't this exactly what abtract classes are for? 
Thanks. Bookmarked. 

Irony is most (not all) java developers are coding like jdk1.4 style. Avoid generics like plague. We need a movement to push those developers to start using generics (&amp;annotations etc etc.)

Cool overview! You might want to add something on how to update GoF patterns with lambda's.

For anyone interested I just followed this lecture at Devoxx belgium:
https://github.com/forax/design-pattern-reloaded

I thought it was neat stuff.
I don't like how they downplay
    java.time.Instant
As a way to create legacy java.util.Date Objects.  Instant should be your preferred storage mechanism in your application since it's the proper way to reason about time (at a given instant) and only fall back onto the Zone aware variants when you need to visually represent something to a human
TIL I write modern java.
Great! Now to find a client with a jdk greater than 1.7...
This is great, but it only covers language-level features. Can we get a list going of good modern libraries? Being able to write Java 8 code doesn't mean much if you're also writing 100-line XML files to configure your AbstractFactoryBuilderVisitorDecorators.

My personal favorites:

* [Guava](https://github.com/google/guava) should basically be treated as the missing standard library. Apache [commons-lang](https://commons.apache.org/proper/commons-lang/) is a runner-up in my mind. I wouldn't want to write Java without one of them.
* [Guice](https://github.com/google/guice) for dependency injection (instead of Spring). [Dagger2](https://google.github.io/dagger/) also looks promising, though I haven't used it yet. It aims to make problems with auto-wiring configuration into compile errors instead of runtime errors.
* [Spark](http://sparkjava.com/) for web development - has the feel of Sinatra for Ruby or Flask for Python. Ships with an embeddded Jetty server so you can get a basic HTTP server up and running with a tiny bit of code and no WAR file or XML bullshit.
* [MyBatis](http://mybatis.org/mybatis-3/) beats the hell out of Hibernate for database access / ORM.
I'm having a bit of difficulty grasping the PersonFactory example:

    interface PersonFactory&lt;P extends Person&gt; {
        P create(String firstName, String lastName);
    }

    PersonFactory&lt;Person&gt; personFactory = Person::new;
    Person person = personFactory.create("Peter", "Parker");

Is "create" some sort of magic keyword?  Would it work if I changed "create" for "build"?  What if my interface had both a "create" and a "build" method?
Looks like Swift with more verbose and messy syntax. But nice to see in case I switch from iOS to Android development. Although I'd probably try Kotlin instead as that seems much more like Swift. That is it does the same as Java 8 except with clean syntax. 
Modern Java? All of this is almost 2 years old :P
This is great. I have a question though: how does one go about *actually* using these new functionalities? I mean, let's say you want to do *something* in a java class. Before you start applying your knowledge of Java, do you stop and think to yourself "How can I apply the new Java8 techniques to solve this problem, instead of recurring to the old techniques I knew from previous versions?
As a .NET developer, this is very cute.
I would love to see the JDK be refactored from the ground up, using lessons learned.  A brand-new runtime lib, with a consistent API pattern among modules,  removed redundancies, deprecated stuff out,  a lovely new functional collection/container library, etc.

Call it JDK2.0 (not Java 2).  Make it a breaking change with little or no backward-compat, and continue maintaining 1.x.

This isn't the Java I know! Where are the abstract factories - where is the XML ????
How does reduce handle a stream input of exactly one element?
Thanks
&gt; “Java is still not dead—and people are starting to figure that out.”

Sorry for my ignorance but why would somone think Java is dead?
As Java developper, this is fantastic, thank you.
It's still missing an insane amount of important language features, though.  I am working on a java web back end that is very up to date, and in addition to the web framework being a fucking disaster, the language is just....missing too much.  It feels like I'm constantly trying to make up for something that is just easy in C#/.NET
https://en.wikipedia.org/wiki/Church_encoding

You can write an integer math library using just Optional&lt;&gt;.
Thanks. Starred and bookmarked.
it's ironic that coming from a .NET background, I've been looking for these features and finding them in Java 8. the snake eats itself
No scala love?
Java had done a really big leap with it's 8 version. I'm using most of these features almost a year, but still missing some cool boilerplate-reducing syntax sugar like safe-navigation operator (?.), elvis operator (?:), C#-like properties, smart map and list constructing ([key1: "val1", key2: "val2"] and ["el1", "el2"] respectively) etc. Most of this stuff is available in Groovy, but, well, this language has a **much** bigger number of ways to shoot off your leg.
Or just use Scala ;) Next version 2.12 that is in milestone stage right now will have effective compilation of functions to Java 8 closures so will be on par, but offering a lot more power.
I wish C# had such a github MD instead of shaky blog posts.
Oracle recently fired some of its Java team. Given Oracle's history, that doesn't bode well for Java, regardless of its utility as a language.

http://fortune.com/2015/09/07/oracle-cuts-java-execs/
"Aww, 'modern'! How cute!" - C#
Cobol is not dead either!
&gt; modern java

&gt; Table of contents: A lot of things that other languages have had for a decade and are completely taken for granted by now, and no one talks about them anymore because they're not "new", "modern" or anything like that.

Welcome to 2005, java.

Anyways, how do you java guys manage to code without throwing up every 50 seconds?
Modern Java - more verbose than before.
&gt; Let's start with a simple example of how to sort a list of strings in prior versions of Java:

For fucks sake no, let's start with how to do it _now_ and only then show me how fucked up it was previously. Learn to make me interested, not bored.
Edit. Before more people reply: I tried to troll because I hate (I mean *hate*, think 800 lines of traceback + all the guice in the world) Java but didn't read the original article in detail. Hats off to the people replying in good faith.

    Collections.sort(names, new Comparator&lt;String&gt;() {
        @Override
        public int compare(String a, String b) {
            return b.compareTo(a);
        }
    });

Python:

    sorted(names, cmp=my_cmp)

Haskell:

    sortBy myCmp list names

"modern"
Minimum 6 years Rust experience required ;)
I don't think this should be to the surprise of anyone. That said, this would be a cool project to work on.
Does this really belong in this sub? 

I mean there are many companies which hire developers in interesting positions. 

Creating a new thread for every interesting job position is probably overkill. 
&gt;We aim for double the performance of current engines, with no crashes.

That doesn't seem like a realistic goal.
[Soon...](http://img15.deviantart.net/d909/i/2009/144/4/f/tom_servo_cutaway_view_by_koku_chan.jpg)
Rust is amazing. Great opportunity for any developer who wants to work on the best web browser 
The last time I worked with Rust, the syntax kept changing and I would have to update my code all the time to make it compile. Is there finally a stable version of it now?
I was unimpressed with Mozilla's hiring process. I applied for a platform engineer position with potential for it to be a remote job. I passed their coding challenge with flying colors then when I told the HR rep that I was in **grad** school, she told me "this isn't looking good." How unprofessional can you get?
Really? Best Brains managed to work Servo with one puppeteer, and repairs were managed with some super glue and duct tape. I don't see why they need three engineers.
Isn't Servo open source? You can work on it without putting your livelihood in Mozilla's traitorous hands.
They require Rust.
Oh, I thought they were going to post this Servo

https://www.youtube.com/watch?v=N34MmqmxqD0
Mozo drones are deep in this thread.
My dick is pretty hard for this position.  I will probably get smoked by the average engineers applying though.
... another project from ground up? really? haven't those people learned ANYTHING? see you in 10 years... when you decide to do it again, because your code base is basically the same it is now. fuuuck, what a waste of time and talent. at least someone gets a paycheck for reinventing the wheel yet again.
I wonder when the recently announced Firefox ~~spyware~~ in built adds will appear in the engine.
&gt;One of those mails came from a set-top box manufacturer, stating that thousands of customers were unable to watch TV because their boxes crashed when Telize didn’t return any data, and demanding that I return an empty JSON object for a two weeks period.

&gt;This didn’t end there, as the person wouldn’t take no for an answer, and came up with the brilliant idea to ask me to redirect the endpoint to a server they would host themselves in order not to just serve empty data this time, but simply restoring service entirely.

So basically, the set-top box manufacturer:

* was leaning on a free service to run their commercial business

* didn't give any thought to privacy, being quite happy to transmit IP addresses of people watching TV to this random third-party service

* didn't give any thought to security, being quite happy to trust whatever values were returned to them by this random third-party free service

* didn't give any thought to robustness, being quite happy to implement functionality that depended on the third-party service continuing indefinitely, and didn't bother to implement error handling, preferring to let the system crash in case the service was unavailable

* thought it had any business collecting/using IP addresses in the first place

* only thought there was anything wrong with this arrangement once the service stopped working, and their solution was to demand control of the service domain and perpetuate for others the situation that led to their problem

Brainless twats. The Internet of Shitty Things.

Better that the free service had never existed, frankly. The source code is trivial and anyone who genuinely needed that functionality could host it themselves.
&gt; This didn’t end there, as the person wouldn’t take no for an answer, and came up with the brilliant idea to ask me to redirect the endpoint to a server they would host themselves in order not to just serve empty data this time, but simply restoring service entirely.

This entitlement mentality. From business.
&gt; One of those mails came from a set-top box manufacturer, stating that thousands of customers were unable to watch TV because their boxes crashed when Telize didn’t return any data, and demanding that I return an empty JSON object for a two weeks period.

The correct response to that is "pay me or fuck off".
I have a similar story that has unfolded recently. I had been offering an api to the public for free, and before shutting it down it averaged around 850,000 queries per day. I shut it down about a month ago, however I did configure nginx to return the default value for what my api (it was basically to check if something was x or y, x being the default that would result in success/okay to the person using the api in their services). So a month later I still have around 70% of the traffic hitting my server on this api endpoint which unfortunately is also hosted under the main domain on the site so I can't exactly just zero the hostname. At what point should I stop being nice and start returning 404s or some other http response to cut down on the traffic? I don't want to break anyone's services but I also don't want all the traffic.
The EU provides a service that allows you to check the tax registration number of basically any company in the EU. Lets you see if it's a valid number (i.e. not just made up) and if the company is still trading and stuff. But it's a consolidated service - the EU doesn't maintain a single database, the service queries each country's database individually depending on your query. A relay API, if you will. It's pretty cool. 

But it's as-is. A free, best-effort service. Some country's databases will go offline for a couple of hours at a time, then come back, no biggie.

So in a place I used to work, I used this in a basic web form for registering new clients. It meant we didn't have to try and verify tax numbers later on, every new client would be verified before we even recorded them in our ERP system.
To handle outages, the user gets a message that the number couldn't be verified, come back and try again later. They can save their form as a draft, as they couldn't submit a "formal" new client request without this number having been verified.

Happened one day and I get a phone call asking what the story is. I explain it's a best-effort service, same the form and come later. User *loses* it, talking about how they need to get this form in today, loads of money will be lost if we don't. They go to the application owner, demanding the issue be fixed today. It comes back to me again and I again explain it's a free service, it'll be back in a few hours. 
An email goes out from this person, CCed to about twenty people, demanding that I call up the EU department involved and put pressure on them to fix it.

I replied with a link to the relevant webpage telling them that if they can find a number for this department, I will happily ring them. From what I recall, the page had an FAQ with a question like, "Who do I contact if a service is unavailable?", the answer being something like, "You can't. It will come back on its own", though more professional.

That stopped the email chain. The service came back a couple of hours later, and dingus submitted his form.

Moral I learned from this is that if you provide a free service, don't give users a way to contact you about it. Or at least don't provide an email address that you can't throw away.
Surprised he didn't offer to extend it two weeks in exchange for $X, like $8,000.

Company might have paid and he still gets his graceful shutdown.
Are you aware of the concept of a [tarpit](https://en.wikipedia.org/wiki/Tarpit_(networking\))? Might be easier/better than returning empty responses.
&gt; So basically, what’s the morale here?

Pretty low I bet.
Sounds like a way to make money:

* offer a free API
* get it used by "everybody"
* shut it down
* restore the service only if somebody pays for it

(I know, this probably won't work in 99% of all cases, but reading through this story, I can't help but think "if it's this valuable to you, you'll pay for it, right?").
I believe there was a somewhat similar thing about 10 years ago.

Routers from one  manufacturer was hitting the NTP time server at the University of Wisconsin.  Eventual resolution was, I believe, the manufacturer gave UW money to add capacity.

To make it worse, this was a higher level server not intended for general usage, but as a feed for other servers.

I may have details wrong.

Just like this, I suspect somebody just used the first idea that came into their head.

To my eternal shame, I recall bugging the author of an open source package for help to customize it for a new CPU architecture.  Yes, I was working for a gigantic company.  It never even occurred to me to suggest we send the guy some money, or even send him a bit from my own pocket.

If it ever happens again, I think I will do one or both of those things.
You just threw away tens of thousands of dollars. You could have easily charged the set top box company $30k for restoring service.
 
Actually even more than that. Imagine if you had converted even 10% of those free users of the API to paying users. Billion dollar companies have been founded on less.

EDIT: I see you actually do offer paid plans. Good job, and good luck!



The only problem I see is that you did not reply with a quote for the requested service.
If you do it for free, people will assume your time is worthless.
Could they have mitigated this by simply having made a domain of their own and point it at the api? So that they could point it at a selfhosted solution once it is gone? Or would that be illegal?

For the set top box guys. I would have asked them to pay a silly amount of money to make it happen.


So what this is telling me is to have an escape plan ready ... something like responding random queries with dummy substitute data with increasing escalation.
So you relied on github to protect your own bandwidth costs? How is that different from someone else wanting you to front their service for free? 
    service nginx stop

Or

    service apache2 stop

?
This is great. I just made changes to my application at work to account for this. I had a (paid) fallback, so I didn't notice at first, but my integration tests let me know on the next release. It's a shame it's gone, but it's even worse that people were depending on it so heavily. 
Does anyone have any advice for running a free public API?

I've built one on Node, express, and Mongo. It's read only so I think I might switch it to SQLite. I've only hosted on Heroku, but I'd rather not host the API on their free tier because I want it to be always up. 

Any ideas? 
When the problem was malware and ransomware accessing the service, what keeps them from using someone else's or host their own? Telize is open source.
Not sure what might be the correct solution, but completly decommissioning the API? How about a registration and subscription model?
This article has my mouth wide open.
The part of this story that I don't understand is when the developer turned off the service and suddenly got an immense traffic flood, why didn't they just turn it back on temporarily, and then switch it over to returning a static well-formed (empty or otherwise obviously bogus) response?
Setting up a 'free, public API' in the first place is probably asking for this sort of problem in a way.
What "can" I put in a refrigerator? Anything that fits actually. I have left my keys and remote control there in the past...

What "should" be put in a refrigerator would be an even more interesting question.
Any object that implements the IRefrigerable interface?
I am not sure which problem in programming he is trying to illustrate with this example. The only lesson I can draw from his example is to not try to write specifications without understanding the real business requirements (e.g. what kind of refrigerator do we have, and why do we want to prevent people from putting things in it).
First, as a human who has worked in a restaurant, I can tell you that many many things **can** be put in a refrigerator. Ever been in a walk in fridge? They're pretty big.

Second, as a programmer, what are we defining as a refrigerator? 
So, continuing on how big a walk in fridge is, ever seen a fridge truck? How about a cold storage unit. 

Basically, huh?
Not sure what he is talking about but my uncle has put motorcycle parts in a refrigerator.  Not sure why they were in there but they were.
If you turn the fridge inside out, the whole universe is suddenly inside it.
Anything *putable by you* that fits in a refrigerator *accessible to you*.

A bomb the size of a bed stored at the top of some missile somewhere undoubtedly fits in some walk-in refrigerator somewhere. But I don't access to the bomb, the means to move the bomb (even if I had access), or access to said refrigerator. So I still can't put it in a refrigerator.

The most relevant part of this answer to type systems is probably the "putable by you" portion. While any sized object can be stored in an array, putting (moving) certain objects into an array can violate aliasing rules, so you aren't allowed to do that.
But if you follow the last rule, you'll get complaints that the type system doesn't stop you from pouring orange juice into the fridge.
Anything that fits and has not been specifically disallowed for this particular refrigerator.  IOW a blacklist, not a whitelist.
Can't we say, "anything that needs to me refrigerated, where need is a property of the object?" Isn't this how we do it irl? We don't look at our refrigerator and say, "what should I put in here," we look in our bag of groceries and say, what has to be kept cold?

It's also the objects that are incompatible with the refrigerator. "That refrigerator contains food, which is not compatible with this bacteria culture."
This is a problem which demonstrably cannot be wholly solved at compile time with judicious use of clever types. In the event of a full fridge, some kind of run-time `DoesNotFitException` is inescapable.

Naturally, in the event of an empty fridge and a single item which is larger than the fridge, the same `DoesNotFitException` would be thrown.
The key here is the question 'What Can You Put in a Refrigerator?' raises more questions than it answers.

Most words in the question above need a clearer definition.
Anything that fits

In the real world, the above description is sufficient

Yes, there might be discussions about what should be put in a refrigerator, and why..but none of these discussions are pertinent to the question

In the programming world, you need to define the Refrigerator class and all of its acceptable data members
Glad to see the ending.   Anything can be in the fridge.  Now of the question should be "what should be in the fridge ?" you can follow more interesting logic.
It's better to ask forgiveness than permission:

    try:
        fridge.insert(my_object)
    except NotRefrigerable:
        # TODO: Handle unrefrigerable objects
        raise
    except ItemTooBig:
        industrial_fridge.insert(my_object)
That's basically socratic arguing. 
In a real fridge, there's "affordances" and in that case, the last answer is the only reasonable one.

You people and your memory safety.

    struct fridge_t* fridge = create_fridge();
    struct motorcycle_t* motorcycle = create_motorcycle();
    add_to_fridge(fridge, (refrigerable_item*)(&amp;motorcycle));
Anything that is marked as refrigerable. Some things are refrigerable as-is (implement IRefrigerable), and some things need to be marked (wrap with an IRefrigerable adapter).
I understand his argument to be one against inevitable and unnecessary complexity, but I think that itself is quite a difficult line to draw.

I would agree with the final spec, but more simply because that is everything you *can* put into a refrigerator. His strawmen cover what *should* go in to a refrigerator based on his own preconceptions, but why burden the final owner of the refrigerator with rules that serve no purpose and, worse, may inhibit some purposes?
Wildcard, bitches! (Don't forget to catch Throwable!)
For the same reason,

    /^[^@]+@[^@\.](\.[^@\.]+)$/

is probably more-sensible for email validation than, say

    /^(?:[a-z0-9!#$%&amp;'*+/=?^_`{|}~-]+(?:\.[a-z0-9!#$%&amp;'*+/=?^_`{|}~-]+)*|"(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21\x23-\x5b\x5d-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])*")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21-\x5a\x53-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])+)\])$/

The latter is very clever and I'm sure it took a lot of effort to write. But in general "has a single @-sign and a dot somewhere after it" is good enough for many purposes. Just like "fits into the fridge". 
something is IFridgeable &amp;&amp; something.size &lt;= fridge.remainingSize
Indiana Jones.
Living creatures shouldn't even have been considered as a requirement, I've gotten live lobster to cook later on and you're supposed to leave them in the fridge.
["It's better to fix problems than prevent them."](https://signalvnoise.com/posts/2440-we-all-know-the-saying-its-better-to-ask-f)
This is exactly why I want my refrigerator open source. I don't want the refrigerator manufacturer to tell me what I can put in the refrigerator and what not. It's my refrigerator, I'll put there whatever I want.
I see the value of an arts major now. 

PUT WHAT EVER YOU FUCKING WANT IN YOUR FRIDGE. 
I'd say anything that fits into a refrigerator and is perishable.  Why would you put anything non-perishable in a fridge.  That eliminates most of the dumb stuff like salt and allows for the things like lab specimens.  The rest of the details are irrelevant and you should only go so far as how much dummy-proofing your target audience requires.  I wouldn't bother with the container thing.  No one would be stupid enough to pour something out of the container it's already in.  If they are, then they deserve the consequences.  You can't code for everything. 

But again, it means taking into account the target audience.  If the target was chimps, then I might add some extra dummy-proofing regarding containers.  Just like, if you're designing a low level interface that will be distributed widely you might code more error handling than if it's an interface that you only intend to use for one purpose that you control and it's not a critical infrastructure thing. 

This is my general philosophy when designing software and generally helps to balance time/money between coding up front and possibilities of bug fixing later on.

Edit:  good points made below.  I guess my main point was you can't code for everything, so knowing your target audience is important to know what to code for.  If the fridge is going to be in a lab and the lab already has rules against bringing food in, then you don't need to waste time designing the fridge to reject food and only allow biological samples or whatever.  Similarly, if you're designing a fridge for a kitchen, you don't need to code it to reject dangerous biological agents because those are supposed to be locked up in a lab.

A good analogy might be designing a bridge, which is something that needs to be really, really well designed.  Even with that, you don't design a bridge to take a serious impact from above, like a truck falling out of the sky.  It's possible that a plane might be flying above and suddenly drop a large object, but it's extremely unlikely.  But you sure as heck design it to take that same impact from the side in case a truck driving under it is too tall, or an airplane hits it if it's over a long, tall bridge.  

Good point was made about not just perishable items since you also might want to put items in there to cool like water or alcohol, and things to absorb odor like baking soda, so maybe perishable goods only with certain exceptions for foods that might need cooling or odor absorbing items.  The notes about rocks or books are a little unlikely and I personally wouldn't code for that unless a customer specifically asked for it and thus it was worth devoting the time/money to because you'll make more sales.

Edit 2: Having your target audience define what perishable means is also important.  In heaven chocolate won't melt, but in hell it's probably not going to stay solid for long, so that might qualify it as perishable.

TL;DR: Design around your target audience and likely circumstances. Don't try to handle every possible scenario or you'll never release a product.
    if(fridge.object_in_whitelist(object)){
        fridge.put_object(object);
        if(fridge.close_door()==false)
            fridge.remove_object(object);
    }
    walk_away();
This is actually a good allegory for demonstrating the downsides of type systems.  Looser typed languages have a leg up here.
Is the answer: JSON?
I enjoyed reading this as well as some of his other entries. Are there any other blogs/essays that take these sort of different perspectives on programming/cs?
Anything you want to make colder.
Perhaps you could just create some sort of opt-in interface for applying and receiving state. For example, a refrigerator gives items the quality of being chilled over time, so you'd probably want to create some sort of "temperature" variable for objects that are stored in the refrigerator, if needed, and then apply some sort of generic function called `chill(Object foo)` or something similar that could apply the temperature cooling effect to the object over time.

The object would be responsible for handling any of its internal state changes due to temperature or perhaps mutation by some other state, like being infested with mold, that also depends on temperature or time.

So in the end:

- Refrigerator can store objects generically, and applies its generic `chill()` function to things inside of it.

- Objects can opt into the `chill()` generic function--otherwise, it has no effect on the object--the system doesn't affect it because you didn't think about the relationship and can easily just add a new handler or it shouldn't affect it.

- Because we have designed the temperature as the mechanism for morphing the base object, we can also extend our system to interact with other systems that might chill an object, like the weather or close contact with freezing chemicals.
Why, anything from the *Füd* category: https://www.cs.cmu.edu/~edmo/silliness/burrito_monads.pdf
Pretty interesting, but a look to English and logic could have made this train of thought a lot shorter :)

Edit: Let's not lost sight of the fact that this is a computer **science** problem, not computer programming problem.
&gt;If there is no code in your link, it probably doesn't belong here.

~~Clickbait.~~It's an article of pure shit.

Edit: I can admit I was inaccurate earlier.
If you put something wrong inside it the Refrigeriler will just throw you an WrongInputExeption. 
It's not that NULL is bad. What's bad is:

- not being able to opt out of it (absence of non-nullable references)
- not being able to track it and properly handle in a safe manner
&gt; +1 for everything but especially bringing up generics. It's easy to forget there were periods of time in both Java and C#'s history where generics didn't exist.

And yet some people are reliving it with Go.
Okay, so this turned up in the accepted answer to the question:

&gt; "We were after the C++ programmers. We managed to drag a lot of them about halfway to Lisp."
&gt; 
&gt; -Guy Steele, co-author of the Java spec

......_what?_
The discussion and the comments here miss some extra options people have.

[**Ceylon**](http://ceylon-lang.org/) and [**Crystal**](http://crystal-lang.org/), for instance, use **anonymous intersection types with syntax-directed type narrowing**. One example of this is

    void printType(String|Integer|Float val) {
        switch (val)
        case (is String) { print("String: ``val``"); }
        case (is Integer) { print("Integer: ``val``"); }
        case (is Float) { print("Float: ``val``"); }
    }

Of course, then a nullable type is just some `Null|T`. [Ceylon gives that an alias `T?`](http://ceylon-lang.org/documentation/1.2/tour/basics/#dealing_with_objects_that_arent_there) and uses `if (exists nullable_object)` to clarify the type, whereas `Crystal` just does the same where [the `nil` value implements boolean evaluation, so is type directed with just `if nullable_object`](http://crystal-lang.org/2013/07/13/null-pointer-exception.html).

This is not the same as a an optional type like `Option&lt;T&gt;`. Firstly, types are automatically flattened - there is only a `flat_map` equivalent,

    foo = tautology() ? "string" : nil

    if foo
        foo = tautology() ? 1234 : nil
    end

which is like Rust's

    let mut foo = if tautology() { Some("string") } else { None };

    foo = foo.flat_map(|foo|
        if tautology() {
            Some(1234)
        } else {
            None
        }
    );

An actual map (or even mapping on errors) is syntactically identical, so the whole thing is lightweight, but it does mean the option is insufficient to represent multilevel options. You might find singleton classes help

    class NoString end
    no_string = NoString.new

    class NoInt end
    no_int = NoInt.new

    foo = tautology() ? "string" : no_string

    if foo
        foo = tautology() ? 1234 : no_int
    end

since the type is then `(String | Int32 | NoString | NoInt)` and carries all of the information, but if even that amount of flattening is too much you should probably just use an option type. Personally, I'd aim towards the automatic flattening.

Union types are also more convenient. If you have a union of type types, `T | U`, it's much like having their *implicit supertype* in that any operations which are valid for both will work:

	# Results in type Int32
	(tautology() ? "string" : [1, 2, 3]).size

	# Resuls in type (String | [Int32]), since
	# the results are of disjoint types
	(tautology() ? "string" : [1, 2, 3]) * 2

Further, it's much more implicit. Consider

    class Box
        getter :value

        def initialize(value)
            @value = value
        end
    end

    def make_box(n)
        case n
        when 1, 2, 3
            Box.new(n * 2)
        when 4, 5, 6
            Box.new(n * 3)
        end
    end

There are no annotations, no `Some(T)`s, no `None`s, yet `value` is a checked optional type in the return of `make_box`. Neat.

Anyway, the point I wish to make is there isn't a black and white gradient between what functional languages do and what OOP languages do. There is a massive design space to explore, and one shouldn't get bogged down on binary questions. Also I just wanted to show off this cool thing.

Hmm im not sure why NULL is considered a bad idea? Can someone explain this to me? I have never had any problems with NULL, and have used it extensively in almost all of my programs to great effect. 
&gt; Consistency of semantics is important; if we have reference TheKingOfFrance equal to null does that always mean "there is no King of France right now", or can it also mean "There definitely is a King of France; I just don't know who it is right now"? or can it mean "the very notion of having a King in France is nonsensical, so don't even ask the question!"? Null can mean all of these things and more in C#, and all these concepts are useful.

This isn't a reason for null, this is very much a reason against null, and why having null is just bad, regardless of the "design considerations". Null should only be exposed through effort as a means for backwards compatibility, never as a core feature.

How to solve this? Option/Maybe and strong typing.

    sealed trait Ruler
    case class King(...) extends Ruler
    case class Queen(...) extends Ruler

    val currentRulerOfFrance: Option[Ruler] = ...

&gt; &gt; Also implementing an option type isn't really much more complex than null references.
&gt; 
&gt; I beg to differ! The design considerations that went into nullable value types in C# 2 were complex, controversial and difficult. They took the design teams of both the languages and the runtime many months of debate, implementation of prototypes, and so on, and in fact the semantics of nullable boxing were changed very very close to shipping C# 2.0, which was very controversial.

Not a single sentence of that paragraph is contradicting the statement that an option type wouldn't be more complex than null references. In fact it completely ignores the possibility of an option type. How is the fact that some debate was very controversial even tangentially related to a comparison between nullable values and an option type?
Language design is hard

Pundits and critics may invent and promote rules that they claim are complete and useful

Actual language implementers and users find all sorts of messy complexity that doesn't quite fit into the neat rules


Because it is easy. UI systems still implement api to make alerts/popups, and I dare say I have never once encountered a situation where it was the right choice for the user. It was just the easiest for the programmer to implement.
I think a lot of people also miss the point of null derived errors.  Instead of just wrapping your statement with 

    if(obj != null){}

Try to find what is nullifying the object if possible. The above code can make sense but generally if things are being passed as null (when not intended to) there is a problem somewhere else in the code.
NULL isn't inherently bad, the real problems are tracking/handling... and the lack of restricting it from valid values.

In Ada there's this really useful idea called subtyping^1 where you restrict the valid values that a type^2 can assume. So, if you were modelling Speeds you could say `subtype speed is Integer range 0..200;`. (Fixed point would probably work better, but this is just illustrating the concept.)

The non-negative and positive subtypes are so useful that they're predefined in Standard by:  
&gt; `subtype Natural is Integer range 0..Integer'Last;`  
&gt; `subtype Positive is Natural range 1..Natural'Last;`

Well, we can do the same thing with access types (roughly pointers) like so:  
&gt; `subtype Safe_Access is not null Access_Type;`

That takes care of the issue of being forced to treat null as a valid value; the tracking issue is still much harder and TTBOMK usually handled by storage-pools and/or scoping.^3

^1 -- Yes, there is the OOP 'subtyping' idea, that's a separate thing.  
^2 -- A type is a set of values and a set of operations that can act upon them.  
^3 -- One of the nifty things about using an access discriminant is that the lifetime of the access is exactly the same as the type's.
Hmm, I use a dynamically typed language (scheme). Does this issue only come up in statically typed languages? I use null for empty collections, and I use boolean false for "does not apply" values.
null is useful when you know how to make it work for you
What is the alternative? C# tried to put in a whole Nullable system.  But there is absolutely no difference.

Instead of 

&gt; if ( x != null) 

It becomes

if  ( x.HasValue )


Is this really a big problem in practice? Whenever I read an article like this which discuss an objectively small language feature (relative to the other stuff a language provides) and its advantages and disadvantages, and discuss it with colleagues (game developers), we agree that it's such a non issue. There has never been a case when we have lost a significant amount of time tracking down a bug because something is null where it shouldn't be (I would define significant as something that takes longer than a couple of minutes and disrupts your work flow or brings you out of the zone). 

The same goes for memory management woes in languages like C++ or "a pointer pointing to something it shouldn't" etc. These seem like such trivial issues that just don't cause any real world problems for us. 

By the way I don't mean that we never run into "woops something was just null here" bugs. It's just that these are in our experience so trivial to detect and fix that (combined with the infrequency of these bugs) its net effect on our productivity is on the level of having to type "nullptr" instead of "NULL", requiring us to type 3 characters more. And I'm saying this from the perspective of someone who gets annoyed in a major fashion when there isn't a bash script that combines 3 command line cmds for me so I have to type them out manually.

Also languages like Rust that try to solve these (from our perspective) small issues tend to introduce a myriad of much larger problems as a trade-off for fixing these small problems. 

Here's a hypothesis: I get the slight feeling that this is one of these things where this particular feature directly represents something that exist in all programs (invalid references, perhaps I could say "invalid state" as a larger issue, or as another example, memory allocation and deallocation when you're talking about manual memory management vs garbage collection), and various languages try to work around it because developers like to stick their head in the sand and not deal with the underlying realities, but I get the feeling (which I cannot substantiate or prove at all) that there are no optimal solutions to any of these problems, you can only try to hide them with various language abstractions which come with costs of their own. At least until the underlying hardware representation of programs changes to support completely new paradigms.
Good to see some reasoned argumentation about `null` instead of the regular hysteria "billion dollar mistake" nonsense for a change.

The short answer to OP's question is: because `null` is not as bad as people say and it comes with large amounts of convenience.

Of all the exceptions that a Java application can throw, `NullPointerException` is close to my favorite because these are trivial to find and fix. Which means that with decent regression testing and usage, your program becomes reasonably quickly NPE free.

Having said that, I'm certainly liking the more modern approach to nullness (e.g. Kotlin's question mark types) and I'm glad that the programming language landscape is finally moving away from this problem. But really, it was never as bad as portrayed.


So, correct me if I'm wrong, but I don't think the question is particularly meaningful because null isn't really "implemented".  Null itself is a concept, a property of languages and programming, which is encapsulated in the "null" implementations we're familiar with.  Getting rid of the expression of null and the various ways of handling it (or not) doesn't solve the problems but instead redirects them. I'm seeing examples in this very thread, with people pointing out languages which don't allow for null references in the same way that Java and C++ do, and there are always problems, restrictions, or at least *weirdness* which arises elsewhere because of this design choice.

Maybe I'm getting too wishy-washy and conceptual here though.
I agree that non-nullability by default is the right thing for most "normal" code, and more languages should support it.

However, in data-like objects with tons of fields, making them nullable/optional might be a better answer. For example, any data structure representing an HTTP request must use nullable/optional all over the place, because most parts of a request can be omitted. The same is true for command line arguments, config directives, HTML tag attributes, properties of UI controls and OpenGL objects, etc. A good rule of thumb might be: if a struct is likely to be versioned, make its fields nullable/optional by default.
&gt; C# 1.0 did not have generics,

Oh, so they too did that blunder… What were they thinking?

&gt; Should .NET have slipped for two years while the runtime team added generics, solely to eliminate null references? 

Yes, they should have, but not to solely eliminate null references. An option type is good, but full algebraic data types are more generally useful. I wonder why they didn't make it into the language. Was that deliberate, or were they ignorant of that 25 years old feature?

Fair enough, but that is true of so many of our tools.  
Can someone ELI pimple faced kid?
I never came across it when I was C programming, I only discovered null when doing java in a database world.  null being a common value for uninitialized items.
Not using null is a new concept to me. Say you have a method that takes an email address as a parameter and returns a User. What should be returned if the user isn't found (assume vanilla Java)? In my experience, null is the easiest way to deal with this.
Null is to useful to not included but its existence is a problem that can be solved I think the new Optional&lt;T&gt; class in Java 8 is maybe a step in the right direction but it is far to cumbersome. If I was designing a Java-like language I would include a "nullable" attribute

You can cast a regular variable to "nullable" but not the other-way around unless you explicitly check.

For example:

    public nullable Foo getFoo() {
        return null; //Fine
    }

    public Foo getFoo() {
        return null; //Compiler error
    }


    Foo someFunction() {
        nullable Foo myFoo = getFoo();
        if(foo!=null) {
            foo.bar(); //Fine
            return myFoo; //Fine
        }

        return null; //Compiler error
    }

    Foo someFunction() {
        nullable Foo myFoo = getFoo();
        foo.bar(); // Compiler error
        return foo; //Compiler error
    }
 
It's interesting to me that some of the most interesting StackExchange threads are closed for some reason or another.
If no one mentioned how Hack lang deals with this, someone should. Nullable types in Hack must be checked for null before used on right-hand side, like

    if ($x != null) {
      // safe to use $x here
    }

They must also be declared as nullable, with a question mark like `$x : int?`.
What is 1/0?  We have null for the same reason mathematics has undefined.  1/(x^2 - x)  is only valid if x != 0, and not checking for it is an error.  Your C function may be valid for all inputs except one, null, and not checking for it is an error.


In my experience most of the problems with nulls are directly related to OO style of programming. Any time you call a method on an object there's a risk of that object being null. This leads to fragile and error prone code where the programmer has to remember to pepper null checks everywhere.

With the functional approach the functions aren't tied to an instance of the data and majority of the code is written by combining functions from the standard library to transform the data. Even when you write you own functions that have to handle null, the check will be encapsulated in the function.

This way the edge cases can be handled by these reusable functions and null values can bubble up. If you care whether the value is null or not then you only need to check at the end of the transformation as opposed to having to pepper these checks all over the place.
Also, the state and culture of the field matter.  In 2000, there were a number of very good and very experienced developers who were absolutely convinced that they needed null and that there was no other coherent way to represent an unknown, missing, or non applicable value or that if there was, it's distinction from just having nulls was irrelevant academic nitpicking.  

It's not a short or simple conversation.  Even now, you get people who can't or won't hear it.  Back then, this was a lot of people.  Microsoft would have lost the majority of their user base by implementing Option instead of null, especially if you consider the other language features that you need to really make Option work nicely (generics, pattern matching.) 

In 2015, when you have a lot of the existing community understanding this, writing blog articles on it and explaining it, it's a lot easier for a new dev to say "yeah, that's easy and makes sense.  people were so dumb back then." 
Because not everything is something. As for Option types, it can still be null in effect, it just forces you to write the equivalent of a null check so the code will compile.
All glory to [**?.**](https://www.youtube.com/watch?v=8AOfbnGkuGc)!
It's not bad. It's just easy for inexperienced programmers to shoot themselves in the foot with it. Like some developer of MySQL did. 
nulls are not inherently bad. Programmers are inherently lazy, however. 

I'll never understand the problem with null. How the fuck else are you supposed to represent a value or type that hasn't been set? Use zero for everything? Oh okay so that customer's order, where they ordered item 0 for $0 and 0 quantity, being sent to house 0 on street 0? Yeah, exactly.
If we're gonna get rid of NULL, we should get rid of 0 from the number system too.  Why do we have a number that means nothing?  It's crazy!
i like null 

edit: looking at the down votes it appears I was wrong. i apparently dont like null. 
&gt; Null values propagate when used, but null references throw exceptions when used.

Depends on the OS. Null is zero and zero is a valid memory address on some systems.

I mean, I can do all sorts of crazy memory accesses in C if I want with shitty pointers that will blow up.  Why not have some value to help indicate they're shitty.  (nullptr is the new thing btw)

Inexperienced programmers obviously have this complaint a lot. 
Programming languages are written in a fairly liberal manner with respect to how they should actually be used. For the most part they are supposed to be general purpose tools. 0 messes up a lot of calculations in mathematics and yet in some places it is very useful...
I love how many languages seem to tout 'thing that saves you from null' but it turns out when you dig that it is basically 'we renamed null to something fancy'
Because null is a fact of life you can't simply ignore, but modern languages tend to work around the consequence of null, namely Null Exception.

Newest version of C# has new operators to embrace null. There's object**?.**DoSomething() that calls DoSomething() only if the object is not null, otherwise ignore the statement and move on as usual. Instead of null checking array and range, you can just use **?** like customers?[5]?.Orders?.Count().

The new Kotlin language by IntelliJ favors non-null by default and the compiler expects all normal variables to be non-null.
Null is necessary. 
The **same** people who don't check nulls are the **same** people who wont check your invalid optional value

They wont wrap in try catch and if forced to (java) wont put **anything** useful in the catch section

**Null isn't a bad guy**, in fact i love nullexceptions they are a blessing, hidden bugs like an unknown state that mutates on and on, race conditions, and hard to reproduce bugs.. Those are the evil ones

What we need is **not** more restrictions by the compilers, but **less**

We need code contracts, we need to be able to mark a method as **"if you dont check the return value for x, dont compile"**
What is a "modern language" and why should everything be done using one?


Smalltalk has the singleton class "nil," which works just fine.

What do you use for things that haven't yet been used/defined/if not some signal that these things have not yet been used/defined?

Let the compiler do it for you? Great. And so, the compiler has a concept of things not yet used/defined. Isn't that a "null" or "nil?"
It's not bad

Next question?
I'm glad *algorithms* worked out. Solid idea.
Does anyone know what is meant by Capabilities? I googled for capabilities in computer science but the results are all over the place.
So the biggest gripe I have is that the author is looking at the evaluation criteria from a subset of software development, although probably the most common subset, which is web/IT style develoment.  If we look at a different slice, like safety critical embedded systems we see strongly typed, verifiable languages, and solid software engineering practices followed.  If we look on the web or in a purely research context, then no, we do don't see those used.  

If we look at the embedded space we'd see many of the yes and no inverted.  For example, many embedded devices run on some variation of a RISC design.  This can range from Rad-Hard PowerPC/SPARC to tiny little ARM Cortex devices.  Garbage collection where real time is important is not practical because it introduces non-determinism.  That's why real time Java, for example, would disable garbage collection.  

If it hasn't already done so, it may come to pass that more software will actually run on embedded, situated in the world devices than on laptops, desktops, tablets, and servers.

We've become more discriminating.  When I finished my master's in the mid-1990's, everyone seemed to be focusing on each piece as the next big thing that would come to dominate the industry.  Now we realize that the industry is no monolithic and there are several different areas of focus.  

So now we don't say everyone should use only strongly typed, compiled languages.  Now we recognize that in some areas these languages are a better choice than weakly typed or dynamic languages.  Instead of saying we are all going to use RISC for everything, we recognize that RISC designs have been able to embrace a lower power and size envelope, making them useful for certain types of devices but probably not your laptop.  
I'm going to have to disagree with some of this. Maybe this is just nitpicking but:

&gt; RISC

&gt; If this was a Maybe in 1999 it’s certainly a No now. In the 80s and 90s a lot of folks, probably the majority of folks, believed RISC was going to take over the world and x86 was doomed. 

&gt; If there’s any threat to x86, it’s ARM, and it’s their business model that’s a threat, not their ISA.

While I think it's pretty clear that x86 is dominating the server, workstation, and desktop spaces, ARM is undeniably huge. Android and iOS have powered billions of devices sold over just the last 7 years. In my opinion, this is a solid "Yes".

&gt; Fancy type systems

&gt; It depends on what qualifies as a fancy type system, but if “fancy” means something at least as fancy as Scala or Haskell, this is a No. That’s even true if you relax the standard to an ML-like type system.

I think I would agree in general that Haskell and Scala haven't caught on to the same scale that Java or C# have, but I would argue this is at least a "Maybe" possibly even a "Yes". I think fundamentally the reason these languages aren't more popular is that startup culture has decided that system stability and bug-free code isn't as important as shipping code as fast as possible. It seems that once these companies become more successful, this changes. Facebook cross-compiles PHP into C++ using a custom compiler they wrote in Haskell. They also wrote their spam filtering in Haskell. Twitter was written in Ruby back in the day and now uses Scala among other more traditional languages. Furthermore, in markets where real money is on the line and there can't be bugs like financial services, languages like Ocaml are popular.

&gt; Maybe I’ll get to use F# for non-hobby projects in another 16 years, but things don’t look promising.

I've talked to quite a few developers that have internal-facing projects written in F#. It's true, actually shipping F# is rare but it seems to be catching on for build scripts and internal projects. We've been using F# at work for 3.5 years now, including in production. 

&gt; Functional programming

&gt; I’d lean towards Maybe on this one, although this is arguably a No. Functional languages are still quite niche, but functional programming ideas are now mainstream, at least for the HN/reddit/twitter crowd.

While I think most people think of Scala, Haskell, or Ocaml when they say "Functional programming", it's important to note that FP also includes the Lisp family of languages as well as Erlang. Erlang has been popular in telecom for decades now and several of the "billion dollar startups" in last few years have been built on top of Erlang. Additionally, it's impossible to argue that Javascript isn't a functional programming language. Especially when most modern JS libraries make heavy use of higher-order functions. In my opinion, this is a "Yes".
Topic|1999|2015
-------|-------|-------
Web Scale|What?|YES!
Javascript|NO|YES!

The author's definition of "Worked" seems to be something like "What is commonly used throughout industry". I think a better judgement (more inline with the original talk) is "proven itself to be a useful and valuable contribution", regardless of adoption.

With that in mind, basically everything here is Yes (with the possible exception of "Software Engineering", which as a research field has yet to permeate into industry at all, and IMO, is of dubious value).


Food for thought:

What new (since 1999) technologies and concepts would you add to the list? In particular, which ones would go under the No column?
&gt;Every language that’s become successful since 1999 has GC and is designed for all normal users to use it to manage all memory. In five years, Rust or D might make that last sentence untrue, but even if that happens, GC will still be in the yes category.

I always thought D was garbage-collected, or isn't it?
As someone with only a passing understanding of scripting (by no means a programmer) I love these types of articles. Probably not the main topic of /r/programming, but delightful nonetheless.
Also big in the 1990's but gone today: 

* Significant amounts of hand-coded assembly.
 * One project I worked on was perhaps the fastest fixed-point 80x86 FFT (for X &lt;= 3), who's inner loop modified an instruction from a + to a - right before the CPU's [prefetch queue](http://www.rcollins.org/secrets/PrefetchQueue.html) fetched it from memory.    Currently even on DSPs only tiny bits off assembly are used
* [Waterfall software design methodologies](https://en.wikipedia.org/wiki/Waterfall_model) - and even though small companies realised they were stupid, they were even [mandated by some government contracts](https://en.wikipedia.org/wiki/DOD-STD-2167A). 

* [Java chips](http://www.developer.com/tech/article.php/610041/A-Java-chip-available----now.htm) were very exciting in 1999
&gt; In retrospect, the reason RISC chips looked so good in the 80s was that *you could fit a complete RISC microprocessor onto a single chip, which wasn’t true of x86 chips at the time*. But as we got more transistors, this mattered less.

What in god's name is he talking about? Is he talking SoCs? Because when did intel **ever** have a commercial x86 processor that came as an LSI chipset? 
Intel the biggest x86 supplier sold 400 million CPUs last year. ARM the biggest RISC supplier sold a billion! RISC IS DEAD!! 
&gt; Software Engineering

I thought "software engineering" involved the practices behind developing software like Agile and UML. I also thought those being widely used right now?
This was a great article. I was barely a teenager in 1999, and the following years was when my interest in computer science peaked. I lived through some of the stuff mentioned in the article, like the failure of Alpha. Even in 2015 I still have a dream of using a desktop or laptop computer that's PPC or RISC based. Never happened, and may not ever happen, although if I ever get my hands on one of those Chinese MIPS systems I guess that'll be good enough. (I don't even know if MIPS is related to RISC, but it still sounds cool and not at all x86-y.) I have mobile devices that use ARM, so I guess I should just be satisfied with that. I also remember what a spectacular failure IA64 was. Anyone remember that?
I await his predictions for 2031
This guy doesn't even know the difference between parallelism and concurrency, I wouldn't trust him with a deep understanding of computer science.
If "fancy type systems" is a no because those languages aren't generally used in industry, then "formal methods" should absolutely be a no.  You're telling me there are companies that are writing Javascript code and then in the same breath proving that Javascript correct?
RISC vs. CISC

There's a lot of debate around the fact that internally CISC machines implement a quasi-RISC instruction set. This, actually, is true for a number of high-end RISC machines that performs a great many similar optimizations at the front-end. A superscalar micro-architecture could not be implemented if not for large macro-operations to be split into micro-operations that can be readily performed on individual functional units.

People often neglect to consider that one of CISCs proposed weaknesses is also a great strength. The considerable flexibility in the instruction encoding enables the architecture to be trivially extended over generations. This is not the case in RISC architectures. The Intel architecture for example has been extended with all manner of vectorization and transactional extensions that would have otherwise been very difficult to introduce after-the-fact into an established RISC architecture. In the transactional extensions (?), Intel have been able to modify an existing set of instructions by simply prefixing established instruction encodings with an additional byte. This is unthinkable in RISC architectures. It's true that the front-end of CISC architectures is very complex, however this is a solved problem, and has been for a many of micro-architectural generations.

CISC isn't going away, and even with the advent of heterogeneous programming models, I'm would believe that RISC architectures may have to evolve to become more CISC-like to be able to keep up.
I think everything in that table from 1999 is in the "yes" column in 2015. The irony is that the stupid article claims RISC failed, but there are literally billions of devices using them today (aka every single tablet and smartphone to my knowledge).
There are more risc computers than anything else. I.e. ARM so it's a blinding success ! So that part is wrong. So pretty clueless really.
&gt;the contiued [sic] demand for compute

What does this mean? When did we start using "compute" as a noun?
Wrong title: this post isn't about what *works*, but about what's *popular*.

The so-called fancy type systems for instance definitely work. They're just not popular.
I am pretty sure RISC should be a solid *Yes*, unless smartphones aren't a thing. 
I disagree with a lot of the author's assessments. For what it's worth, my PhD is in formal methods, fancy type systems, and functional programming ;)
I've found functionall programming to be quite efficient and time saving for one off projects.

If you're not building libraries of reuseable code, OOP is a tremendous waste of time and overkill, like using a hammer to break an egg.
I gotta completely disagree on the "fancy type systems" point. Sure, the languages with type systems considered "fancy" *by current standards* don't see much use in industry (looking at you, Adga, Rust, etc)... but *by 1999 standards*, even modern C++ has "fancy" types, and C++ is ubiquitous.

The fancy type systems of 16 years ago haven't gone anywhere, we've just stopped thinking of them as "fancy".
&gt; Fancy type systems - It depends on what qualifies as a fancy type system, but if “fancy” means something at least as fancy as Scala or Haskell, this is a No. 

&gt; ...

&gt; Maybe I’ll get to use F# for non-hobby projects in another 16 years, but things don’t look promising.

&gt; ...

&gt; Functional programming - I’d lean towards Maybe on this one, although this is arguably a No. Functional languages are still quite niche, but functional programming ideas are now mainstream, at least for the HN/reddit/twitter crowd.

 If you're a noob don't brag about it for the "HN/reddit/twitter crowd"...
&gt; F#

Well, some companies are starting to adopt it. Jet.com, in particular, uses it for their order management/processing systems and pricing engine, IIRC. 

That said, I really like F# and would love to use it seriously again someday.
I'm confused by this one:

&gt;"Parallelism"
&gt;This is, unfortunately, still a Maybe.

Much of the advances in existing languages/new languages in the past 10 years focused around concurrency. Even in dinosaurs like Java it's easier than ever. Actor-style frameworks spring up on what seems like weekly basis. How is that a "Maybe"?
&gt;  Boy, would I love to be able to do everyday programming in an ML (F# seems particularly nice to me), but we’re pretty far from that.

Why?
As someone who was very young in 1999, it's really interesting to hear the perspectives of someone who was in the thick of it at the time.  Weird that paralellism is still a maybe, even with the custom GPUs nowadays.  

2030 prediction: Absolute yes.  
My quick response, just the ones where I disagree:

RISC - yes (as other comments have said, phones)
Fancy type systems - Maybe. I am interpreting "fancy" to mean "dependent". Typelevel programming is promising but far from mainstream. However the popularity of type-hacking in scala, haskell, idris etc is on its way up, not down.
Functional programming - yes. Even java has lambdas now. In the vast majority of cases, not doing functional programming is wrong.
RPC - no. These have been superceded by REST and if REST doesn't suffice, distributed computing.
Security - no. We are still doing security wrong

I am very sceptic about GUI:s. They compose badly or not at all, they introduce funny behaviours when copying representational data from one point to another. All in all, I would say that they don't work at all, but that is an opinion. Not many options around, but I'd say that a good option could be a combination between the dreaded terminal and the gui. (Un)fortunately, the terminal needs a redesign on many areas to.
Informative, sure, but elitest nonetheless. 

How about a link to a good explanation of the topic? I happened to know about RISC as I was following PCs at that time. But I know "software engineers" (that's their job title) so "software engineering" is a Yes...

Unless, author, you'd like to explain your definition of Software Engineering and Yes in this context.
There is no measure. Back then you could draw all the application's classes on a single diagram. Where is something like "OOP worked"? That was hell of a topic.
Computer Science and IT are separate, but related disciplines.

In order to innovate in IT, you need customers; which is why the computer science approaches usually fail.

Anyways, some critiques from someone with tens of thousands of customers.

    Parallelism

Multi-core systems and technologies like RSS have been (and continue to be) disruptive technologies.  

In 1999 I built a nationwide 10Gbs content delivery network, for about a million dollars.  I can build a 20Gbs CDN on a single 1U Dell server, for about a grand in 2015.  Assuming I have somewhere to host it, that is!  Entirely due to parallel computing technologies.

I also use gnu parallel every single day, I count on it for many of my scripts and work extensively with HPC multi-threaded apps, like the suricata IDS.  Windows 10 and DirectX12 are fully-multithreaded, which will revolutionize PC gaming.  

    RISC

The ARM architecture is on a billion+ devices and all modern x86 systems are considered hybrid CISC/RISC systems.  They have a CISC front-end, which decodes instructions into micro-ops for a very RISC-like core.  

    Garbage collection

Lots of companies are migrating Java apps to C++ due to performance and reliability issues with Java's non-deterministic garbage collector.  This is what Bjarne is doing at Morgan Stanley, actually!  GC is firmly in the "Maybe" category, as it is still appropriate for many applications; particularly non-realtime ones.

    Reuse

Yes, Open Source rules.

    Capabilities

Wat.

    Fancy Type systems

Phd fodder.

    Functional programming

Phd fodder.

    Formal methods

Phd fodder.

    Software Engineering

Agree with all the points here.  Again, if you want to innovate in IT, you need to have customers first.

    RPC

Thought this was big in 1999 as well, but maybe that is a bias on my part.

    Distributed systems

We call it the cloud now, so yes.

    Security

My gig and a massive business.  It has more in common with quality-assurance, accounting and zoology than computer science, though.  
Functional programming was way before 1999, Erlang was in 1986...
He seems to be ignoring Swift at least in the fancy type system category and arguably in the GC and FP categories. It's a niche language but Apple platforms are a pretty large niche, with Objective-C and Swift occupying positions 14 and 15 in the latest TIOBE index. And if Apple comes through with open sourcing it, there's potential for it to grow out of it. It's a decent enough type system, instead of GC it uses compile time reference counting, and it at least encourages functional programming, even if it doesn't force it on you.
Functional programming a niche?  JavaScript anyone?
&gt; Every language that’s become successful since 1999 has GC and is designed for all normal users to use it to manage all memory. In five years, Rust or D might make that last sentence untrue

:(
Love Julia. Do lots of scientific programming (mostly in C or Python) . Can't wait to shift my code over Julia.
Congratulations to the Julia team! I hope this lets them accomplish all their most important goals! :D
I love Julia. The biggest downside is it's still dynamic, like other technical computimg languages. Also type instabilities can bring performance down,  where other statically tyed languages achieve the same but with good performance. I really like julia, it is the best technical programming language, but certainly i would not use it fr large projects... the same as i would prefer java over python for such projects
Great. We need new scientific computing languages like this. Python (or R or Matlab) is great, but slow. C/C++ (or Fortran) runs fast, but can be painful to read, write and maintain.
Julia looks great, but the 1-based indexing bothers me.

/edit: I get downvoted for expressing a personal fact? I didn't say "it sucks" or "I'll never use it because of X!" just it "bothers me". Which it does. Cripes.
I'm really waiting for the moment when we can finally really forget C in application code, for performance sensitive parts, and can use something much less awkward.

Some people say it's a problem that julia is dynamically typed. But everyone who wrote serious C code can say that C is practically untyped.

I still have nightmares of void pointers
Start-ups, privately invested companies, and small fresh public companies all really value the young and naive that will work ridiculous hours for $60k. These companies also have a high percentage of failure, and also a high percentage of turn-over. Many engineers stick around for 2-3 years, then realize they're working insane hours and being massively underpaid. They either move on to a mature company, start their own company, or move to another hot start-up type where they do the same cycle but getting $70k this time.

However, there's also mature companies that treat developers well and compensate fairly. They're just not as flashy and don't get the hot press coverage.
if you look around Silicon Valley...most people who work in circuits are older, and not many young people seem to be in that business. When circuit work wasn't make people rich anymore and software took over the Valley, young people flocked to software. Now if you look at "traditional" software (i.e. you buy it and run it), it trends older. In the 90s, young people flocked to web work instead of places like Sun or Oracle. When mobile emerged, young people flocked to that. The correlations of the "hotness" of a market and the average age of the workers are like tree rings...I can tell you when a Valley trend was hot by looking at the age of the average worker.

Once a new market emerges that makes people rich, young people will flock to that. The web industry will age since the workers will tend not to be interested in taking the big risks to migrate to The Next Big Thing...

The only question is what the Next Thing will be...robotics? 
&gt;Posted Aug 28, 2010 by Vivek Wadhwa
60k for a fresh grad in silicon valley? God, please ... show me where you find those! 
Another factor of why older programmers might make less could be the bullshit-intolerance level that grows as you grow older. Never have I ever seen any other type of industry that systemically tries to perform sophisticated work with so little specifications/design as in software. I used to accept being pressured into programming something before the specifications were well documented. After being burned so many times, and redoubling my efforts when something goes wrong, often at my own expense, I realized that some managers knowingly capitalize on a young programmers need to prove themseĺves for self-respect, and their fear of a failure early in their career (if you want to stay programming as a career, you need a good recommendation/resume if you don't have a lot of experience). I no longer accept such rediculousness, and throw their pressure tactics right back in their faces by asking them to provide documentation as per IEEE specs, or give me the time and resources to do so. I can only see this attitude of mine growing larger in the future. 

Tldr: The young workers are exploitable.
And how many of these neophytes are working on bullshit consumer applications as opposed to doing enterprise or B2B software, working in test driven, continuous integration environments with multiple legacy systems, coordinating with global teams or reporting to multiple stake holders?

My bet is "not many."

If this means more senior engineers are available for work, fuck, send them my way. I have tons of clients and contacts with cash who need someone who has been writing code for more than 20 minutes and understands the fact that big enterprises are marathons, not sprints, and that long term conscientious thinking is what wins out over kludge hacks 99 times of 100.
&gt; And engineering is an “up or out” profession: you either move up the ladder or face unemployment.

Where?  In companies that have no concept of a tech ladder?  Coding mills?  Promotion only through a switch to management is a rather discredited style and has been for many years.

Normal tech companies have some solution to this, but the most common is to have a tech ladder and a management ladder, where one can thrive (as seen in compensation, influence, and freedom) either as an individual contributor or as a team leader.

What I see is "hey, you have lots of experience in this field; we really want to hire you for quite a lot of money".

It's ok, there's tech outside of the Valley.  Older coders are valued in industries where grown up code is essential, e.g. military, transport and finance.
Silicon Valley is absolutely ageist, and that sucks; three of the best people on my team are in their fifties (one hardcore algorithms type, one mobile developer, one web); my conclusion is that there's an arbitrage opportunity here if you can build an age-diverse culture.


&gt; At greater ages still, salaries started dropping, dependent on the level of education. After 50, the mean salary of engineers was lower—by 17% for those with bachelors degrees, and by 14% for those with masters degrees and PhDs—than the salary of those younger than 50.

This is a natural result of quickly rising market rates, not because your salary actually goes down at 50. 

The rate of increase in hiring salary for software developers in the last N years (for almost any value of N, up to maybe 20) is higher than raises have been staying at a particular job. So your salary relative to the market goes down as you stay at a job, which means that the numbers are biased towards younger people entering the market more recently. This is exacerbated by younger people being more willing to change jobs for a higher salary - they'll keep up with the increasing salaries around while the older people are stuck with a couple percentage points raise every year. It does *not* mean, as the article concludes, that your salary will go down when you get older.
I think there are two underlying issues.

One - companies don't have good strategies about career development of a technical person - at some point, to get a higher salary, you have to move to managing people. Overall this is a terrible idea, because you turn your best developers into your worst managers, but the business world is full of terrible ideas. So the salary drop in the older ranks of developers is a result counting a lot of people that aren't being promoted for whatever reason. Lack of promotion is corellated with lower salary. Personally I feel managers should be paid the least because they do the least, but hey...

Two - an experienced developer does not really churn out features faster than a junior developer. They are more likely to do it right, avoiding bugs, rework, security and performance issues and so on and forth, but that is not how the business world sees things, it comes down to "features now!!!" (and the issues can be dealt with by the next manager who replaces me after i get my bonus and fuck off). So in the eyes of those people. $60k vs $180k for the same output - you choose the 60. Basically the value seniority provides isn't acknowledged.

If it seems like I am treating business people in the world of software as stupid and unethical, it is because they *are* stupid and unethical. We could change the world. But no, let's just focus on the next quarter....
&gt; At greater ages still, salaries started dropping, dependent on the level of education. After 50, the mean salary of engineers was lower—by 17% for those with bachelors degrees, and by 14% for those with masters degrees and PhDs—than the salary of those younger than 50.

How much of this is selection effect? I.e., strong engineers get promoted to engineering managers, leaving their lower earning cohort to languish, and freeing up  a slot for another young and able engineer?
&gt;The young understand new technologies better than the old do

lol
you get what you pay for, cannot stress that enough
I worked at Google in 2013 when I was 62 and I didn't notice any age discrimination. This is just one data point, and just my experience.
Everyone here in Silicon Valley knows about the ageism. It's not actually a secret.
As a 57 year old software developer I can tell you that the best way to make a decent living at this age is - freelance software development  consulting.  I've been doing it for about 19 years now, and I haven't looked back - hourly wages are quite high, and you have freedom from office politics.  The important thing is - keep up with latest technology and programming languages and frameworks.  And do colour your hair to get rid of the grays. 
In a few words. If you do not want to be discriminated by your age learn the programming languages use in the enterprise world:

Java
C#
C++
Cobol
Javascript

Enterprise companies will be using those language for years to come. Let the youngsters play with Elixir, Scala, GoLang, Julia, Rust, Clojure etc.
Reddit, this is an Older article than you realize
I had an interview recently in SF that was going great until he asked me how old was my son and I said 19.  Things got awkward.
What secret? Everyone has been talking about this for years...
It says right there in the article that they hire young people because they can pull all-nighters. They are not ageist, they are all-nighterist, or workalotist.
The commenters in the article asserting that clothes don't matter are in for a bad surprise, although they may never realize it. The clothes you wear are an important signaling tool, and make sure you're sending the signal you want to.
It has nothing to do with age, it has everything to do with money. They want cheap labor, and as a correlation, young and stupid work for cheap. That's all there is to it. 
It really is all about age - don't even look at the salaries. I know of older people  with more experience willing to work for entry level wages, and they are still unable to get them. Nobody will admit to it, but it's largely about ego, whether you feel intimidated or not by someone's age or whether you can have your own loyal 'padawan' you can tower your experience over. People will instead claim they are selling themselves short, or if they don't lower their salary expectations, what the article states, but a lot of people simply don't want to look behind the curtain.
&gt;Why would any company hire a computer programmer with the wrong skills for a salary of $150,000, when it can hire a fresh graduate—with no skills—for around $60,000?  Even if it spends a month training the younger worker, the company is still far ahead.

Both the numbers and the logic make it quite obvious that the author of this articles knows about Silicon Valley only from reading other, similar articles.


Its really not. I was the youngest by far for a long time. In fact I would almost say there is bias towards older people sometimes because they are the people who know other people and in the valley you get your jobs from people you know.

Maybe at big companies, but at startups this article seems wrong in my experience in SV.
This is 5 years old...
This article is from 2010. Probably not much has changed. BUT 2010!!1
Oh wow.  I was going to point at http://badmicrosoft.com, but it's gone(!!!).

Found it in the Internet Archives.  The comments are what make this site significant, btw.

https://web.archive.org/web/20150205230328/http://badmicrosoft.com/
I am just finishing a phd. someone please give me a job. thanks.
I'm not in Silicon Valley.  I was always interested in programming, and got a Computer Science degree in 2010 after going back to school (already had a Bachelor's and a Master's).  Developed a decent portfolio (web applications) and applied to the few local companies that were available as well as elsewhere in the U.S.  I was told, straight up, by two of those companies that I was too old.  One of them also mentioned that I had a family - that was also deemed undesirable.  Never got a full time programming gig.  Luckily I still have my previous career (mental health).  Another fellow "non-traditional" student wasn't so lucky.  He got out of the military and decided to get his degree.  Our younger colleagues were snatched up immediately.  He's talented, but still hasn't been employed in the field 5 years later.  
&gt; After 50, the mean salary of engineers was lower

I'm not sure how you tell the difference between numeric age and generational experiences here.

I'm over 40 but under 50. I'm *just* young enough to have grown up with technology. I had an 8-bit computer when I was young, so I could learn to program in BASIC. People who were a little older than me (my older sibling and friends' older siblings) didn't have that opportunity. Mostly they didn't have access to a computer at all. Not at home, not at school, and not at a library.

The first popular home computers that came out were:

* 1977: Apple II
* 1979: Atari 800, TI-99/4
* 1981: IBM PC
* 1982: Commodore 64

And those are just the introduction dates. They didn't get affordable until later. (When the Apple II came out, it was over $5000 in inflation-adjusted dollars, and that was the base model without a floppy drive!)

If you are 50 years old, you were born in 1965 and graduated high school in about 1983. Perhaps you got a Commodore 64 when you were a senior in high school, but that's about the best most people of that age range could hope for, and that's assuming your family was the early adopter type.

Point is, will that drop-off in salary stay at age 50, or will it shift as the people my age, who did grow up with computers, continue to work in tech?

TLDR: Do employers dislike old people, or do they dislike people who didn't grow up with computers?
Soooooo Michael O. Church is correct then? That silicon valley culture fucked up and that ageism really is a huge problem?

Funny how many people dislike his writing and all of a sudden TechCrunch is pretty repeating what he's said, maybe toned down or watered down a little.

Lol dark secret? Everyone already knows this. 

Ageism is one of the most accepted forms of discrimination and is especially rampant in Silicon Valley.
There is more to life than Silly-CON Valley DotBomb culture.

I'm absolutely certain that we are in Tech Bubble 2.0, which is just slower to inflate in a post Sarbanes-Oxley world.  The failure rate for these startups is and will remain 90%, regardless.  

Anyways, startups should skew young for all the reasons mentioned in that article.  The "clean slate effect" and capacity for crunch time being paramount.  But there is still lots of work left for mature engineers.

*However*, I will admit that there is a population of older tech employees that have let their skills lag.  As I saw elsewhere on Reddit today, IT isn't a good field if your plan is to rest on your laurels. 


This article treats the tech industry like it's only web based start ups.
some of the comments in the article are interesting indeed.
How is it "about age" if the issue is that old people reqiures 3x the pay, without being worth the 3x?
Truth has been spoken. It is true not only for California it is very much true in Bangalore in India. The average age at which the salaries peak is around 8 years. If you have a job beyond that marginal increases increase in salary will still be there because of inflation. Once you hit the 12 year mark and you lose your job most big companies will not hire you. If you do get hired than don't expect a raise. It is much easier to find a job at that experience in a startup because they always want more experienced people and sometimes they pay well too. However once at a startup you are expected to behave like bachelors or freshmen which means late nights and working on weekends if you don't follow the norm you are out. If you are 15+ years experience and lose a job that might be the end of it. The way I see it is once you have 15+ years experience you should stay in the industry if you love your job enough and not doing a job only for pay raise.
This smells like a clickbait piece. The best companies/projects I've worked in/with were always a good mix of young and old.
It's all about age... right up until the first Silicon Valley generation gets older, then it'll be about experience, or something else.
"The harsh reality is that in the tech world, companies prefer to hire young, inexperienced, engineers."

Because young, inexperienced, engineers are cheap.



Pretty sure fresh grads here in Silicon Valley get 90k+. Also pretty sure that most top tier startups tend to hire the best, meaning not fresh grads.
Is this "bubble" mostly based in Silicon Valley? As an almost grad living in a large city on the east coast, it's kind of worrisome to read and hear these kinds of things when I am just now trying to get my foot in the door. 

Also, it seems like the "unicorns" are companies who's valuations all depend on how many people use their service. It's a weird world we live in when a company like Activision buys a company like King (makers of Candy Crush) for $5.9 billion just so they can get the ~470 million players integrated into their ecosystem. 
It is the same here in Europe. It is just simply the case that young juniors are pushing out the seniors due to their low salary expectations. 
I'm an independent contract web developer and I'm nearly forty. Granted I'm not "old", but I am when it comes to the tech world. My first computer was a CoCo II with its BASIC interface. 

That being said, I've found that it's not tough for me to get a good paying (for me at least since I live in the rural south) job. In fact, a lot of the people looking for remote workers are looking for dependability above all other qualities. I'm not a cocky kid who thinks his coding chops are amazing. I just have a family and want steady work, so I make myself available nearly 24/7, sleep few hours, and write good clean, easy to maintain code. I've actually heard quite a few of my clients mention that it's hard to find good help.

They just view "good help" as more than someone who is a "rock star" coder. They want good communication and availability. They all seem to have bad experiences with programmers. My work takes priority over everything because I have to feed six people, two dogs, and a cat. I probably couldn't get hired by Google or one of the big dogs in Sillicon Valley, but I have work, so I can't complain.
This is completely untrue. The problem is CS is way harder than people think it is and too many colleges run easy programs that don't prepare them.

(I've interviewed hundreds of engineers in silicon valley)
Software dev is a treadmill, it takes a lot of energy to keep up with the latest tech.

I interviewed a candidate in his late 40s the other day, he literally said 'i can't keep up with the latest tech anymore'.

As I'm building the team I need to make sure I'm hiring enthusiastic coders, young coders bring the passion, they usually have their finger on the pulse of the latest trends.




This article is from 2010 - it's only gotten worse. 

If you're over 35, your best bet is to leave and never go back. 
And gender.
*I'm 28, and in UK. I am on the way out of being young, and not in Silicon Valley, so maybe my view isn't worth hearing... but...*

I've worked as a developer for 9 years now, and I've worked with very few older (over 50) developers that have been any good. This is just my experience, and I'm certain there are many good ones, but I've not experienced it myself.

I put this down to a number of reasons, based on the people I have worked with.

- It is quite visible that they haven't kept up to date. I was told just a couple of years ago that a web application could never do what was required of the Access application one guy had made a decade previously. I've worked with a senior developer working for a huge company, and was on a lot of money, that was struggling to get his head around webforms, and that was 3 years ago. That could be ok in some developer jobs, but not the ones they were in.

- They seem to not have much interest in progress, both in their career and the applications they work on. If they are career developers by 50+, it suggests they have a trait where they like being in a comfort zone, and this shows in their work.

- They have a big issue with "the young kids coming in and trying to shake things up". Being good at your job is what should get you respect, not how long you are in it. It can sometimes be difficult to get them to see reason when someone appears to be more inexperienced.

Again, I'm not saying this is everybody over 50, but it is my experience so far (across about 8 companies - most contract). I have worked with really good 50+ people in other similar jobs, such as analysts, project managers, architects, etc.

Having said that... a lot can be said about hiring an early 20s person who comes in without the understanding of budget, risk, benefits, etc.
&gt; The young understand new technologies better than the old do

Ageist, just like the techcrunch article.
Ugg is that stock image really necessary for this article? It made me uncomfortable for some reason.
Honestly it's not about age, it's about youth. You can be older and still be youthful.
It is called neuroplasticity. It is tough for software developers because technology evolves far more rapid pace than what our brains can keep up with.
I am young and would like this to be true...
I think there's a lot of cringeworthy stuff in this article, but more than anything, the way the author talks about "legacy software" seems to signal an attitude that's very endemic in developer culture. Any well thought out software project really ought to have clearly defined boundaries upfront--this isn't to say we should waterfall the entire specification. If we have an application used in a production setting with clearly defined boundaries and goals, my question is *why on earth is it a bad thing that we stopped adding features, and are doing more maintenance*, if the software meets requirements? If the software meets the requirements, great, if not it's a regression, and we have bug fixes for that. The best software is often boring, because the best software is usually simple, well-defined, and has good abstraction; the end goal should be to produce pieces of software that go and go and go, and only require a small part if any of our limited capacity for cognizance. Often requirements do change, but hopefully the original application has facilities for IPC or is modular, and additions or changes can be introduced sanely. Requirements may also change enough, hopefully infrequently, to warrant embarking on either a major overhaul or an entire rewrite. Above all, these processes should be carefully considered before undergoing what may be needless work. It, on the contrary, seems the author is advocating churn for churns sake. I enjoy greenfield development just as much as many of the other developers working with me, but it's really the candy of the development world; more often than not, users seem to detest churn, and every rewrite potentially throws away hard learned lessons of the past and costs business money that may not have been necessary. Software maintenance is absolutely part of the job, and as a developer or software engineer, it's absolutely something you can't and shouldn't avoid, and would absolutely be a major red flag for working with the author.
Why should we be entertaining the musings of a "cto" of a company that hasn't shipped anything and most definitely doesn't have the experience to prove their methods successful? 
"Fixing bugs in a service is boring. That's why we'll rewrite the service using a new language and new tools!"

Oh man, https://www.jwz.org/doc/cadt.html is alive and well
You know what's slightly boring and really easy?  Maintaining code you wrote.

You know what's really boring and unpleasant?  Maintaining or extending code someone else wrote, especially if they cranked it out quickly to be Just Good Enough.

Working on a team that switches project every few months sounds terrible.

I wonder if this sort of thinking is the reason all the apps on my phone keep slowly evolving so everything works slightly differently every few months.  Dealing with that isn't "exciting", it's a waste of my time.




hiring advertisement, and a bad one at that
I'm pretty sure I'd kill myself if my job told me I *had* to spend one night a week with coworkers at an "enkithon" where we did something completely random. Maybe this is a Bay Thing or a Hipster Thing but yeah no. 
I'm not sure why this article irks me. Is it that some programmers have a hard time finding a job, while others are just bored with theirs and decide to change it? It takes a significant effort for me to even get an interview. Am I just a shitty developer? Is it so easy to just "quit" a job (because you're bored of it)?
Wow. Talk about neurotic and precious. 3 month churn and no legacy support sounds like a mess. Granted, all legacy is boring as batshit but this swings too far the other way.

I may be a dinosaur in spirit but I get far more pride in putting out high quality code that works really well compared with punching out something "reasonable" in a couple of months and then turning my back on it.
&gt;Oh, we totally understand. You didn't deliver our product because you *got bored*. We'll just put off all of the projects that were depending on the software we paid you to develop. No big deal.

-No company ever

I'm sorry, but being bored is part of the job. If you have the luxury of being bored and actually being able to change jobs when you want, I have no sympathy for you, nor do I particularly trust you as a developer.

I kind of want to upvote this because it's ridiculous and indicative of some of the bullshit in our field. On the other hand, I want to downvote it, because it's basically someone bragging about having a job where they don't have to be bored, with no real merit of its own. Ehhhh, I guess I'll do neither.
https://cdn-images-1.medium.com/max/800/1*h4Z7DjCx4TfDnJw7_HEarA.jpeg

This image made me cringe more than the text.

Caution with the posture at work!
The keyboard are too far from the edge of the desk, the screens are too low, and people there don't have ergonomic mouse pads.
After a few years they will probably suffer RSI and neck pain. It will be "boring".
Code does not "become faulty". If code stops working properly, then either you have a hardware problem, or a change to some other code it interacts with (which is a bug in *that* code instead), or the problem was always there to begin with.
I resigned from my position of 7 years 2 months ago. 7 year old framework. Obsolete. PHP 5.3.10. Too afraid to update to 5.3.27.
Year after year the same story. We scheduled it for 2 months down the road. Then moved it. Then again. and again.  If I want to stay relevant. If I want my skills to stay relevant. If I want a lifelong successful career. Working as a dinosaur isn't way to do it.  I'm using the latest available tools now. Bleeding edge development. I'm a lot happier. My former employer though is definitely trying to "retain the knowledge". Its retaining the creativity, what I brought for the last 7 years, that will cost them. 
In over 20 years in the industry, I've worked with 2 types of companies:

1. Those that tried to create a great culture for their developers to encourage them to stay.

2. Those that paid well above market rates.

In every case, the second option was more successful.
Stopped at microservice. If you can't build a monolithic codebase without creating a huge mess, microservice will just provide you an alternative way to hang yourself.
For someone who spends a lot of time thinking about retaining developers, he doesn't seem to have spent _any_ time thinking about ergonomics.
"...50% of my code was a direct copy/paste of Stack Overflow..."

Huge red flag on so many levels. Stopped reading after this.
I like boring....   I like things that work,   no problems.  I like smooth deployments where nothing is unexpected.

Why use Gradle, if Maven will do?     Why go for a microservice architecture if a monolithic will get the job done.  

There are places for these technologies......  but I would not use them unless I have to.   In fact, I dont use anything, unless I have to.


http://thedailywtf.com/articles/Programming-Sucks!-Or-At-Least,-It-Ought-To-

Is making sure your website is at least on the first page of the search engine hits for your company name too boring to spend time on? 
&gt; Internal tools are usually boring

*Boring* internal tools are boring.  So are boring commercial apps.

This statement just shows he has experience at places with boring tools.  I've worked on a lot of amazingly interesting internal tools.  Some required a ton of domain-specific knowledge to write them.  Very challenging - and interesting.

At least he qualifies this with "usually".  But it still reflects a single person's experience.
I wouldn't want to hire someone who expressed his opinions.  He doesn't have experience developing software that has to be maintained for years.  If he gets this bored doing development he is not properly architecting it to keep it interesting. 

Sorry if you get bored, but I see that as a weakness in a developer. 


It's not a job's responsibility to be interesting, it's your responsibility to find interesting &amp;&amp; beneficial things to do at your job. 

New jobs every 2 years is a yellow flag for me when I interview candidates. It takes at least a couple of months to ramp a dev. to a point where they are meaningfully contributing. I hate wasting that sort of time. 
Check out @simonbrown's Tweet: https://twitter.com/simonbrown/status/573072777147777024?s=09
According to his LinkedIn page the guy seems like a professional intern/grad student who's had 2 industry jobs after his PhD. 

No doubt he's smart, but I would take his career experience with a gain of salt. 
The writer comes across as an almost completely un-hirable , petulant child.

Spent most of his time copy/pasting code from stack overflow, didn't discipline himself to learn new ideas/technology when the opportunity arose, gets bored easily and becomes disinterested with bugfixing etc.

There is no WAY I would ever hire this guy, and, having read his coding style, no way I would ever buy one of his products.
The very number of upvotes this delusional piece got here is an extremely worrying sign. Tells a lot about how crappy our industry is.
So just when a developer becomes competent in a project and has mastered the requisite skills, the developer is moved to another project that they're slightly unqualified to do? It sounds like a code quality anti-pattern. 
Referring to software engineering as coding? He **should** be called a code monkey.
There is nothing funnier than a man-child trying to sell his bullshit as some sort of wisdom. This is fucking hilarious.
"As a developer, I never managed to stick to the same job for more than two years."

As a lead developer at my company I would not hire you because you said this.
From just reading the headings in this article makes me think the author has never been on a greenfield project or a project where there was any true ownership of his work. If those are false, what a whiny fuck.
Click Bait link Title

&gt; Coding is boring, unless...


First sentence of the article

&gt; As a developer, I never managed to stick to the same job for more than two years.

This is not how you transition such a thing. Continue the fucking sentence. 
I just quit a 6.5 years old job and got a job which uses tech stack i know nothing about because i got bored in my job. Not sure if I am over my head... New job starts on Monday.
I think a chance to work on your own code (or, better, code owned by you) for a couple of years is really important - you have the ability to view the code evolving, and (if you do your job badly) how it changes from "shiny new microservicish" to "old legacy code we need to rewrite from scratch". You can see long term impact of your design decisions.

The author seems to be deprived of this experience and his "rewrite from scratch" approach seems to deprive his employees of this experience also.

wow...you are a CTO, you must know everything. The post is embarassing
It's articles like this that make new developers get in the industry, thinking they can do random fun stuff with any random tools without any regard to business commitments and deadlines. 

(I have to remind my youngish fellow developers suffering from Facebook envy that we* are writing software to support a business, not as fun projects.)
I don't get why people expect to have fun at work. I mean, of course it is great when you have fun while doing what you got paid for, but sometimes you have to accept you work to get paid, not to "have fun".

That "entertain me at work" attitude creates the playground office bullshit that somehow people seem to love. I usually get job offers that tell you how you get "free drinks" or "play area" at the office, cut the crap and tell me how much you pay and how much overtime you expect me to work. I can have fun on my own free time, thank you.


I was agreeing 100% until the last point:

&gt; We also organize team off-sites (e.g. Secret Cinema) and we have a weekly “enkithon” (pizza night + activities) with no predefined agenda. Sometimes we hack something together. Sometimes we brainstorm a new idea. Sometimes we just play League of Legends. Or we go to the pub. The beauty of it comes from the fact that we don’t know what we’re going do until the last minute, when we decide together.

And sure enough, the "Team" photographs: https://enki.com/#team six middle-class white men, all aged 25 to 35.  (UPDATE: unfortunately I hadn't considered this paragraph would be quite so incendiary to so many, I only mentioned this to put in context why a weekly "League of Legends" night works for them, but would be boring to so many others.  My point would be equally valid with any other socio-demographic groups.)

You know what I find really boring?  Monocultures.  Spending 40 hours a week with people who all think and behave in exactly the same way; and worse?  A team that defines themselves as continuing to be all identical in the evenings too.

DISCLAIMER: I don't know that company or any of those people, and I'd probably fit in alarmingly well if I did, so none of the above is a personal attack.

EDIT: This is why I love Reddit, before today I didn't know monocultures were the last line of defence to state-sponsored collectivism.  My eyes have been opened.
What the hell does his company do? 

Is it an app to help you learn programming? 
Coding is boring unless you get well payed for every single minute of work.

Tired of this shit hipster philosophies about stuff to improve programmer work life.

Fucking hell, just pay me more and stop thinking about useless in a unnecessary job with a fake CTO position.
Dogshit article
SPOILED / LAZY / DUMB PUNK
My opendns is blocking the site.. It's saying this contains pornography.. Is it so?
So the goal of software is to keep coders entertained?  Imagine if accountants or engineers acted like precious little butterflies.
The cavalier attitude of the author scares the crap out of me. 
&gt; A key ingredient here is diversity: hiring people with different backgrounds and origins (e.g. our team of 6 is currently British, French, Russian and Greek).

I am extremely surprised to find this statement on the public blog of an American-based company.  Someone who has a British background who didn't get hired could sue for ethnic discrimination and use this blog as evidence that ethnicity was more important than skillset.
I think this would be a good idea for some people, but others like being very specialized and comfortable with their work. I understand it's not good to "pigeon-hole" yourself in the field, but it also seems like some people would work better and be more efficient working on a specific piece for a long time, that way if something goes wrong with that part, they can quickly and easily fix the issue instead of trying to learn the part of the code at that time that it goes down in case people have been recently rotated during the change. 
Only there are only two reasons to code:

1. Grow benefits
2. Cut costs

And some people say these 2 are one and the same.
Free hugs also seem to help from the photos.
Part of this article rings very true, part of it is bullshit that should be solved by hiring competent, professional engineers without ADD.

Especially when it comes to bugs, tooling and legacy code, "boring" is often just a euphemism for incompetence.

Um, can we talk about the picture of the two dudes cuddling?
what i've taken from this article is that faggots don't know how to manage a programming team. OP why did you feel the need to include your relationship with your team mates in this article
[deleted]
Unless what? You blog about it?
I'm mostly skeptical about the diversity moral panic that's currently sweeping the industry.

But for crying out loud, if you mention diversity as one of your "good practices", and the photos picture only white dudes in their 20s who (as you admit) are all from the Western culture save for one token Russian guy, then you are just making a satire out of your own point.
This has quite a few gotchas initially which people may not be expecting.    

 - To acquire the initial certificate you need to install their Linux software with root privileges (other OSs not supported, including Windows/Cisco IOS/OS X). It is complex to do without their software, more complex than a normal CA.        
 - It doesn't support nginx yet. You'll need to do the stand-alone option (it has a built in webserver) and then manually configure nginx. That's true for most non-Apache web servers also.     
 - The certificates, by design, have a short expiration time (three months). The software auto-renews for you (remember, running as root), unless you're doing it manually then *you* have to renew it every three months.    
 - No wildcard certs. Subdomains all need to be configured individually.      
 - Not *really* suitable for anything more complicated than a single web-server/home setup. For load balancers, multiple servers, or unusual web servers (i.e. non-Apache/nginx) you're frankly better off spending the $9 for a regular one year domain CA certificate, since your update/redistribute workflow is simplier.      

Not a shill, and actually *like* Let's Encrypt and their mission a great deal. But the way they've decided to validate domain-validated certificates is extremely complicated and has limitations either if you do use their software or you do not.    

Some of the issues above will definitely be resolved in time by additional software (e.g. packages for other platforms, additional web server support, optional non-root, etc). This is the reality if you try to use Let's Encrypt *tomorrow*. A year from now who knows...    
Please note how "SSL" is no where to be found on that page.

SSL is a long dead technology that has been replaced by TLS. Please follow Let's Encrypt's example and stop using "SSL Certificate" as a phrase.
I'm pretty excited about this.  No more self-signed certificates for web management portals and easy implementation in test environments.  I'll definitely be signing up to give it a try.
Are they doing Certificate Transparency yet? The cert they gave me recently for https://bradfitz.com/ says "The identity of this website has been verified by Let's Encrypt Authority X1. No Certificate Transparency information was supplied by the server."
Welcome to reddit where everyone is a critic and hardly anyone actually does anything. Does anyone know how hard this is? And for free? Sheesh!
So how does this technology prevent me from e.g. getting a certificate for, say, google.com, or any other site I want automatically after I hijack somebody's DNS request for google.com, and reply to that with a bogus IP?

In more general terms, how is it prevented that this CA doesn't just become abused as a MitM proxy?
ELI5: I own a domain on godaddy, which just points to github pages. How do I turn my site to https? 
If I'm already happy using StartSSL, is there any benefit to switching to LE?
No wildcard certs:(

EDIT: I find them very useful even if you don't
So I'm gonna ask it.. who pays for this? In the long run, I mean? 


Anyone know if this will work with Java Web Start? 

Edit with answer from their FAQ, bummer:

&gt;No. Email encryption and code signing require a different type of certificate than Let’s Encrypt will be issuing.
Personally, I think this is a bad service, for the simple reason that they refuse to sign certificates for longer than ninety days.

The purpose behind the short renewal appears to be to force you to run automated update scripts on your servers.  This increases your attack exposure, and it forces you to open outbound communication channels from supposedly secure machines; you can't firewall them completely.  And the scripts have many dependencies and auto-update capability.  

For a service that's supposedly about security, this is a *terrible* decision.  To protect against theoretical weaknesses, they're imposing *real* ones. 

Startssl will give you free certs for personal use; you have to pay to *revoke* them, so you need to be careful, but you don't have to add in a whole new layer of things that could go wrong.

Because you'll start charging for free SSL certificates from the day after? :)
And not one word there about *how* to get a certificate.  Some of us don't already know.
Can anyone get one for the spanish  certification authority, please? Correos y telegrafos 
The double emulation test is an insanely good way to test correctness, also insane to try to get working. 
It still blows my mind the testing pipeline they set up, with the render output diffs. Bet you not many professional houses have one of those.
These guys are totally insanely skilled
I've been playing Phantasy Star Online again thanks to Dolphin.  Awesome stuff.
This is professional grade if I ever saw it.
I've tried this several times over the last few years and I am always disappointed with the progress they have made. It's either been a hassle to get setup or just plain didn't work. Last time I tried it I almost immediately cancelled my subscription because it was so terrible but they continued to charge me for several months. Despite several attempts to contact their support, I was never able to get them to stop charging me so I reported it to PayPal (it was the only option they had at the time) and PayPal stopped it. 
Scores a 10/10 on the "looks like sublime in screenshots" section of the exam.
Call me paranoid, but the reliance on cloud services is going to match dedicated game servers. They're going to get shut down when they stop making money, taking your data with them, ToS or not.
Euh... Does this have e.g. build support, source control support, debugging...? If not, it's not an IDE, it's a cloud-based file manager and a text editor.

Or am I missing something?
I really don't see ANY benefit to a web-based IDE. Great, so you can code anywhere you have a browser. I've got a great one on my laptop which is where I do my development. I can update it when it's convenient for ME at the right point in MY dev cycle. When I have no internet, it works great. If the company that makes it goes under, I still have a working IDE and all my data.

I get it. It was a challenge to build an IDE in a browser but that's a solution in search of a problem.
So, to get increased security you need to pay at least $7 a month. That is when two factor auth gets enabled. Sad to see a developer system online take advantage of increased security like that as an upsell. I believe if they really cared about user security, this wouldn't be an upsell. If you have two factor auth, everyone should have it available and be encouraged to use it. That is just the right thing to do to help protect accounts and the data in them.
Soo, every editor with code highlighting and terminal is now an IDE? 
I prefer Cloud9 (c9.io)

It has build support, auto complete, can ssh, gives you terminal access.

More importantly, it can be self hosted with the node js SDK available.

For people who prefer to use gedit+terminal, this gives a similar lightweight interface for OSes other than Linux!
What is the big deal about this? You can always remote in to your home PC if you really need a persistent environment. Then it's a lot more stable than having to access other people's servers ("the cloud"*).

Or, as other people have said, if I don't need access to my home environment (which is mostly about data access), I can easily clone everything else on my laptop.

\* I really wish we would call "the cloud" what it is. Not for us, we all know what it is. For our less tech-savvy parents, bosses, and colleagues. "The cloud" isn't magic -- it's just another person's computer.
I like it. I'm not using it, because if i'm developing anything nowdays it's either on my home machine (the server) or my laptop, and if i'm on my laptop somewhere
i connect to my home and mount sshfs for all the files i will edit.

but i could see how this would be better if you develop from a lot of different machines, or perhaps other's machines.

Yet another pointless thing.  It'll never be as rich as a proper development environment.  Github/bitbucket/TFS provide decent source control.

I'll avoid this, thank you.
I'm working on building something very similar to this that you can install on your own server. I'll likely release it FOSS.
It bothers me that this is at the top of /r/programming, and even more that any programmer would find this an acceptable solution. I thought programmers in general were supposed to be computer literate. There are thousands of solutions to whatever problem a cloud based IDE is supposed to solve that are way superior. Most of them would give you full control of the tool set, files and repository. Many of them could act as your primary development environment, not some half baked "in case of" environment. You are programmers, ffs. How can you possibly accept anything less?
Redditor for 1 month and posts a giant advertisement, then you fucks upvote it? Jesus Christ....
I will never trust any cloud based IDE. I'd rather code with notepad or on a piece of paper than that. If it is free, I am the product, or in this case, probably my code. 
That is the dumbest idea I've heard all day. Browser-based apps are horrible.
I was excited for cloud9, a similar service that provides an editor plus good hosting. The idea was that you would have a dev/production environment on your laptop, chromebook or whatever.

The truth is that coding on your browser is not very productive. Might be good for non-professionals but I had to go back to my editor
[deleted]
Works great actually

I see these as being great as a learning environment, but nothing more. I'm still just starting with Python and I use vim/pycharm/notepad++ depending on which PC I happen to be on, but if I have downtime at work I'll hop on the free version of nitrous.io. I have no expectation that anything I make will still be there when I log back in, which is fine for my learning purposes.
Where is the list of 75 languages?
I was thinking about this concept last year, and I'm glad to see its actually been done.  At first glance this looks incredible.  Hopefully its not hard to use, or irritating as I've read in some of the comments, and hopefully this eventually suits anyone's needs. If this is stable, and a good solution, than I honestly think this could be incredibly useful, especially for CS students
My web IDE: Emacs and the CLI running in Byobu windows on a Linode/AWS/whatever.  At home I run Emacs as a desktop app but otherwise this is an exact duplicate of my local setup.
I tried this out for a bit, it's a good service, one of my favourite cloud based coding services. Unlike other services you have fairly decent reign over your entire virtual machine. So yup, I can actually develop, compile, debug in languages that aren't web based, unlike most services! 

Having a Ubuntu box you can ssh into is really handy. I set up vnc with a basic wm and had a fairly functional way of testing gui apps too (I was creating a basic rpg game in java at the time). It was a little bit of a pain to set up, but it's pretty satisfying having a centralised point for your projects. I could even run my own choice of ide if I'd prefer not to use vi. Ultimately the constraints got the better of me and the lack of processing grunt made it more hassle than it was worth. But for someone with a thin client who travels frequently it's really quite nice. It's the only service I've seen that is actually a viable cloud based solution for development, and I've no doubt things will get a lot better in the not too distant future. Very promising.
VPN + Remote Desktop works excellent :-)
I find myself thinking the same thing about every web based IDE/editor - how would you write a graphical application? How would you write a game? I guess its possible to install X on the remote and then somehow stream video over the wire but that seems like a chunky experience. I guess that's the difference between these and traditional solutions, in a web based IDE you can only write text based applications.
The real question is: Does it support vim?
Jesus Christ what a shitty idea.
Each time I try to use any of those IDEs they one way or another don't work up to the point where opening a file is a problem. It's really incredible given that they seem to have customers and such.

This time it was connect to bitbucket -&gt; select project -&gt; file menu does nothing when clicking any of "clone" "refresh" etc.

This time it seems to be the same-origin policy not working, but the actual reported user experience (clicking buttons don't do anything useful) is really terrible. I would almost prefer everything to crash
There is also [sourcelair](https://www.sourcelair.com/home)
I've tried this over the past couple of weeks and more often then not the containers just time out and I can't get a terminal or edit any files.  Good idea, but its just not useable reliably at all.
I really love the concept of Codeanywhere. But as a small independent dev who mostly supports open source, I can't afford the monthly subscription. I have no need of the virtual machines which the paid tiers seem to be centered around. All I need/want is a tier that is extremely cheap (or the free tier) with an increased the number of remote connections... If they did this I think their product would be much better.
So how do I get started? Yes I can write my code, and how do I run it? How do I see my output? I browsed the documentation but it looks really irretating to me, I only find information how the Editor works, how Syntax highlighting is etc. But no tutorial how to get started.
Cool.

I have a side question.

For a few weeks I've been wondering how to get my niece and nephew interested in programming.

The requirement of a computer for python or scratch is surprising to me, given that damned near every kid has an ipad or android tablet.

I looked around for "coding for kids, learn on your ipad" style books and tutorials and came up empty.

Would this work for that purpose?

Could someone write, and run, python or javascript scripts in it, from a tablet with a mobile OS?

Would it be a paid subscription, or is there a free/ad-supported level for hobbyists?

I don't have a mobile tablet so I can't try.
Hi:  I am the founder and CEO of Codenvy.  We are big fans of CodeAnywhere, and the service has come a very long ways since it was PHPAnywhere.  They have solved some hard problems, and the cloud IDE vendors, which there are about a dozen, continue to solve some tricky problems around environment initialization, environment interoperability, collaborative development, performance related to muscle memory, and so forth.  Codenvy's business is more on-premises vs. CodeAnywhere's online business, so we target different segments of the same market.

I wanted to discuss some of the questions on the value propositions of cloud IDEs.  These are three core problems that cloud IDEs can solve. These are the ones that we focus on - there are others, but this is where my expertise lies:
1. Developer bootstrapping
2. Securing distributed workspaces
3. Modernizing legacy development processes

The bootstrapping problem is the most obvious.  This value applies to individual engineers, team leaders, instructors, bloggers.  Any place where the effort to duplicate an environment is filled with friction, a cloud IDE has the potential to solve this problem.  Cloud IDEs can solve this problem in a couple ways.  First, the browser or API access provides a sort of universal UI that doesn't require initial software to be installed on a machine before it's accessible. This is commonly understood.  The second is the portability of the environment.  A cloud workspace must be powered by some environment.  Each of the cloud IDE vendors take different approaches. Some use bare VMs, other provide a single container, and others provide specialized processes / containers for individuals tasks like compilation.  Since these environments can be defined through recipes and have their state saved, they can be made interoperable to different locations.  So this means that you could have a workspace that has three projects inside of it, and have it be powered by a specialized stack that includes an esoteric version of the JDK, an embedded maven repository that already has artifacts stored within it, and a database that has some prefilled tables.  This environment could be bound to a workspace and also made portable.

Portability of hosted environments is not a critical function for a vendor like c9 or codeanywhere as they provide a singularly hosted service. Eclipse Che, though, makes it possible to export workspaces &amp; environments that could then be saved in the cloud, or imported into another Che instance.  It's kind of like having a portable localhost.

The bootstrapping problem has a tendency to appear increasingly in teams and education institutions more than for an individual user.

The second problem space is security. We see this primarily within insurance, financial services, and HIPAA-driven companies. Any time you have a distributed workforce that has access to code that must be managed under some compliance policy, the organization lacks visibility into what is happening to the code in between pushes when the code is cloned onto a desktop.  A centralized service provides a set of workspaces where the code cloning happens, and then tracking behavior for everything that happens within that workspace.  Once you can track access to the workspace, you can also start limiting behaviors according to corporate policies - time of day the workspace is accessible, the types of plugins that the worker is allowed to use, how much RAM they are given for the workspace, and so forth.  Most of Codenvy's revenue business is from on-premises installations from organizations that have these complex policies.  We have a way for organizations to bundle their own plug-ins and then inject behavior policies for different users.  Sounds complicated, but for organizations that have &gt;100 distributed developers in different countries, they love this sort of thing.

The last problem is around modernizing workflow.  We can collectively make agile more agile.  It turns out that a lot of organizations follow similar patterns around branch flow and how they want to enable feedback being given to a developer pre-merge of their code.  Cloud IDEs make it possible to programmatically launch a workspace with a particular configuration &amp; environment. Once this exists, there are patterns for how the workspace can be integrated into issue management, source code repositories, and continuous integration so that a "workspace-on-demand" with context to the issue, repo, branch, or build can be launched and then have this launch ability integrated into the workflow that developers are using every day.  

This last bullet has the furthest to go and to demonstrate.  We use a variation of this internally at Codenvy where we have every Jira issue integrated to launch its own custom workspace with a git branch that is generated based upon certain actions (ie, developer says "start work" in Jira).  We then load testers, PMs, and beta users into Jira so that they can participate in the flow.  And when branches are ultimately merged or altered, the workspace(s) have their definitions modified.  We do a similar sort of thing with GitHub / BitBucket pull requests and Jenkins build failures where another workspace is embedded for diagnostics.  We have a lot more work to do here, but this is the essence - developers want their IDE to be the place that they code, not the place where they are trying ot navigate their issues / branches - so the industry is increasingly exploring ways to embed the workspaces into the existing tools that developers are already using. 
**FOR THE LOVE OF GOD DO NOT USE THIS TO WORK ON A PROJECT WHERE THERE IS ANYONE ELSE ALSO WORKING ON IT!!**

At my last job, my boss used this like a dumbfuck, constantly overwriting people's code and forced us to have to do a lot of manual merges with git. Fucking horrible.
I have a web editor like this on a host I own. Is nice to put into code small web hacks.  It works for the most part.
Do you plan on having any plans for opensource communities? I help people get started on programming languages, and it would be wonderful to have some nice-to-start interface for them where I could jump in and help, realtime. 
What libraries are used for the code highlighting and syntax completion and such?
Well, if it can sync well with multiple dev editing things together, I guess it can be great for some coop project.
I carry a usbstick with my devenvironment and all my projects, so that I can develop wherever I have a computer that supports x86 and can boot from usb, internet connectivity or no.
Ehh. Its a good concept in theory but after trying to use it I'd rather just not.
Giving me a command line with gcc is not an "IDE".

NetBeans is still a great IDE. Not as industrial and confusing as eclipse. Nice simple installer package. Straightforward project management. No bizarre windows legacy and paywall stuff like VisualStudio.

NetBeans is the last hope for humanity. If only they could make it 100x faster ... 
i love the idea of a cloud based IDE, let people write all their private code and scoop it up for personal gains!!
&gt; Code completion (js, php, html, css)
Thank you Holy Turing that I don't have a job where this "IDE" would be acceptable.
I found this very interesting. 

Reading the source code of the (state of the art) Boehm-Demers-Weiser Conservative Garbage Collector, it might be easy to assume that garbage collection is a black art that no one can hope to understand; but here, we have a collector that is practical: it integrates with C and can deal with pointers stored on the stack or in the heap, and it still remains simple - its full source code on GitHub a mere 600 lines of code. That it is practical is, I think, a very key point. I think people are much more captivated by something that is directly useful and not confined to the hypothetical.

I would definitely recommend anyone with an interest in memory management or even just C in general have a read through this. It's one of those pieces that are loaded with a lot of information, which all makes it through to your mind without your even noticing.
Front page has a typo, an `i2` should be an `i1`.
I think there's a problem in this code in `Cello_GC_Mark_Prelude()`:

    volatile int noinline = 1;
    void (*mark_stack)(void) = noinline
      ? Cello_GC_Mark_Stack
      : (var(*)(void))(NULL);
    mark_stack();

The article states

&gt; we can ensure that the function is not inlined by only calling the marking function via a function pointer, who's value is decided using a volatile variable (volatile variables are immune to optimisations).

I think that's not enough to ensure no optimization will happen. The compiler might notice that, if `noinline` is `0`, then the result will be undefined behavior (because a `NULL` function pointer will be called). So it's free to do anything it wants in that case -- including calling `Cello_GC_Mark_Stack` instead of the `NULL` pointer. That means it doesn't need to make the call via the function pointer, since it "knows" what it points to at compile time.

Compilers taking advantage of undefined behavior in optimizations is a known problem, see [here](http://blog.llvm.org/2011/05/what-every-c-programmer-should-know_14.html) for example.

What's with the usage of $ and var keyword? Where do these come from? I've never seen them in C code before and I couldn't find any good explanation in the sources. Is var really a part of the C standard these days?
This seems interesting, but maybe someone can illuminate me why you would want a garbage collector with C. My perspective of C (and C++, to a degree) is that it provides direct access to the memory. In exchange for *having to* manage the memory yourself, you *get to* mange memory yourself.

What situations are there where you would want to use C, but without having control over memory management?
Isn't it easier to GC stack just by adding an extra step which will clear all local variables set when popping out of the function?
so how is this different from the decades old Boehm GC?
What if I have a local variable that happens to have the value of the address of a Cello object? It's not that unrealistic on low memory applications like an Arduino is it?
&gt; First - pointers must be memory aligned - which means for 64-bit machines they can only be located every 8-byte boundary, and must only point to some value on an 8-byte boundary. This means the pointer value must be a multiple of the pointer size, and the address of the pointer must be a multiple of the pointer size.

I think you'll need to add one more rule for the application to follow: don't make pack(1) structs that contain pointers to things that are GC:ed. E.g.:

    #pragma pack(1)

    struct baz {
        bool foo;
        void *ptr;
    };

Valid code that has unaligned pointers on the heap.
I implemented something very similar in C++ in 2011: http://www.codeotaku.com/journal/2011-09/simple-garbage-collector/index

It's about 100 lines of code. The stack pointer trick is an awesome hack :)
The problem with Cello is that it's _flippin' awful_.
&gt; Looking at computers qua structure, qua challenge, qua potential, I can only conclude that the automatic electronic computer is a gadget absolutely without precedent. If that view is appropriate --and I believe it is-- we should be very suspicious of anything imported from elsewhere into computing --in the widest sense of the word-- with the mere justification that elsewhere it worked or made sense. Computers are such exceptional gadgets that there is good reason to assume that most analogies with other disciplines are too shallow to be of any positive value, are even so shallow that they are only confusing. I can only conclude that the wide-spread adoption of the term "software engineering" is to be regretted as it only hampers this recognition.

[E. W. Dijkstra, 1978](https://www.cs.utexas.edu/users/EWD/transcriptions/EWD06xx/EWD690.html)
These consideration can be applied to every kind of engineer which doesn't work with critical systems.
You can complain about sound engineers, industrial engineers, agricoltural engineers, electronic engineers... not being "real" engineers.
It seems to me some kind of misplaced elitarism or envy of some kind.
This whole argument about engineering in software kind of misses the point.

Rigorous standards might produce better, safer, software, but the trade-off is that such software would cost orders of magnitude more, and/or have orders of magnitude less features.

Most people actually prefer Angry Birds or even Microsoft Word are affordable and feature-rich, even if they crash and lose data once in a while.

Rigorous engineering-quality methods for high quality solid software do exist.  But nobody wants to pay for software like that, except maybe NASA for the space shuttle, etc.
 What I love about articles like these are that they simply don't matter. And thank goodness.

 They generally illustrate one person's sudden revelatory **glimpse** at the complexity of the world and then their angry condemnation of something they didn't like about it. It's everywhere on youtube comments, and some of these people also happen to write for news paper columns.
TFA misses the point of the Atlantic article, which is that other engineering disciplines are regulated industries where certain levels of quality are mandated.

And the Atlantic article misses the point that there are electrical engineers bodging together cheap junk under time-to-market constraints, and aeronautics engineers meeting rigorous standards, and everything in between, just like there are enterprise software engineers crapping out the latest mobile app junk and NASA software engineers meeting rigorous standards, and everything in between.

Blame the widespread culture of deregulation which allows our enterprise software to be developed without stringent security and privacy requirements.

Or look at countries without strict engineering, where [civil engineering projects](http://lost-and-found-adventures.com/wp-content/uploads/roads_07.jpg) don't always have really high standards.
The Atlantic piece boils down to a bunch of special pleading.  A quick Google search of "bridge failure" will give us a clue to just how fraught with catastrophe all engineering disciplines have always been, right up until the present day. No amount of government certification eradicates the basic problem of human error and rent-seeking cost cutting measures that typically lead up to an engineering disaster.  Our civilization has thousands of years of bridge-making under it's belt, and yet every year there is another story of a unsound structure being erected by the lowest bidder.  

The main difference is that there have only a few decades of computer science, and perhaps 45 years total for software engineering as a discipline.  That doesn't mean that it's not a real discipline, it just means that it's a new one.  The need for it is very real, and the people who are working in the field are the ones who are trying to establish the best practices that are sorely needed.  The argument that software engineering doesn't exist because there have been software failures is ridiculous. 

The argument that software hasn't killed nearly as many people as every other engineering sounds to me like some sort of circular logic.  *If* software was written the way bridges are built, then planes would routinely fall out of the sky when the autopilot is engaged.  How does one define an aerospace engineer, then?  Were the Wright Brothers just tinkerers, whereas the NASA employees who blew up a bunch of space shuttles true engineers?  That's just special pleading and it doesn't make any sense.  The fact of the matter is that the 1968 conferences on software engineering had led to the establishment of extremely rigorous engineering standards in the very mission-critical applications where failures would result in lost lives.

&gt;  the ease of rapid repair became an excuse for rapid development

This is nothing short of an inversion of cause and effect.  It was, in fact, the extremely high cost of software mistakes (what the author considers pointless throwaway software) that led to the development of rapid repair, which ultimately enabled rapid development.  This wasn't just some random coincidence, but a concerted multi-decade engineering effort.  It's really ironic that a few sentences later the author compared us "fake" software engineers to automotive engineers, even as over-the-air (OTA) software updates are set to be a boon for the automotive industry: http://press.ihs.com/press-release/automotive/over-air-software-updates-create-boon-automotive-market-ihs-says  That's $35 billion per year in cost savings to the automotive industry thanks to the *software engineering* approaches that have been developed over the years by *software engineers*.

&gt;  Earlier computing systems were imbricated with other aspects of business, industry, government, and society. An automobile customer-management system has to integrate with dealers, suppliers, shippers, banks and lenders, regulators, legacy systems, and customers. But today’s software mostly stands alone. Instagram, a photo-sharing service valued at $35 billion last year, just uploads and downloads images between its servers and its app.

This is so self-contradictory and so far removed from reality that it encroaches on unintentional comedy.  Software such as Instagram are integrated into *hundreds* of other software products used by millions of people in a variety of ways.  And all of that software is integrated into vast advertising networks, which these days routinely rely on sophisticated AI systems to learn as much as they can about the customers of tens of thousands of the companies these networks serve, which are, ultimately, also integrated into humble automotive customer management systems and dealer services.  But to claim that the older, smaller, less sophisticated, and far less valuable system is software engineering while the new one is not is some sort of a joke is, I don't know - it just smacks of petty jealousy.  The truth is that the new software systems appear to be so simple because to the end user, it seems like they "just work".  You upload a photo, a computer suggests some flattering image filters based on sophisticated algorithms, another computer recognizes the faces of you and your friends in it, and some other computer lets one of your pen pals on the other side of the planet "like" your photo, all the while Ford and Chevy and thousands of other companies build their brand and sell you a new car.  That's like, just like an FTP site, right?
http://thecodelesscode.com/case/154
This article somehow manages to be even worse than the original one they're complaining about. Can these journalists please stick to writing about something they understand? Or at least do some research and talk to someone who does work in the field? 

&gt; Bogost is partially right. Today’s average software developer connects prefabricated pieces of code together to make it work, but touches little of its underlying infrastructure.    

And how do they think supposedly proper engineers work? Do they make every capacitor from scratch? Do they mine their own ore to make steel for the support beams in that bridge? The good ones aren't, because they are interested in controlling costs, construction time and assuring a constant quality of components ensured by having components and tools made by specialists.     

The [Atlantic article](http://www.theatlantic.com/technology/archive/2015/11/programmers-should-not-call-themselves-engineers/414271/) she is referring to creates a false dichotomy, as though proper engineers produce quality without bugs, and software engineers release buggy rubbish, but they're comparing apples with oranges. 

I've had toasters and ovens that malfunctioned because of a design flaw or a construction error. One can also find examples of roads, buildings and bridges full of engineering errors. There is also software running on airplanes, space craft, nuclear power plants, that is extremely reliably engineered. The difference is all in cost and impact. 

One could make an iPhone app to the same standards, if he were willing to wait 5 years and pay a million for it instead of $2. Things we buy, whether they're software, hardware, a house, clothes, food, are designed and produced to a quality that we are willing to pay for. If that's not good enough, if everything has to be made to the absolute highest possible quality standard, then I hope you're happy with your $50,000 loincloth and your spoon with a $120,000 mortgage that are the only possessions you can afford. 
Software development is not yet a mature engineering science

It's not because programmers are lazy or incompetent

It's not because of lack of government regulation

It's because nobody knows a rigorous formula, that if followed, would produce perfect, bug-free code

I have been programming since 1972, and have seen pitch after pitch, each one promising miracles

High level languages, structured programming, object oriented programming, agile, scrum...etc, all failed to deliver the perfect solution

Software is the most complex thing the human mind has ever tried to create. I suspect, but can't prove, that large software contains infinite complexity..kinda like the Mandelbrot set

We need major breakthroughs in managing and understanding complexity. What we don't need are more crappy layers of complexity..crap piled on crap over a foundation of crap

Yes, people who write software are engineers

We are just frustrated that software is still mostly an artform and craft, with very little engineering science to guide us
Re: the authors remark that no one dies from poorly written software.

I've worked on E911 software for the telephone network.  Every minute those lines are down, multiple people can die.

There are many other life and death software applications that demand 99.999% uptime:

* Avionics 
* Medical devices
* Medical monitoring 
* Public telephone network
* Military software
* Spacecraft guidance
* Spacecraft life support
* Vehicle autopilot

and many more.

Read comp.risks if you want eye opening stories of how software failures led to expensive (billion dollar) losses and death.

These types of projects are tightly spec'd and extensively QA'd and are executed quite differently than your typical web page or mobile game app development.

It may not have the imprimatur of the Canadian or Belgium Engineering Association, but it's as close to an engineering discipline as software development can be made at this point.

Edit: Fixed list per comment below--thx!
of course. it was a hit piece like the washington post drivel about linus not caring about security. as far as i am concerned, these articles are not written for technical people, they are just raw meat for a public that is increasingly suspicious of tech
This debate is driving me crazy.  Of course people who develop software are performing engineering:

&gt;**An engineer is** a professional practitioner of engineering, **concerned with applying scientific knowledge, mathematics, and ingenuity to develop solutions for technical, societal and commercial problems.** **Engineers design** materials, structures, and **systems while considering the limitations imposed by practicality, regulation, safety, and cost.**  The word engineer is derived from the Latin words ingeniare ("to contrive, devise") and ingenium ("cleverness").

&gt;The work of engineers forms the link between scientific discoveries and their subsequent applications to human needs and quality of life.

I do engineering every day.  You can call it Bilbo Baggins'ing for all the hell I care, but this semantic pedantry is absurd.  The process of developing software involves engineering from start to finish.  Scientists discover things, engineers take those discoveries and apply them to real world solutions.  All engineers in all fields build on the backs of scientists and engineers who came before them.

Are you building software for the space shuttle?  You increase safety and rigor at the expensive of features and cost.  Are you building a calendar app for your grandma?  Get it done as fast and cheaply as possible.

Are you engineering a space shuttle?  You increase safety and rigor at the expense of features and cost.  Are you engineering an RC car?  Get it done as fast and cheaply as possible.

Writing code is hand writing blueprints for a computer, it's not transcription.  Even if one engineer is coming up with the concepts and another is putting together the blueprints, they're both still engineers.  One difference between code and actual blueprints though is that technicians can always tweak blueprints at the construction site, whereas if your code is wrong you gotta re-write it.
Who cares? A rose by any other name would smell just as sweet.
This debate is the reason I hate people who pointlessly argue over semantics.
Using OpenSSL as a rebuttal to the argument that software engineering is not engineering. Whoa.
They do exist: They studied engineering in university, and they are members of their respective orders. For example:

http://www.mcgill.ca/undergraduate-admissions/computer-engineering

As a former defense contractor software engineer I can wholeheartedly say that just because you add more regulations and rules onto software processes does not improve quality whatsoever - it might impede quality, in fact. Most government guided software projects tend to be primarily about compliance first rather than functionality / viability, and that's the typical result - compliant software to some arbitrary regulatory standards that has minimal functionality of any sort besides the ones vaguely defined according to the contractual terms set out in the beginning because the customer didn't know what they wanted to build but only *how* to build it (the process being some waterfall process across dozens of stakeholders with political clashes fundamentally even if using "Agile").
Sounds like the whole thing was semantics anyway which makes it a huge waste of time.
&gt; Bogost is partially right. Today’s average software developer connects prefabricated pieces of code together to make it work, but touches little of its underlying infrastructure. Someone building software in 2015 will have a much easier time than someone in 1998, because so much of the work has already been done for us.

As opposed to all of those civic engineers who don't just design roads - they also fabricate road signs by fabricating metal sheets, mixing paint, and drawing the labels on them *with their own bare hands*.

And then they go out to the site to grade the road surface, mix and pour the concrete, and plant all those signs in the ground *with their own bare hands*.

And if you think that's amazing, consider the engineers who build bridges like the Golden Gate - *with their own bare hands*...

The reason software engineers are called engineers is that what we do is engineering. 

Whether it is difficult or not is beside the point (although I promise you it is more difficult that anything Bogost ever had to do in getting his degree in Comparative Literature). 

Whether it is critical or not is beside the point (although I promise you it is more critical that anything Bogost ever had to do in getting his degree in Comparative Literature). 

Whether it is infrastructure or not is beside the point (although I promise you it is more fundamental that anything Bogost ever did with his degree in Comparative Literature). 

What makes engineering engineering is that it is the **synthesis** to science's **analysis**. 

**Science is the discipline of taking the world, and breaking it down into principles.** 

**Engineering is the discipline of taking those principles, and building the world.**
As someone studying software engineering I have to discuss this a lot. I agree that most software development is not engineering, and that's because it's about what you're doing. If it concerns the public's safety it *should* be software engineering and therefore regulated, even if that's not currently the case. Software engineering is the software that shouldn't come with "we claim no responsibility for our code" type licenses. I think that we need regulations for software that could compromise the public's security or safety. When a developer is willing to sign their name and take responsibility for problems caused by their work without the malicious intervention of other parties, then they deserve to be accredited as a software engineer.
So now we have both sides of the "My butt is hurt because of the preceived glamor of a title" argument, can we put our junk back into our pants and just realize engineering is going to change over time but the base idea fits programmers and people in the physical space. The base idea is simply "The practical application of science"
They both are wrong. The problem is not that programmers are not engineers or not all of them are engineers.

The problem is that our profession does not have a legal separation of the terms.

In engineering there are engineers and mechanics. They are separated legally by requiring engineers to obtain licenses.

In medical field there are doctors and nurses. They are separated legally by requiring BOTH of them obtain DIFFERENT licenses. A nurse cannot come to facility and take a position of a doctor. No matter how brilliant he is.

Only in programming we have a situation where people like Anders Hejlsberg (creator of dotnet), Linus Torwalds (creator of linux) and some shmuck who makes worpress websites are called the same thing: Software Engineer.



&gt; Bogost is partially right. Today’s average software developer connects prefabricated pieces of code together to make it work, but touches little of its underlying infrastructure....

&gt; Instead of writing complex JavaScript, we can use jQuery. 

Um, who does he think wrote JQuery?  Is _that_ person/people an 'engineer' then? 
Parsing out a definition of a broadly used term and then defining its scope not only for yourself, but for everyone else is condescending and foolish, regardless of how liberally or conservatively you apply it.
Only a train operator can be called an engineer. None of you plebeians deal with 100,000 tons of steel moving at high speeds!
Right, because programming the software the powers commercial aircraft, military equipment, and NASA rovers is written by script kiddies without any sort of rigorous discipline...

Whoever the author of The Atlantic was, sounds like they're a bit of a dipshit, if I'm honest.

At any rate, engineering is a disciplined, measurable, deliberate approach to a given practice. Sometimes it's as simple as fitting the right known patterns/solutions to common problems, other times it's devising new solutions to new problems in a rigorous way. Given that, it doesn't matter *what* you're doing, all that matters is that there is a well established discipline behind it, and you are properly applying that discipline to achieve a desired outcome within a given set of constraints.

To be fair, software engineering is a very young discipline, but that doesn't mean it's not a discipline.
Ian is one of those people who goes on Craigslist asking for a Google clone on a $50 budget.
Duning-Kruger effect, the article.

The sentence where the author mentions "instead of writing complex JavaScript, we can use jQuery," as an example of rising above the "underlying infrastructure," is especially hilarious.
As an aerospace engineer, the Atlantic is garbage.
I'm guessing that in the United States anyone can call themselves an engineer? Here in europe (Belgium), the titles Ir. &amp; Ing. are protected &amp; you have to have a real engineering degree to use them.

Dear Atlantic blogger: Stop calling yourself a journalist. 
Why would anyone bothering to think otherwise respond. If someone is lacking that much knowledge, they don't deserve a second's thought from someone who *is* that knowledgeable.
Well, I came here to post the Atlantic's article and the first thing I see is a rebuttal to it. Quite the conundrum, I tell you!

At any rate, I think most of the main points of rebuttal have been covered- the short of it is that we would hardly be able to ship software at all if we were to adhere to the most rigorous standards when developing software. Add to that the pressure from management to provide a product in a timeframe that can often be called unreasonable (for commercial products at least, government products don't necessarily get by without being held to serious standards and requirements)- and you get a situation which can be be seen by outsiders as something that looks like what the Atlantic article claims. 
I've always seen "engineers" and "engineering" described as identifying and quantifying a problem, and then creating a solution to it.  How is that not what programmers do?    
I think one difference is that there is no standard of certifications, licenses, or requirements to prove competency to some sort of review board.
I'd say there are programmers and engineers. The last one implies you're the first one as well, but the opposite is not always true. We just can't say every programmer is an engineer.
I always thought that the difference is rooted in that engineering is about physical systems and programming about logical ones. And each other require a different mindset. 

I was very worried about this mostly in the mid 2000 when they were trying to use the same approach to problem solving in the way that you first build a project with its specs and then you build it, missing all the iterative evolutionary process in software development. So I fought calling programming engineering, because you cannot use the same approach.

Now with agile and the culture growing around it this has radically changed how software engineering is understood and I don't mind any more about the confusion as the different methodology is clear.

edit: minor clarification
Engineers use software created by programmers. Doesn't that make programmers even more important? /semi sarcasm
The real question is who gives a fuck? Computer science incorporates many aspects of many disciplines. The one goal of computer science/engineering/whatever the fuck is problem solving and leveraging quality/correctness against other variables such as resources/time/etc. Its an art more than anything.
I don't like the term "**Software *Engineer***" per se. 

I like to use when I want to make my profession sound nice.  However I feel it's a bit fraudulent as I don't do know anything about "engineering". 

However I also think "**Programmer**" is a deragatory term, lol!

"Software **Development**" (like developing photographs from film) sounds like the best option.

I think there is a place for describing certain types of programming as software engineering (and thus, those who do this as software engineers). If I were to make the distinction it would be that engineers design systems or parts of systems. This would be different than people whose work is primarily maintaining code, installing it, or configuring it.

It does seem in the computer science family of  professions that the word engineer might be applied to people who don't necessarily fit the description. That is, people who by analogy would probably not be given the title in other fields.

By way of annecdote, I have no formal training in networking, programming, or operating systems. When I was last searching for a job a friend who works at one of Google's datacenters sent me a posting for a position as a "network engineer". The job description did not suggest that the prospective employee would have any work architecting, designing, or configuring systems outside of what was already built. My friend suggested to me that it would be great, "You'd get to have 'engineer' in your title." The requirements were essentially an associates degree and an A+ networking certification. There are certainly many jobs at Google that, given the description in my first paragraph, would be described as engineering jobs. But if the word is to mean anything I would suggest it needs to be restricted at least a little. Again, my preferred distinction is that between having a formative role in product development or application and implementing something that has already largely been developed by others.
I feel I have grown beyond the programmer name, but programming is part of what I do, too. Merely I can now think about whole architectures like a hawk flying high and I can predict problems month away and change the design to avoid these problems. I dont build houses, I build skyscrappers, buildings that without carefull planning will just collapse. So you can label me software architect or programmer, whatever you want. 
The point of Engineering is: Scientists research how nature works, Engineers then use this knowledge to build more or less useful things from it.

Or from wikipedia: "Engineering is the application of mathematics, empirical evidence and scientific, economic, social, and practical knowledge in order to invent, design, build, maintain, research, and improve structures, machines, tools, systems, components, materials, and processes. [...] The term Engineering is derived from the Latin ingenium, meaning "cleverness" and ingeniare, meaning "to contrive, devise"."

Engineering isn't new, the people who build the cathedrals of old age were engineers as well as the people who build the pyramids or the Ballistas of ancient rome. 

Todays standards in engineering are quite new, possible because of hundreds of years of experience in certain fields. But software engineering is much to new to have settled enough to create such high level of standards. But that doesn't change the fact that by the definition of the word it's of course "engineering".
I could read this comments thread for months. Please keep going. 
The author of the original article seems to be under the impression that civil engineers are the only sort of physical engineers in existence.
Just ask any software engineer
Software engineering as a profession is incredibly immature. I wouldn't go quite as far as saying it doesn't exist, but it's a lot thinner on the ground than job titles would lead one to believe. Most programmers *are* much closer to carpenters than engineers.

Engineering doesn't require PE certificates or sitting for the FE, and I've known plenty of people with degrees and jobs in other engineering fields who didn't bother with either. The major benefit of regulating software engineering this way is job protection for the certified and increased influence for certifying organizations. I don't see any way, at present, that the consumer benefits.

This also comes up at least once every six months, if not once a quarter.
I think it's time to repost [They Write the Right Stuff](http://www.fastcompany.com/28121/they-write-right-stuff) Fast Company article.


Before I moved to the US I never heard anyone who writes code for a living call themselves an "engineer".

Honestly the only reason that the Atlantic published that article was to upset people, which gets more people to click on the article.

Software engineers are clearly engineers. Period. Applying technical skill and theoretical knowledge to solve problems? Sounds like engineering to me.
Too many half baked web dev's and network admins calling themselves engineers. Grinds my gears....

The title should be protected. Do degree accredited by whatever engineering body oversees that stuff in your country and then call yourself an engineer. In Oz it's a tougher degree than a regular IT degree.
&gt; An article written by Ian Bogost 

Oh, you could have stopped reading there and known it was going to be full of bullshit. Ian Bogost is a hack who makes a living attacking the tech industry.

He just happened to be the broken clock that was right when you looked at it when he went after Farmville and got his 15 minutes of fame.
Quoting [Professional Engineers Ontario](http://www.peo.on.ca/index.php/ci_id/2266/la_id/1.htm):

&gt; At present, many people who develop computer software refer to themselves as "software engineers" and to their work as "software engineering", even though they have never studied engineering and are not licensed or regulated in any way. PEO believes that this is a problem because it misleads the public. Because in Ontario you cannot call yourself an engineer, or say you practise engineering unless you are licensed by PEO, the public clearly perceives that a civil engineer, mechanical engineer or computer engineer is capable of providing services within the practice of professional engineering—and that they are accountable to their regulatory body for the success and safety of their work. There is no reason to believe that the public's expectations of software engineers are any different. Unless the title "software engineer" is applied only to those who are licensed professional engineers, the public will never know whether their faith has been misplaced.

A software engineer should be someone who is qualified and appropriately licensed to work on systems that affect the health, safety and welfare of the public.
I've been doing what is called "software engineering" since the 90s but still like to just call myself a programmer.  

Recently I attended the Amazon Web Services conference and they used the term "builder", which in a way makes a lot more sense for what I actually do.

Besides programming, I also gather requirements, manage people and projects, architect and configure systems, deal with security issues, script build and deployments, perform some QA and BA work, help document, help design, etc.

I think I may steal Amazon's title and just call myself a software builder.  It's less pompous and more about getting real work done.
&gt;The Atlantic Was Wrong. Software Engineers Do Exist.

Maybe they will exist once we develop methods for creating reliable software that are based on empirical knowledge  and not solely based on heuristics. 
If they were real engineers, someone should have been jailed for the Heartbleed. 

They're tinkerers, not *engineers* with all the responsibility that comes with this title.
I would consider many functional programmers to be 'software engineers'. Like other forms of engineering, functional programming is based on provable mathematical theorems, both mechanically and structurally.
It's clear that software engineering is different from "real" engineering. A software engineer doesn't typically take classes in, say, mechanical engineering or anything like that. In the same way that one might differentiate the social sciences from natural sciences, you could just be clear in what you mean. But it's such a stupid point to make in this case. It's not like anyone goes to parties and tells people they're engineers and they turn out to be software engineers. You say "software engineer." There is no ambiguity over that. People have some sense of what that means. They don't think that means you know how to build bridges or whatever.

So let's be clear about what we're *actually* arguing about here. What level of prestige should we give to software engineers? It's like an argument about who is better: men versus women. Or is curling a real sport (because sports that suck aren't real sports)?

P.S. computer scientists aren't real scientists either
Ahhh Asgard was a great name. 
Can someone explain it in simple terms what this platform is for, and how good it is?
that Pipeline view is a very pretty sight. I've been looking for an all in one deployment tool like this.


I haven't used Asgard, can someone explain how this improves upon it?
[Whenever I hear the word Spinnaker, I always think of this.](https://www.youtube.com/watch?v=rw-9be8ShHA&amp;t=16s)
So does this mean I can finally leave Jenkins? Please? 
I really like this visual view of the deployment pipeline/stages, and it something I've been talking about wanting for a while now to colleagues/contacts. I'd like a slightly more general scheme of stages having a pipeline and stages within them as well, almost like "hypercard for DevOps/CI/CD", where the cards are scripts themselves rather than building all the logic with the GUI.
The Netflix team has been pretty awesome. Always very high quality OSS. Also all the technical talks from any member of their team I've seen has been great! They really seem to know what they're talking about. Good stuff.
Wow, they've already replaced Asgard with something? Interesting. I wonder what motivated a rewrite.
Does anyone know how this compares to BOSH?
Is it written in Groovy mainly? 
Ugh, nobody should use Slack for FOSS community organization. It's far too limited for large numbers of transient participants.
interesting, but still very lack of document.  not sure does support docker?
&gt; *We will not be able to model wave or quantum optics effects*

:(
Jesus christ even his bachelor thesis is better than anything I'll ever do
Why can't I replicate the Dark Side of the Moon poster?
The demo doesn't work for me.  I keep getting "Your platform does not support the draw buffers extension. This demo won't run in your browser.", What sort of requirements is it expecting? My browser supports both WebGL and the WEBGL_draw_buffers extension. What more does it want?
I'm so gutted, did this in the 90s and only published on ompf.org, which has since disappeared (yes there's ompf2 now, but all those great posts...)
Someone should use this to make a realistic version of [Chromatron](http://www.silverspaceship.com/chromatron)
Fascinating stuff. The simulation was amazing (and fast, holy hell!)
This was a really awesome read.
Wow... just wow... This is spectacular.
I have never felt so inadequate. 
Has anyone tried to extend this stuff to 4D?
Top notch, as always.
Its nice to know people like you exist.
This is amazing work.
Correction to title: FreeBSD systems *at one company* serve one third of peak internet traffic in North America.

If you include FreeBSD systems running elsewhere, it's obviously going to be higher.
The BSDs are more esoteric but I like administrating them. There's a bizarre simplicity in their configurations (methods, structure) and I honestly have yet to find something as syntactically pleasing as packet filter (PF) on linux.
BSD is not dying &amp;mdash; Netflix confirms it!
I built a content delivery network back in 1999.

Even back then, FreeBSD had the 'reference' TCP/IP.  It was the only OS around that could saturate gigabit links at near 100% (that's how well tuned it was (and still is)).  Since all NetFlix is doing is caching and pushing fat TCP streams, I'm not surprised its their platform of choice.

My only question/concern would be that last I checked BSD didn't have a full implementation of RSS (receive side scaling), so its lagging the Linux market in 10G+ deployments. 
i have used FreeBSD and linux since 1996. both have their strengths and both have had their ups and downs

my impression is that there are no downsides to using FreeBSD, but also no upsides. its a linux world. there will be tiny little annoyances in the FreeBSD world as a result of most people developing for linux first, and i'm not sure what you get by getting stuff working on FreeBSD

the one legit advantage is the BSD license. if you want to sell a router or something that has a full OS inside it, and you want to keep all your additions secret, then BSD licensed code is the only way to go. this is why FreeBSD shows up in stuff like routers.

and to be sure, there are bits of FreeBSD that suck. like there's no inotify api. i can't figure out why the FreeBSD people push you to use more generic apis when it is clear that inotify is something a lot of people want. and the FreeBSD inits and other system tooling...hasn't moved in a very long time. if you are a sysadmin you probably like this...if you are a developer, it can feel like you are in the stone ages. Jordan Hubbard brought up the lack of innovation here:

https://www.youtube.com/watch?v=Mri66Uz6-8Y

i think his point is valid...there *is* cool stuff happening elsewhere in OS systems, meanwhile FreeBSD mostly looks like it did a decade ago...and most of the distinctive features (ZFS,gcd, all the security stuff from openBSD etc) are external codebases that are integrated in thanks to licensing
My first ever open-source operating system was slackware; the second was FreeBSD. I still have fond memories of getting it to work.
I wonder how much of that is Netflix
Remember RSS? Good times.
I'd love to see the source code for this! It's simple enough to be very valuable for learning, I think (I too would like to learn sail.js!). Any plans to open source it? Or at least host the code somewhere?
you should include more dates and time stamps on your site. Im very interested in it though :) Im reading the article about closures :)
Interesting, I like this idea! What are your sources for headlines? I take guess that some RSS feeds or Reddit?

Any future plans for the site or is already feature complete? :)


Great aggregator with a simple interface. There are several duplicates on the page (with even the same title, never mind click-throughs).  Perhaps deduping quickly based on the title of the link before presentation would be nice?
I love it, and I love however you're dynamically doing those thumbnails.

I've been working on something similar for video game news:

[newsbrute](http://newsbrute.com)

I've been focusing on the mobile view and the preview pane (click an article) lately. I'd be more than happy to have a couple people check it out and give feedback, as long as they don't mind me periodically updating and breaking it :)
Haven't looked at the content yet, just one small feedback: You should change the reactive-ness a bit, between around 900 to 1000px width there's a huge white space left and right - horizontally scrollable - and the text in the title gets cut off.
nice site.  I've added it to my bookmarks.

It would be nice to see a date when the article published or link added so as I scroll down I know how old the articles are.

Good job.
I saw a link to adsbygoogle.js (but no ads). Anyway, if you want to publish ads on it make sure you review the rules as this type of website is not allowed by Google (you actually have to create content). Anyway, it's a clean, legible website that doesn't suffocate you with unnecessary info. People love that
Why not enabling RSS for this? I'll be surely using it if it would have RSS.
Like the site. +1 for RSS
Great site, I've added it to my bookmarks!
What is pagination?
This is really cool! Any interest in open sourcing it? It's definitely something I could get behind.
Bookmarked!, great site ;)
Is this reddit? :P
Lovin the name Devalate, the site is nice and simple too.  GJ

Any info where the name came from?
Thanks for the productivity drop
Neat - I'll use this for a week and post any comments/issues - looks great on first impression!
I see that Bower is alive and looking for contributors. Also that Bower is alive and looking for contributors.
Looks nice. Bookmarked. Like other have said, I would add timestamps, and filters so I can go straight to news about favorite languages.
Looks cool! However, is there a reason "Bower is alive, looking for contributors" is on the list *twice*?
Now make an email service so I receive emails on programming news! 
Funny, this is basically how FARK got started.  Pre-Reddit stuff.
Looks great on mobile. Needs comments though! 
Awesome website! Thank you for creating it! Any chance you might share the code or part of it? Or maybe are you planning on writing about it? Pretty interested on how you put it all together.
You might wanna set your &lt;body&gt; background-color, it's commonly set to white by default in the browser CSS but it's easily adjustable at least in FF and there are good reasons to actually change it (while loading a new page I get a nice dark grey canvas instead of a white one).

[This is what your site looks like for me.](https://i.imgur.com/e4Q5xWu.png)
Can you add iOS / mobile thumbnails please, I'd like to add this to my home screen.
Works great on mobile! Thanks!

Bookmarked on my home screen.
This is awesome! If I could make a small request, I'd love to be able to include this in my protopage portal... Any chance you could output it as an atom/xml?
Swanky! Will add to my reading list.

I assume it's already on your todo-list, but it would be nice to be able to filter categories. 

Keep the procrastination strong!

We've hugged it to death.
We use Racket as one of the primary teaching languages at my university. We have a lot of the language devs as faculty, so they teach with an intimate knowledge of the language. As a student, I love the language. It was the first programming language I learned and served as a much better intro to computer science than C or Java. 
Racket is quickly becoming my go-to language for fun side projects. It's a very pleasant language, and it helps keep me from going crazy due to the stack I have to work with in the office ;)
I used Racket to do my term paper in college. It was so fun and easy (although the problem at hand was complex) that it felt like cheating! :D
Can you use Racket for SICP?
If I was to make my own hipster[1] tri-fecta of languages, it might be Rust, Haskell and Racket. That should cover most bases that I have in mind for hobby stuff.

^([1] I can't make a post like this without being a *little* self-deprecating.)
What is so unique and fun about Racket? 
I write racket with sublime and run it with bash because i don't like the way the official IDE feels but I miss the way the IDE indents a lot.

Life is compromise.
How apt is the Racket ecosystem(libraries) for writing a web API?
I am currently doing all of my functional programming and a lot of scripting in Racket, but I have just barely scratched the surface, haven't even started using macros and call/cc yet. I am mostly an Emacs user (and also use racket-mode) , but DrRacket is such an enjoyable environment, I find myself using it quite a bit.

I think it can actually be used for teaching programming to young people and I wished I had learned this as a first language instead of BASIC.
A new macro expander is just what I wanted for Thanksgiving.
i'm torn on Racket. i'm liking what they are doing with Typed Racket...but once you introduce strong types, you seem to lose the primary cool factor of s-expressions...being able to change the shape of your data at will

when i use Typed Racket, i feel like the parens are just syntax, since the shape of my data is determined by the type

it is nice to see a Scheme-like tool that just dumps all the idiotic srfi-* names for libraries...what a terrible idea
Accompanying talk https://www.youtube.com/watch?v=ABWLveMNdzg
What would be the best book to get started in Racket programming? Are there any books that cover the Racket compiler internals in detail?
GTK3 support and typed racket improvement are really nice, thanks racket team!
I wish they would improve the scrolling performance. It's awful on retina displays, and really slows productivity. I have to throw my files into sublime when I want to browse through them, which isn't a great workflow.
GTK3 support, thanks guys!
I like scheme and I like the idea of racket. But the editor and environment is a big part of it. And in practice, the editor and environment just doesn't work for me, since you can't change the keybindings, and the defaults work badly with my Non-English keyboard and language setup. 

I come back to it over and over, only to give up in frustration after a while.
My university uses racket as a teaching language. Nobody likes racket. 

EDIT: downvotes for making a true statement that nearly everyone in the course dislikes racket? nice.
That was fascinating. Thanks OP.
Can't comment on the blog, but there is a line that reads funny:
&gt; However, that still isn’t doesn’t explain...


Hey /u/burntsushi, looking forward to buying your book, whenever/whatever it will be.  I really enjoy your long articles!
Maybe I am missing something obvious, but it seems that the FSA construction could be made a lot simpler if the keys are analysed forward AND backwards through the network, however this requires a bidirectional link-list so that every node is aware of both the upstream nodes and the downstream nodes that are connected to it.  Since the network starts with a common node and ends with a common node, then any new key can be searched forward through the network until a character is found that is not yet represented, then backwards from the end of the network until a character is found that is not yet represented.  You will then know which middle characters need a new branch in the network and where to connect them.  For example:  
  
Given a starting network of:  
A-B-U-N-D-A-N-T-L-Y  
  
To add the key "ABSURDLY", the FSA builder would search the key forward and determine that prefix "A-B" is already in the network, then search backwards and determine that the suffix "L-Y" is already in the network, which means that the middle nodes of "S-U-R-D" need to be linked between "B" and "L" nodes.
I have question. Why does this work?

    let file_handle = try!(File::create("set.fst"));
    let buffered_writer = io::BufWriter::new(file_handle);

Shouldn't it be?

    let mut file_handle = ...
I agree with OP's point about server programming – the libraries aren't "there" yet. And I also agree with the conclusion: If you can afford GC, use it. 

I initially found the syntax alien, too, but it grew on me faster than I had anticipated. OP should certainly look into third party crates for things like HashMap literals or server programming (there was an interesting work to use the type system to ensure certain predicates for client-server protocols recently, I keep forgetting the name), or simply ask on IRC or rust-users. Also [awesome-rust](https://github.com/kud1ing/awesome-rust)!
&gt; Explicit borrows. I really dislike the fact that I have to tell the compiler that I’m the function I’m calling is borrowing a parameter when the function signature itself only takes borrows. It won’t compile otherwise (which is good), but… since I can’t get it wrong what’s the point of having to express intent? In C++:

&gt;     void fun(Widget&amp; w);
&gt;     auto w = Widget();
&gt;     fun(w); //NOT fun(&amp;w) as in Rust

This one of the things C++ most definitely gets *completely wrong*. Being explicit about this is *absolutely necessary* for readability! I don't want to have to go looking up headers for every single line of code to find out if a value passed to a function will be mutated or not!

You're going to write that ampersand once. You can afford it for the one hundred times you're going to *read* that line.

Rust gets this one right in my books.
Historical note: we rejected syntax for HashMap literal because there are just too many kinds of maps. What if you need a BTreeMap instead of a HashMap? Or a VecMap? Unlike vectors, it's not clear which one you'll "usually" want. And HashMaps aren't nearly as commonly used as they are in more dynamic languages. And there are crates which define macros for this if you really want it.
&gt; Serialization.

https://github.com/serde-rs/serde?
What's so good about rust, this subreddit is always full of it, why? 
I'm getting somewhat bored of "language impressions" posts from random programmers on their personal blogs. They all end up restating the same pros and cons of each language, and none of them have any analysis of the pros and cons (i.e. why certain choices might have been made, why the author thinks they weren't the right choices, etc.)
On my front page again Atila?

Nice article/blog, though I suspect I'm not going to be touching Rust any time soon.
This guy has a monstrously high opinion of himself, but I don't think he's wrong about YC, venture capital, and startup culture. I think DHH made a similar overall argument [recently](https://medium.com/@dhh/reconsider-41adf356857f), without calling himself "visibly smarter than almost everyone in the startup elite".
Arguably, this post is a better fit for Hacker News than /r/programming. (I'm serious. while critical of the whole startup world, it is relevant, well written, and contains no obvious bullshit —I wouldn't know about subtle mistakes.)

So I searched for it in HN, and ended up submitting it. It [appears](https://news.ycombinator.com/item?id=10523672) the post (the whole domain, maybe?) was moderated away before hand.

_EDIT: what do you know, the submission isn't dead any more! But it hardly matters since it is now in position 88, far from the front page. Well, now HN has plausible deniability…_

Well, I have no problem with those guys playing their little "Let's pump up trivial image sharing apps to billions and sell them to bigger idiots" ponzi scheme in private.

The thing is that they poison the minds of many many people. Not only in the US - but also here in Europe. Politicians are all like "we're creating a second silicon valley" without understanding why the SV VC ecosystem works. Otherwise smart technical people waste their time on creating trivialities/copying US startups and then wonder why no one buys them out or why the business never makes any profit/revenue. Normal software studios don't get any funding because "boring products" (which would be totally profitable - but sadly are not shiny enough for the new startup driven economy). 

And in the states a shit ton of human potential gets wasted on making people click pixel cows. Potential that could do wonders in medicine, aerospace, energy - hell everywhere where there are real problems to be solved. 

It's just all a big waste of human capital. :/
The article touches on a very important subject, and it's something that we as software developers need to stick together to help each other out on.

That subject is, developers being under-compensated, even though we do the bulk of the work. This comes mostly from not knowing how to negotiate with employers.

YC teaches founders how to negotiate against investors *and* against employees (developers and designers).

YC is not out to help the engineers. It's out there to help the business types. In fact the whole startup culture seems to be about [business types swindling engineers][0].

Something needs to be done. Some foundation similar to YC. Some program to teach *makers* how to negotiate against business types.

The YC narrative would have you believe that's what they do. But the type of people that YC accepts are not makers; they are business types.

If you read PG's article about [founder characteristics][1], it's written all over it. Determination, not intelligence. "Naughtiness", a euphemism for "knowing how to exploit". Having some engineering background when you're a business type helps, but is not the main requirement at all.

YC is not about empowering makers.

[0]: https://medium.com/@rboyd/our-team-won-startup-weekend-and-all-we-got-was-a-shitty-new-boss-35f1d1f1f267
[1]: http://www.paulgraham.com/founders.html
He's got a point about in the past, a company founder would work super hard on getting the business and product running. And these days, they're working hard on getting additional funding, and maybe spending 10% of their time on the actual business goal.

Hustlers. Scammers. Whatever you want to call them -- what ever happened to providing good value to customers and building the business over time?

I've seen them at SXSW.  It's not "pivoting", it's "flailing-about hoping to keep the VC money coming in."

&gt; If you wanted to get rich, how would you do it? I think your best bet would be to start or join a startup.

Not technically wrong but not mentioning the odds makes this extremely disingenuous and, in Graham's case, very self-serving. A similar argument would encourage you to play the lottery to become rich, because let's face it, joining a startup if very similar to playing the lottery.

The easiest way to avoid all of this is to build an actual business. Then you at least have *some* measure of control over your personal/professional outcomes. 

But building a company that has little purpose beyond trying to be a funding magnet...that kind of honey dicking is soul killing, and rarely ends well.

The irony is that the real core advice from YC - "build something people love" and "start by doing things that don't scale" - is extremely sound. It's when the money and need for external stamps of approval come into it that things get...weird...fast.
For those curious why Church was banned from HN, [here is the announcement](https://news.ycombinator.com/item?id=10017538).
&gt; The lesson of the 19th-century gold rushes, that much of the money was made by people selling tools and water, is well-understood. So why didn’t more of the prospectors go into the tool-selling business? Because that was the boring, slow way to get rich. Discovering a gold vein was the fast, flashy way that had more visceral appeal, although it was far more unlikely to work out.

Weren't the guys selling tools and water *also* startups?

It seems to me that's more a lesson about what kind of startup to start, not (in itself) a warning against startups. Sell something obvious, boring but necessary, in an area where the market isn't already saturated.

The problem is, of course, that for most of us, finding areas where the markets for obvious, boring but necessary items aren't already saturated is rare and pure luck - if it were a predictable matter of ability or investment, the already-wealthy incumbents in other areas would already have predicted it and already have the relevant ability and the money to invest in expansion.

The Economist just posted an article about y combinator http://www.economist.com/news/business/21677636-tech-talent-spotter-has-come-dominate-silicon-valleys-startup-scene-y-combinator-x 
&gt;  the current crop of founders are people who want to be corporate titans: they just want to get there faster than their parents did

Well said.
missed opportunity to say theyre considered harmful
I feel that this is the right place to drop https://lobste.rs/about

Looks like it fixes the objectionable xxx-ban practices of HN (slowban, voteban, hellban, secret moderation).
Now if I could find an invite...
This brand of elitism is depressingly familiar to me, having done a physics PhD, and probably having espoused something similar at earlier, less mature points in my life (though probably never to this extreme). It's quite common in math and physics, so I'm not entirely surprised to read a math PhD write something like:

&gt; 32-year-old, visibly smarter than almost everyone in the startup elite

Most of the world is not solely about your ability to do extremely technical proofs that require a very high IQ and abstract reasoning ability. It is certainly helpful to a certain point for a programmer, but past a certain point it provides diminishing returns compared to people skills, maturity, etc. When you have someone who doesn't seem to mind putting up comments like this for the entire world to see, you are forced to doubt their other qualities.

There is also always a large component of luck in everything, too, and hopefully nobody would deny that. I don't necessarily agree with Paul Graham, but I just find Michael Church's writing unbearable. Reading his writing makes me hope that I never meet him, and more importantly never have to work with him.

Note that I'm not knocking on math and physics PhDs in general, obviously many of my good friends have similar backgrounds. It's just that this kind of egomania is more common there than anywhere else; thankfully most (but evidently not all) people grow out of it.
I love that this enormous wall of text is marketed as just "Part 1."
Oh hey as long as michaelochurch is posting here, what's the deets with that internal google mailing list story? 
&gt; Paul Graham was a protege of Robert Morris at the time, and while I won’t comment either way on the speculation of whether Graham was involved in the debacle– honestly, I have no idea...

"I have no idea if Paul Graham murders penguins, I'm just saying he lives near a zoo."

You lost my interest in the second paragraph, son. I don't see how you can be expected to be taken seriously when there's so much rhetoric in your writing.
I'm sorry -- literally the first couple of paragraphs annoyed me too much to keep reading.

The "Morris Worm" was an accident, because it was an attempt by a (very young) Morris to **fix** a serious security hole in networks that companies were too lazy to fix. His unauthorized patch had a bug, which is what made it destructive.

But that ambitious bit of attempted unauthorized patching is a huge, screaming far cry from the characterization you're shooting for. You just tell the world "they were black-hats, wrote a worm, caused millions of dollars of damage." "Black-hat," used with no other explanation, includes the class of people who want to destroy and deface, and people who take data hostage. (In fact, Morris *did* effect the change he set out to, simply by raising awareness of that specific bug).

And hearing that you're banned from HN doesn't make me very sad -- I think you might have been trying too hard to self-promote on there. I recognized your username in a big way, and not in a good way. And I'm not someone who drinks startup Koolaid, either.
The best thing about this is that PG will read it, and he will be fuming. :-)
This is a pretty strong attack piece on Paul Graham and YC, as well what they represent. It seems like there's an interesting story about a PhD candidate who dropped academia for a startup buried somewhere in there, but I'm having trouble picking through the cruft about how startups are bullshit, Paul Graham is selling us bullshit, and "i'm so much smarter than these jerks around me".

&gt; Is Uber a tech company or a transportation company that uses technology? What about YC-backed Airbnb or Reddit? You don’t need to be at the forefront of technology to found one of these companies. You need to have a friend who knows how to build a website. These companies are marketing experiments using technology.

reddit? Yeah two brogrammers can knock that out in a weekend, the value is in the initial idea of what is basically a community moderated twitter feed and the immensely diverse community they've somehow managed to amass.

AirBnB? It's not a hard problem, but it's definitely an order of magnitude harder than reddit, in my opinion. I've designed a automated rental system and it wasn't terribly difficult, but nobody builds an AirBnB clone as a [coding tutorial.](https://thinkster.io/mean-stack-tutorial)

Uber, on the other hand, is [hiring entire groups of robotics researchers](http://www.nytimes.com/2015/09/13/magazine/uber-would-like-to-buy-your-robotics-department.html?_r=0) and is pretty solidly associated with the future of self driving cars. 

&gt;I don’t blame Paul Graham for selling a lifestyle that worked for him in a time (late 1990s) of radical wealth creation and when the owning class was less attentive to details (read: not capable of capturing 100% of the value generated) than it is now.

I work at a company that went through YC, we're now up to 50 people with the new sales hires. We have millions in funding, tens of thousands of companies using the product, and sales team that's had a pretty solid start. Maybe it didn't work for the author, but that doesn't mean PG, YC, and modern tech startups are bullshit.

The cynical, snarky tone makes it pretty grating as well. As somebody associated with the group being lambasted, it pretty solidly put me off from the beginning (hence this comment).
TL;DR: Paul Graham has a crush on me for my impeccable qualities (I pursued a PhD (in math), am smarter --&gt; he has millions, maybe billions(!!!), but again, am smarter than he is *and* I'm fucking awesome at writing, hence why Paul unleashed his fury on me shortly after seeing how I, and my weapon (words), could tear down his entire empire! (we'll get to that later). At the ripe old age of 22, I was naive enough to let other people do a lot of the thinking concerning the world and my career for me, despite how intelligent, creative, and articulate I was (an am!). It was then where I duped into the ponzi-scheme that is the start-up world, aka. Mean Girls of Sillicon Valley. It all started with my man crush on Paul Graham and the ideas he so eloquently bestowed on my fragile brain. Oh, how naive I was...You see, he PROMISED I would be rich if I founded a start-up. Let me repeat that: He PROMISED I would be rich if I took my oh-so-valuable time to found a start-up that would change the world and make millions. For the sake of my biased and fundamentally flawed argument that definitely doesn't overgeneralize a sector of the economy that, like most, has disproportionately more losers than winners, I will not disclose what my start-up was, whether or not it was truly valuable, all things being considered, or why it failed and any other detail that would require me to take a serious look at whether my company failed because of 1) me or 2)dumbfuck investors. Spoiler: It was the dumbfuck investors. The only thing that matters is the following: In my tenure as a start-up founder™, I discovered that it was all a lie. Yes. You heard that right. Here's an extremely biased and unobjective look at what the start-up™ world is like behind the scenes: Contrary to popular belief, Sillicon Valley is NOT where progress is being made economically or socially. Like almost everywhere in the world, it turns out that the concept of being "cool" and cliques EXISTS in Silicon Valley, wherein NERDS roam the streets, some of which are founders or employees who not only make over $100k a year, but are ASSHOLES. Yes, you read that right. There are ASSHOLES and people with qualities I, or most people, don't like that are mildly successful. I did NOT sign up for that. To my understand and knowledge bestowed upon me by my boo Paul Graham, SV was supposed to be a magical place where everyone gets rich, people only have qualities I like and aren't assholes, and where the idea of an "ego" doesn't exist. Alas, it was all a lie. It turned out that ego's are everywhere and that people become greedy and somewhat evil when they think they're entitled to millions because their start-up has 10,000 users and growing, or simply because they're in the position of an authority figure. It's almost as if humans are flawed in some ways. But that doesn't matter here, what matters is that I was PROMISED millions and a fairy tale--and I got nothing but time wasted. This, again, is unfair because I'm 1) talented 2)articulate 3)intelligent 4) know more than VC's will and understand the world better. Nevermind VC firms like Lowercase Capital, a16z, or Founders Fund that also are notorious in Sillicon Valley or investors like Peter Thiel, Chris Sacca, Jason Calancanis, SLJ, or anyone the valley with a voice that wouldn't hesitate to call bullshit on sketchy tactics employed by incubators that make their investments less appealing. Y Combinator has a monopoly, and because of them, the entire start-up ecosystem is doomed. Consider yourself warned: **Sillicon Valley is not the fairy tale we were promised, and Paul Graham is no longer my boo. Fuck you, Paul Graham.**
You know OP's wrong because I have a Ph.D in quantum physics (one step up from math), and I say so.  Oh wait... no, that's not what I meant to say.  What I mean to say is that having a degree isn't worth squat when it comes to startups.  You have to work super hard and have a diverse set of skills.  One of them may just be how to BS well.
Oh please. Michael O Church is bad for the world.

Is it another one of his walls of text deceptively sensical leading to nonsense? Yes it is.

Remember, this is a guy who's banned from Wikipedia and hackernews because he's a massive troll, disinformation spreader, and a dick. He throws out horrible accusations against named people, then re-targets when it turns out the original perpetrator was innocent without even apologizing.

You can't listen to a pathological liar, because you won't be able to tell the truth from the lies. And you can't listen to Michael O'Church because whatever he is, he's a source of disinformation.
Mooom, Paul told me to work in a startup to get rich, and I didn't get rich!! Waaa waaa, that's his fault, evil paul!
I understand the critique to Y combinator and all the startup church. However, it is pretty pueril to blame paul graham for what he thinks is his failure, and for preventing him "to be where he should have been". This is nonsense, he is the only person responsible for his failure, blaming it on an article of pg sounds very much like delusion.
This article actually made me think more favourably of Paul Graham and Y combinator. His delusions of adequacy, his pathetic attempt to associate Graham with the Morris worm (I'm not saying Bob Saget raped and killed a girl in 1990) and his multitude of conspiracy theories dressed up as facts all make me passionately want to disagree with him.
YC has backed something like 900+ companies, which is a larger group of people than most high schools. So the comparison to gossipy girls and the "cool kids table" is apt, but also a little silly. Maybe it's a meta-recursive case that, just like real life, the gossipy cool kids don't end up doing very well and it's the studious brainiacs that end up ruling the world.

The author accuses Graham of being materialistic, then goes on to complain that he [the author] is behind "where he should be" for someone his age. What does that even mean? Where "should" you be if you ostensibly don't care about materialistic wealth?

There are a hell of a lot of misconceptions about startups, just like the movie, game, and music industries. Who you know matters, but there's nothing that can stop someone with a truly great idea that also has hustle and entrepreneurial spirit.

I don't know many people who don't understand that startups are inherently risky. It's no secret that people with money talk to other people with money. You don't have to be smart to have money or to be an investor, so it's also not a surprise that there's a herd mentality at times.

I just don't understand the point of this essay. He tried a startup once, it didn't work out, and now he's here to reveal the dark, seedy underbelly that isn't really all that dark or seedy? 

Someone as "visibly smart" as him should be able to understand and learn from what went into his own failure. Practically every self-made millionaire and billionaire on the planet encountered failure multiple times before hitting it big. The difference between them and everybody else is they didn't quit. They took their natural talent and refused to stop believing in themselves and chasing their dreams. Any real entrepreneur intuitively gets that...you literally either get rich or die trying (or, better yet, just enjoy being your own boss and having the freedom to work on things you're truly passionate about). 

If that's scary or unattractive or money is more important to you, then go work for someone else -- but not a startup, because the founders are just out to screw you, I guess.

(n/m...shit! 10 seconds of googling reveals the sinister damage control machine the sexist Graham is using to cover his own tracks http://www.paulgraham.com/wids.html)


Almost stopped reading in the second paragraph,when he calls PG a cracker because his "mentor" and business partner 20 years or whatever wrote a worm. I read the next paragraph and he uses a straw man argument about getting rich and stopped reading there.
Some thoughts:

1. Paul Graham's advice was good advice for 2003-2006. Of the people I knew in who were involved in meetup groups coming out of the first startup school, probably 1/10th ended up making seven plus figures. Even those who did not hit it big, ended up with solid jobs in a growing field.

2. Michael generally behaved in ways that would get him banned from most any forums. He was a broken record, made vulgar personal attacks, consistently negative, and engaged in all the [negative anti-community behaviors described by Jeff Atwood here](http://blog.codinghorror.com/what-if-we-could-weaponize-empathy/). You are pretty much going to be banned from any forum if you make vulgar personal attacks against people the forum owners personally know and like. That is the way the world works. Michael has this bizarre habit where he rants about humans who act like perfectly normal humans.

3. I would never work 80-hour weeks for .05% equity, unless it was a temporary crunch for a company that had true Google-like prospects. Heck, I didn't even work 80-hour weeks for single digit percent equity. To the extent that is happening, that is insane. But I don't know anyone who was working like that. Most people I know in the startup world get fairly compensated for their time. If they only get .05% equity, their salary is market-plus rate and their hours are reasonable.

4. "HR practices that will prevent, say, the extremes of harassment for which the “brogrammer” startup scene is known." Starting strength and basketball during lunch were popular at our company. But it seemed to me that the women at the company liked that we were fit and masculine, and not nerds with star trek stickers on our monitor.

5. "They’ve outsourced their talent vetting to a man who routinely makes foot-in-mouth statements about foreign accents or about those uncanny people with vaginas who, according to legend, make up half the species." The Paul Graham quote in the article he links to is not at all objectionable: "It's already too late. What we should be doing is somehow changing the middle school computer science curriculum or something like that. God knows what you would do to get 13 year old girls interested in computers. I would have to stop and think about that." PG is correct. Nobody knows what it take to make girls love programming the way that nerdy guys do. Frankly, I think it is in the genes. Guys just have a much greater obsession with dealing with abstract concepts, girls like dealing with people more.

6. I do agree that the startup world is less about creating innovative technology, and more around capturing opportunities for natural monopoly that have opened up due to the tech created by others. I'm not sure what can be done about this, or how whining about it helps.

7. The startup market is much more saturated right now. It is much more the "in" thing to do. As the ratio of founders to opportunities gets out of whack, then arbitrary factors such as connections or acceptance to YC become much more important. Also, the ratio of people-with-existing track record to opportunities becomes quite big, so all people entering the scene in 2015 have ten years of prior-founders to compete with.

8. "*I cannot say the same of Y Combinator, which seems to be replicating the negative traits of such institutions, without any of the positive ones (such as noblesse oblige or a value on classical erudition and the preservation of culture).*" Noblesse Oblige is something for the shareholders to do, once they have received the profits of the enterprise. YC has an obligation to return money to its partners. Hopefully the partners will use this money for good. Hopefully successful founders will use their money for good.

"Off the bat, that’s fucking terrible advice. Let’s be realistic about wealth. It’s usually attained slowly, not in some get-rich-quick gambit that has a better-than-even chance of costing you money and disrupting your career."

The advice is perfectly fine.

Startups are not solely about "getting rich quickly". You also gain a lot of experience, which you can put to good use lateron anyway.

And several startups became a success - github, facebook and what not. Even Microsoft used to be small at one point in history. Google too.
Michael o church sounds like a nonsensical SJW troll
http://img0.joyreactor.com/pics/comment/monty-python-gif-more-in-comments-funny-1154507.gif
I get the feeling the author wants to rid the world of PG, YC, and anyone who might not currently be the best engineer.
I think some people are slightly overreading the text. I think the key idea is not that dynamic languages are going to actively die, but rather the following:

&gt; This is my bet: the age of dynamic languages is over. __There will be no new successful ones.__

Emphasis mine (original emphasized the second clause of the first sentence).

Let's see... of the up-and-coming languages, the only one I can think of that might disprove that statement is [Elixir](http://elixir-lang.org/). Everything else I can think of as "up and coming" is static.
I've never liked dynamic languages for all the reasons outlined but it's clear that a large portion of programmers don't agree with me and probably never will. 
Nope, dynamic languages aren't going anywhere. 

Don't get me wrong, I agree with almost every point made by the author, and as someone who writes a lot of python and haskell I see the benefits to both languages. I'm also a scientific programmer, I hardly write a python file without importing numpy. Can you imagine what an equivalent library would be in Haskell? What about pandas? Seaborn? Scikit image? These are all very powerful libraries because of their dynamic nature, this allows me to speed through interpreting my data as something useful instead of worrying if I've covered every edge case. I work in a fast paced world where it's more important to have that report put together than to have a clean code base of one off scripts. 

That being said, I find certain haskell libraries to be far and away better than their dynamic cousins. Servant for Haskell is an incredible piece of engineering, it's composability and ease of use astounds me, and having type safe URLs with such little effort is great. Tools like lens introduce entirely new concepts for manipulating complex data structures, and anyone who has become acquainted with arrows can easily see their potential. 

I think both sides of the spectrum have something to offer. Dynamic languages excel at certain tasks, static languages at others, and both are good for general purpose. Know how and when to use both. 
&gt;So my co-worker Adam and I set a goal: turn all these runtime bugs into compiler errors. We wanted a big, fat FAILED TO COMPILE message if we tried to insert null into a boolean field.

Not a Haskell programmer, but as a C++/Python guy I can relate. To me it seems the problem is we either have this binary system of either strong type all the time `const static char * != static char *` or no type at all `new_pointer = (void *) old_pointer`, when in fact what we really need is a data validation at the interface. Having a type system is a rather crude way of doing it. It basics enforces that 2 pieces of data are of the same representation. For example date and month are both integers but flip the two and things can get messy. What we really need is to a constraint that is passed along the interface IE, date is valid 1 to 31. month is valid 1 to 12. You can either check this at compile by labeling every unique constraint, then at the call site you compare the constraint IDs between caller and callee; or at run time when the system halts at the very first instance of month overflow. You can then enable this constraint during debug and disable it at production.
I don't understand what the examples have to do with dynamic languages. You can provide bad documentation and make questionable design decisions in static languages too.
A lot of the power of Ring comes from being able to add custom keys using middleware.

How would you handle adding custom keys to your request type in a strongly typed language?
Not gonna happen.

I have talked with plenty of people that dislike dynamic languages, just like I know many who love them. Often, its got nothing to do with expertise level. Many of each category have a computer science, computer or software engineering degree.

I have found it to simply be a matter of taste. The fact is that the majority of code running the most popular web applications has both dynamic and static code. Facebook has PHP (now also Hack), C++, D. Google loves Java, C++, Python. Twitter has Ruby, Scala, Java. Reddit is built with mostly Python. And you can look everywhere and find the same. Of course, add JavaScript. "You got no choice with JavaScript" is no longer true: you have TypeScript, CoffeeScript, Elm, PureScript, Haste, etc. Still, the most common choice remains JavaScript.

People who hate dynamic languages, to me, are just people who like to think with a more rigorous method. And that's great. But it gets tiring to keep reading things like this post: just because you prefer something, it doesn't mean the alternative sucks.

Personally, I like both. C# is awesome, Python is awesome, everything is awesome. I'm a language paradigm whore and I am not the only one.
Hopefully, more dynamically typed languages are going to take hints from statically typed ones, like Julia does. Then, people using them will gradually realize that the static parts make it easier to convey your meaning to the other programmers, your future self, and the compiler, making all programs more maintainable and efficient, while the dynamic parts only make for abysmal error messages, and their advantages can be easily replaced with a *real* type system supporting existential types, polymorphism on return value, etc. All according to plan :)
&gt; Every library function you call, unless you have it memorized, requires hunting down the source or test files for examples on how to use it

This is why microframeworks are so popular. You can hold the whole thing in your head. I was working with Spring today which is a massive behemoth of features, yet I can pretty much always discover what I need without leaving my IDE.
For a long time I held this opinion too, but I no longer feel this way. The simple fact is skipping compilation and running immediately, and reloading code live are great features, and the languages can be extremely productive.

Lately I've been learning Common Lisp. I can't imagine any statically typed language originally from the 50s being as flexible and holding up as well as Common Lisp does.

I love static typing and static type systems, but dynamic languages can be so infinitely malleable, that with good API design, your programs become *so* simple, short and obvious that the need for static typing isn't there.

Would it be nice? Sure. But you can get by without it.

And if you're writing an OO ball of mud with 50-200 line methods all over the place, sure, static typing will help. The flexibility of dynamic languages will give you plenty of rope to hang yourself with. I have seen all manner of totally insane program designs that should have never been attempted.

But just like you can leverage static typing to catch bugs at compile time (which is amazing), you can leverage dynamic typing to allow you to have minimal code and substantial flexibility at runtime.
Film at 11: Death of dynamically typed languages predicted.

Prismatic Schema? :pre and :post conditions? Records? deftype? Typed Clojure? There are a myriad of ways of addressing this particular itch in Clojure if you want to.

Yes, Clojure is one of those languages (pretty much like Ruby) in which data models tend to end up in maps and other simple types. Years ago, when I was moving from Perl to Ruby, I got frustrated because my first Ruby program just used simple hashes everywhere which made it about as difficult to get right as my Perl code -- nowhere does Ruby force you to model your data more tightly (i.e. in a new class). But this doesn't mean that you are not able to do this, so the second installment of said Ruby program used classes to model my data.

Now, Elben voices his frustration with some of the existing Clojure libraries which use simple data structures where the rules of how to use them are unspecified. In particular, he picks on [bidi](https://github.com/juxt/bidi) where he says that he struggled to figure out the syntax for multiple routes. JFYI, that's documented in the very next paragraph after "Take 5 minutes to learn bidi ". Now, if you can't be bothered to do even the most basic reading, and end up frustrated because Ctrl-Space will not help you to magically add keys to your hashmap, I guess any flexible data structures is not for you. You're now confined to always define types and specialized classes everywhere, congratulations. Oh, and no, use of generic types (Java, templates in C++) are also of course no longer allowed.

Finally, for the record, I'm not against using types. As always, there are many pros and cons to consider.﻿ Choice is a good thing.
For quick prototyping and creating small programs I always prefer dynamically types languages such as python. Many of the things that I write are small tools that I know I will never need to modify and might only use for a few weeks. Considering pythons community (large libraries) and ease of prototyping small programs, i'll always prefer it to something like java or haskell for these sorts of tasks.

With that said. For any project that I expect to work on for more than a few weeks, i'll start in Java, Haskell or Scala depending on what kind of project it is.
Hilariously relevant: https://twitter.com/chris__martin/status/630532950484881412

&gt; "Dynamic typing": The belief that you can't explain to a computer why your code works, but you can keep track of it all in your head.
&gt; This is my bet: the age of dynamic languages is over. There will be no new successful ones. Indeed we have learned a lot from them. We’ve learned that library code should be extendable by the programmer (mixins and meta-programming), that we want to control the structure (macros), that we disdain verbosity. And above all, we’ve learned that we want our languages to be enjoyable.

I have held this bet for a while now. I'm happy to see that static/strong typing continues to be a thing in recent languages. Dynamic/weak types are useful in a tiny fraction of the code. The rest benefits from being clearly defined.
&gt;We will see a flourishing of languages that feel like you’re writing in a Clojure, but strongly-typed.F# is like this. Its amazing. You're writing in what looks like dynamic code but its somehow magically strongly typed static code with all your compile time guarantees.
&gt; A land green with libraries rolling in composible functions, immutable data structures, and kind people. How beautiful is your syntax, and how wise are your sensibilities!

That's exactly how the F# ecosystem feels like, and I get the added benefit of beautiful static typing with strong compile time safety.
It seems like Javascript is taking over the world though, it's on the server side, it's in my Gnome, it's in my QT, it's in the browser which is only gaining ground as a deployment target...is that not a dynamic language?
&gt;But HTML is the perfect DSL for writing HTML—why replace it for another DSL with your own set of rules and restrictions, and lose the decades of tooling and know-how of every designer on the face of the planet?

Hiccup is useful because it represents HTML as a list in clojure.  You can map, filter, reduce, etc. over a hiccup snippet.  You can't do that with HTML.  Even if you have some kind of embedded template language, you tend to get old control structures like if and foreach vs. lists and functional transformations.  Yes, you lose the "designer" familiarity and tooling, but that's the trade-off.  

At any rate, it's a bit funny that in an article complaining about dynamic typing, you prefer to treat HTML as a giant string in your language rather than use a library that treats it as a native language type and makes a lot of guarantees about the correctness of it's output.

&gt; Every library function you call, unless you have it memorized, requires hunting down the source or test files for examples on how to use it. 

That's not a static vs. dynamic thing, that's a tooling thing.  Editors can offer completion, lookups, etc. for static or dynamic languages.  It tends to be easier to do (and more "always possible") with static languages.  Clojure, in particular, has optional typing that makes this a lot easier.

&gt;Unfortunately I have a suspicion that gradual typing won’t be enough. First, it requires a lot of energy to type all the libraries. Can the Clojure community be disciplined enough to add the required annotations in a majority of libraries? 

If there's enough interest in `core.typed`, yes absolutely.  How many large JS libraries have typescript definitions?  Nearly all of them.

&gt;Linters for dynamic languages are crippled to the point of being more about vanity and less about quality. 

Have you used ESLint? 

&gt;If your argument for using your linter is that it finds whitespace issues, I’m not sure that’s solving important problems.

Coding style matters, and it's hardly all "whitespace issues" (though those matter too.)  A good linter does significantly more for the readability of your code than a simple static type checker (of the java/c# variety.)

&gt;This is my bet: the age of dynamic languages is over. There will be no new successful ones. 

You're a uni student writing an idealistic piece about what you wish would happen.  This is not a practical, honest, and objective analysis of the current trends in software development.  It's not even an educated guess.  It's a prescription for what you think "ought to happen."

In the meantime, the top ten languages are primarily dynamic languages.  Three out of ten are static and two of those three offer optional dynamic typing.
As others have pointed out, this debate has been going on for years and that in itself really tells you all you need to know. Static and dynamic typing both have their trade-offs, and hence they both appeal to different people.

The main drawback of static typing is that you're required to prove what you're stating to the compiler. Anybody who has done proofs knows that stating something is always simpler than proving it. In fact, many things are very simple to state, but are notoriously difficult to prove. Fermat's last theorem is a famous example of this.

What's worse is that the proof generally ends up being baked right into the solution and the two become inseparable. This is why you see really convoluted code in typed languages as soon as the problem can't be easily expressed using a given type system. And it really doesn't take long before you hit that point.

Now that we have more code that's really hard to follow we start having trouble keeping track of it. Hence all of a sudden we get the problem of refactoring.

Despite static typing being around for decades, nobody has managed to show any conclusive evidence regarding its purported benefits. There are tons of projects written in both typing disciplines and absolutely no evidence that the ones written in statically typed languages have statistically less defects, faster delivery, or better maintenance stories. You'd think this would give the proponents a bit of a pause.

The thing is that a language is more than just a list of feature checkboxes to check off. There's no evidence that static typing is the defining feature that results in overall better code.

To find out what languages produce better result we need to look at software written in them. Treat the language as a black box instead of jumping to assumptions. Look at projects, say on GitHub, and then analyze time to deliver, numbers of defects, and so on. Only once you have those numbers can you start looking at why there are differences, if there are any.

In fact, one such study [exists](http://rayb.info/uploads/fse2014-lang_study.pdf). While the authors predictably conclude that static typing is better than dynamic, the data they provide says something different.

There's practically no distinction between functional languages, where Scala, Haskell, Erlang, and Clojure are all at the top. Meanwhile all the imperative languages trail behind.

So, the study supports the idea that functional style is in fact more effective than the imperative style. However, it fails to demonstrate that static typing has any impact on the quality of code written in functional languages.

More studies of this sort are needed, so that we can start using some empirical evidence to make decisions instead of arguing about our divergent personal experience.
Programming is just making and maintaining a set of guarantees. That's what you do when you write a set of pre and post conditions. Provided these guarantees are maintained and understood by all programmers, your code is guaranteed to work bug free. dynamic languages tend to throw this out the window without proper type systems. Instead, they have to find a way to compensate, usually through testing, which is not guaranteed to catch all errors, but at least it finds some.
Why are discussions like these still a thing? Some people prefer one thing, other people prefer another thing. At some point it becomes a waste of time and effort to try to justify why the thing you like is objectively better and the other is objectively worse.
Scale also matters IMO.  For small projects or little scripts, dynamic languages are much quicker.
I would never want to code in Python but there are in fact languages like Common Lisp which do give you some help (type inference, declarations, dead code warnings, ADTs, contracts etc.) which are a joy to use and where you don't pass untyped maps everywhere (so types matter). Python is like the Java of dynamic typing in this way, and CL is more like Haskell. I don't think that static wins so obviously, because Common Lisp has a lot of really cool stuff (like CLOS) which doesn't make sense in a static context.
Good riddance.
What you really demonstrated with this article isn't the failure of dynamic typing, but the absolute necessity of good documentation for a dynamically typed language.

Don't get me wrong; for many purposes I think statically typed languages are superior, and I *enjoy* programming in statically typed languages more; but the problem you're running into with Clojure is that your libraries are under documented for a dynamic language.

I've long maintained that a large part of the success of Python is based on the fantastic documentation for of the standard library, and just as importantly the strong culture of thorough documentation. It's expected, for instance that in a high quality Python module every function will have a well written docstring that allows users of the module to quickly determine the inputs and outputs of the function without having to refer to source code.

Static typing has other virtues that aren't so easily replaced (the advantages of statically typed linting are admittedly huge), but most of the uncertainty you're describing is a failure of documentation, not of the idea of dynamic typing.
I fucking love this post.   You put the concern in an easily digestible form.   Alas, more junior engineers might still be doubtful of your claims, and it might still take years of experience before they realize the frustrations you point out with dynamic, weak, or duck typed languages.
This article is spot on, but about 15 years out of date. If you replaced clojure with ruby and c# with java, the same article could have been written in 1998. 

Think of all the popular new languages that have come out since 2000, almost all are strongly typed. The only exceptions I can think of is coffeescript and clojure.  c#, groovy, scala, f#, go, dart, swift, rust, kotlin, typescript, purescript are all languages released after 2000, and are all strongly typed.

The key advantages dynamic languages had were faster compile times, instant feedback, ease of compiler implementation, more dynamic features. Also, the philosophy of dynamic languages (e.g. discarding hugely nested object orientated hierarchies, dynamic configuration) often had a larger impact on developer performance than the lack of strong typing. E.g. I was more productive with ruby on rails than I am with ASP.net^1, since the complexity of the ASP.net life cycle, and the structure of the apps was less productive than rails convention over configuration and simple MVC structure.

However since 2010 the typed languages have cancelled out these advantages, through either new language features or better API's. E.g you can now use ASP.net MVC, which is better than rails in many areas. Or consider typescript, where with a decent webpack/live reload/etc setup, you still get instant updates but with a statically typed language. Tools such as [NCrunch](https://www.ncrunch.net/)^2 give the instant feedback to c# as well. It isn't universal though - haskell still feels a bit primitive in these areas (last time I checked about a year ago).

So while new dynamic languages have already stopped becoming popular for the most part, I think the new change is most new frameworks will target strongly typed languages from now on. I also see typescript eventually come to dominate professional web development, as the advantages are too big to ignore (people talk about react vs angular, but I find javascript vs typescript to be more important).


^1 Not MVC, as stated below  
^2 Paid unfortunately, and fairly expensive at $159. Worth it though if you do a lot of c# dev.
Well, maybe there will be an era of new statically typed languages until people discover that those have downsides as well. Then the cycle starts again.

For me, the worth of a language is defined by their libraries and a language that does not provide a suitable equivalent to Pythons numpy/pandas/scipy is mostly worthless to me personally.

I know a few people who can get really excited about a new language and program in it just for the heck of it. Sometimes I envy them for this, but I usually loose interest very quickly when I see that I would have to re-implement a LOT of stuff on my own to make the language usable for my purposes. I once tried using Haskell to write numerical simulations of communications systems. It felt like I had to re-invent the wheel.

So yes, I give it a couple of years. Either the herd has moved on to the next hot language or there will be sufficient tooling and libraries available. Maybe there even is for Haskell already, it's been around for a while and people seem to stick with it.
Well, there's not a whole lot of heterogeneous map passing in clojure core. For the libraries where we find ourselves passing around such maps ideally we should be able to throw in a `macroexpand` to figure out exactly what's going on.

Look, I can empathise. I too was obsessed with pulling runtime processing into compile time. But there's an even more important time that was missing in all this - hammock time. Clojure isn't in the business of telling you how to solve problems, instead it lets you solve problems in the manner that seems clearest to you.
&gt; But HTML is the perfect DSL for writing HTML—why replace it for
&gt; another DSL with your own set of rules and restrictions, and lose the
&gt; decades of tooling and know-how of every designer on the face of
&gt; the planet?

HTML is not the perfect DSL for HTML.

HTML is similar to XML and suffers from this inflated data.

Why did yaml and json become popular? Because HTML is such a convoluted static mess.

The example of this clojure DSL is bad because there is no net gain.

I am autogenerating all HTML tags by using a language that allows me to do so, without having to care about the HTML language itself.

I simply use a DSL to describe any webpage - and this DSL autogenerates the HTML I need to use.
Static typing is a tool like any other. Sometimes it's useful sometimes it get's in the way.
I don't think they will die anytime soon, and it is because they're easier to grasp, I've tried to use Haskell for years, but is too difficult, I've been using Crystal for a few months and I had to deal to some hard bugs that I never had with Ruby. Take PHP for example, it is one of the most popular languages, and that's because you can have something functional with little effort.

I know a static language is more secure, but a dynamic one is easier, so they will have it's own place in the world.
&gt; Every library function you call, unless you have it memorized, requires hunting down the source or test files for examples on how to use it. Every map you get back requires a println to know what it contains.

"types as docs", it's not just for large projects! Using libraries makes your code part of large project, comprising your code and those libraries. With the same need for docs.

&gt; A red line appeared. It was a bug, caught by the compiler, never noticed until now.

This boast about type checking is not uncommon. But all (sufficiently large) programs contain bugs. If the bug hasn't made any difference, how important is it? Especially in a constantly-evolving project. Of course perfection is seductive, but has a price: attention taken from other tasks.

&gt; Unfortunately I have a suspicion that gradual typing won’t be enough. 

Agree. Despite being logical, it has not taken off in the past. A small minority love the idea, so they add the feature, and a few others use it and... that's it.
 
ITT: guy dislikes certain elements of certain languages due to a preference for certain styles, and poor documentation. Decides that the style he likes is the future on the basis of reasons.
Nah,

I worked with static languages for years before I ever even saw a dynamic language, and this cut and dry reasoning that static is always better is simply not true. Dynamic typing is occasionally useful when you are dealing with problems that are, themselves, uncertain. What's more is that static types are not a catch all, sometimes you are dealing with data which has a shape that can't be determined simply by a "type". Think of all the possible things a string could represent!

Dynamic programming will die about the same time as we develop economic fusion power.
Are types really good documentation?

Consider a type X with an optional variable Y. That leaves a ton of uncertainty on the table. Under what conditions is it optional? Why is it optional in these cases?

Or a function accepts a Date. Is that enough documentation? Does the function work or make sense with any Date? If the function takes two dates does the type system tell us anything about the relationship between the two dates?

And then there is the difficulty encoding information in types. Even with a domain as simple as dates, does the fact it's a date tell me about it's valid values? Does it mean I don't need to understand dates anymore? Does it somehow encode leap years information by telling me which years Feb 29th is valid etc?

IMO, a type system doesn't mean you don't need to look at doco, tests, etc in order to understand a piece of code. What it does is stop the kind of simple "oh I forgot that value was optional!", or getting params the wrong way around, etc as well as making refactoring a lot easier.
Huh? I like static typing, but:

- No one wants static types in a 20-line script.
- Uncertainty isn't exclusive to dynamic languages: weak typing (as in C) and null pointers can bite, too.
- What do DSLs have to do with dynamic languages again? Several statically typed languages (Crystal, Nim, Felix, Rust, ...) allow (and, to an extent, encourage) DSLs.
- The age of COBOL and FORTRAN "ended" over a decade ago. And yet there are still new developments in FORTRAN.

Really, it's all opinion. The percent of people using statically typed languages may be more than those using dynamically typed ones, but I doubt they will ever quite completely *die*.
Strong host language type system harms an ability to build strong DSL type systems (not prohibits entirely, but makes it harder). Typed DSLs with rich syntax are not "confusing" and are the most powerful thing ever. Therefore, sorry, but the gradual typing will win over any too strong fixed static system, and especially such a badly convoluted one as in Scala.
It is not about dynamic languages. It is about feedback, it is about whether you get feedback sooner or later.

* in Clojure lisp, you may get feedback on what you wrote when your tests are running (if your have written tests) or in your test environment or in your production environment when filled with customer data. For some issues no language would help, but for others it can.
* in Java you may get feedback on some things just when you are writing something in IDE that what you have written is wrong. And smart IDE (like IntelliJ IDEA) may even suggest you what you can write at certain places. That is a boon to productivity. On the other hand Java is quite verbose, limited in abstraction and in what can be composed.
* in Haskell you can abstract over more things than in Java, you can compose more things because functional interface leads to more composable program components, so programs can be shorter, but reading programs is hard because when you see an identifier in source code, you don't know its type (unless it is explicitly annotated which would clutter code too much if used excessively). Again IDE can help and give you right feedback, but Haskell lacks as good IDE as are there for Java now. Compiler may help too, it can preprocess code to add type annotations everywhere, but you get cluttered code back.

I can imagine language that would be dynamic yet some things can be inferred about it so that IDE could provide useful feedback.
Don't tell me that in JavaScript nothing can be inferred about function(x) { return x+'a'; } It is pretty clear what it does and what happens when it is used with numeric or string arguments. It requires kind of inspection that is beyond what current IDEs or compilers do. On the other hand when that function would call eval (or implement kind of eval or doing something where information about result require too long computation or interaction with external systems) it would be less certain, but in lot of common cases it is clear enough. Some kind of uncertainty can be handled without significant impact on provided feedback.
&gt; ...Scala...Haskell...Ruby...Clojure...

Go and try APL and Mathematica and reconsider.

I would almost agree. However, hot code reloading is real...and implies a certain level of dynamicism.
Same article different day. Every language has it's use and all have their pros and cons.
I'm going to point at Python [PEP 484](https://www.python.org/dev/peps/pep-0484/). Python is absolutely not a static language and very much a dynamic language through and through. However what they did is extend what is considered commenting to include a standard form of documentation that is machine readable. Static Analysis tools can then read the code and logically reason about it in attempt to discover type errors. The practical upshot is that it stems the onset of incompatible Python dialects that would have come along.

I'm extrapolating that free-form commenting is what is ending, and a machine readable form of comment for static analysis is what will become the norm.
I tried learning clojure through the [koans](http://clojurekoans.com/). 

Then found out that they don't like enums and use [keywords](http://stackoverflow.com/questions/11856077/enums-and-clojure) instead. 

After some searching, I didn't find how they make sure compile-time that all enumeration values are checked. 

Didn't find anything useful, lost interest. I probably can make some macro for that, I just was not in mood for doing something that C compiler can do for probably decades.
&gt; This is my bet: the age of dynamic languages is over. There will be no new successful ones.

I'm working on one that will redefine how you're thinking about programming languages. And how they are being developed.

Here's my bet: Popularity of the tool is disconnected from it's quality because too many people in computing can't find their bottoms. Probably because very few people in general populace can. In that context being successful may mean that you're being declaring dynamic languages won't be Justin Biebers of the languages. In that kin I might even agree if it mattered anything.

If you find the last bet offensive, here's an another: Arguing over dynamic&amp;static typing has been arguing over something greatly irrelevant for a very long time.

Dynamically typed programs can become statically typed when the context requires it. Likewise statically typed programs can become dynamically typed.

Whether your program is clearly notated matters more than anything described here. The clear notation starts with removing everything that is not required for understanding what is being described, rather than mandating that some properties are always required to be present.

&gt; This uncertainty wrecks productivity. Every library function you call, unless you have it memorized, requires hunting down the source or test files for examples on how to use it.

&gt; uncertainty is perhaps worse than all the others. You can resolve all the others by sitting down and learning the thing. But how does one resolve uncertainty? Only with more certainty. But what if the language does not provide a way to make certain of the uncertain?

Here we see that uncertainty and lack of understanding is the problem author is having with his tools. How to write stuff that is well-understood is a long-term big problem of the literature everywhere.

&gt; We begin by adding types, by restricting the space of possibilities to free ourselves from the self-made burden of uncertainty. New languages like Elm and Crystal are on the right track, and of course established ones like Haskell and Scala. We need more languages like these.

Adding types does not always restrict space of possibilities. Sometimes very clever typing extends the possibilities. Additionally restricting space of possibilities doesn't always help to uncertainty. Very simple and restricted systems can produce behavior that exceeds your understanding.

Additionally understability in literature is noticed to be relative to what you already understand well. Possibly one explanation is not sufficient for explaining it to everyone in context where it becomes useful.
Dynamic languages are easy for newbs to get started on. There will always be more newbs than pros. I don't think dynamic languages will die.
One thing I've noticed is that programmers who prefer statically typed languages are consistently more intelligent than those who prefer dynamic languages.

maybe he should try Python
Clojure tooling is bad. Conclusion: dynamic languages are bad.

I think you jumped the gun a bit.

EDIT: explain why I'm wrong instead of downvoting please.
Beautiful writing.
Is not secret that statically type languages are the darlings of r/programming so is not worthy to put a arguments against it. All I am gonna say is that IMHO the choosing of both models are just preferential to further say emotional. And there is not really any evidence that prove statically typed language improve software quality nor workflow.
Sorry, as someone who works in research it is absolutely vital to the prototyping process NOT to have tight coupling and everything decided at compile time. I can develop new functionality in about 1/10 the time (literally) as many of my coworkers because of this. Maybe I run into an extra bug once in a while--I have plenty of free time to fix it while they are still developing the first iteration (or refactoring the universe because now they need a double instead of a integer).

If you need to prove your code correct, type systems can be nice. But almost nobody *needs* to prove their code correct. 

&gt; It shows that people are hungry for certainty.

People are also hungry for sugar, fat and TV but that doesn't make it good for them.

I call this the "epistomology problem". A system that tries to know all the details about everything it handles will completely fail to handle new things well, because it doesn't understand them. Conversely, if you only try to know the minimum you need, you can handle a much wider array of new things.

Less is more. Only use as much type system as you really need, not as much as you "hunger" for.

edit tl;dr: Certainty is the enemy of flexibility.
&gt; 5 to 10 years later a new post on Reddit: **The end of dynamic languages**
there's a happy medium. you want people to have the flexibility to code around past mistakes. Haskell is a great example of imperfect bondage...a great example of how the strict imposition of a broken type system can paint you in to a corner...what's worse is that Haskell uses non-language hacks like OverloadedStrings to get itself out of these corners
&gt; Because Haskell is a strongly-typed language, the linter knows a lot more about your program than just syntax. It can find structural problems too, like:

&gt; * When two equivalent anonymous functions that can be extracted out
&gt; * When a library method already exists for an expression
&gt; * When you fail to match every possible result in a case statement

The first two of these could be detected in dynamic languages too.
What we have here kids is a failure to understand that other people have different needs, workflows, and preferences; and that is *ok*. 
Only tangible upside of using a statically typed language I've ever experienced was that my linter would tell me if the type of a function argument was wrong. Not much of a killing blow, don't you think?
I think a good test of a statically typed language is how easily you can implement something like [Clojure's `complement`:](https://clojuredocs.org/clojure.core/complement)

&gt;Takes a fn f and returns a fn that takes the same arguments as f,
has the same effects, if any, and returns the opposite truth value.

This is a good example of something that's commonly desirable but poorly supported outside of dynamically typed languages: augmenting an arbitrary function.

In Clojure, JavaScript (!!) and others, this is completely natural. Python gives you decorators. C# gives you attributes and asks you to set up post-compilation. The Haskell community has to say "don't do that".

This isn't unfixable, but it requires yet another special case. Statically typed languages are slowly evolving to the point where they can do everything dynamically typed languages can, but they're becoming more and more complex in the process, which comes with a cost to usability.

There are some really cool things that you can't do without a static type system, but they'll never completely shed their awkward parts either. To me it seems there will always be room for both flavours of language.
Might be a good article but I stopped reading at "But anything else is impossibly difficult."
Sorry.
Thanks for taking the time to post this.  I've been considering making a webgl game and this has been helpful.  

While I do agree a web page format would be easier to use, I feel you are getting way too many negative comments in this thread.  You took the time to share something in front of the class and are getting jeered by the peanut gallery while the majority probably find this interesting but stay quiet.

Keep up the good work!
Exported to a Google Doc as per peoples request, not too pretty though.

https://docs.google.com/document/d/135RMvVDO08nITxEq4BkUPYuFjOCHVuAi6yVbRifZF9M/edit?usp=sharing
&gt; In WebGL, you must send the shape to the GPU, and destroy it later to stop it from appearing any more.

No, that is not how it works. There are many similar misconceptions in the slides.
Interesting but honestly, the presentation format put me off. A short article would have been better.
I liked the presentation, but honestly I expected more related to the construction of your specific game.

Something specific to how your player actors move, perhaps with a data sequence diagram to give an idea of server-to-server acknowledgment or that sort of thing.

Perhaps maybe a preview of how the syncrhonization happens when a player hits a spike thing (I just played your game for a few minutes, reminds me of Osium, will play layer though). 

Perhaps a follow up on what happens when bad actors play the game, how the server detects sync errors, etc.

Awesome concept game though.. keep it up.
On page 15 the data doesn't really add up for the sizes. The Pure numbers JSON uses 1/2 Digit numbers presentable in 5bits. But then when you use the binary you chose 8bit(0-255), 16bit(0-65535), and 32bit(0-4294967295). If you chose these values in JSON it would need 30bytes of data. 

So the size of the JSON should be 9-30bytes or the size of the binary data should be 4 bytes.
Thanks for putting that together, great read. 
My ping is displayed as being ~16ms, and framerate holds at 30, but the game plays as if it's under consistent lag; There's very noticeable stuttering in player movement.

[Agar.io](http://agar.io/), on the other hand, runs very smoothly, and at a higher framerate (60). I'm guessing it's because they implement some basic movement interpolation, which maybe you don't ... ?

Anyway, neat project.
Please make this available as a downloadable text or an online article
&gt; Removing an element from a quadtree can take almost no time, up to a few milliseconds as the entire branch gets rebalanced.

Removal should be nearly as fast as doing a lookup. In most cases, it's just removing a node from a single small bucket.

If the the bucket gets empty, you may need to coalesce other buckets along the path to the node you're removing, but that usually bails out early and even at the worst case it's O(log n) in the number of nodes in the tree.

Having said that, yes, I would definitely use a grid first, though.
What level of education do you have, I am thinking about attempting
a multiplayer game for a senior capstone in high school, but I may be biting off more than I can chew

I found the part about iterating a map of players interesting. Were you just using a standard Js object as a "map" or were you using the ES6 Map type? In my game I am using an actual ES6 Map and when I do reads I just iterate over players.values() then I can use the player Ids to do updates when needed. Not sure if I will need to optimize this latter or not.
It sounds like you reinvented the wheel in go. Since you were always reading on one end and writing to the other end you should have just used buffered channels. It might simplify a lot.
[deleted]
For the array vs map bit, you might find writing your own linked hash map has better performance. The built in types aren't always ideal data structures, particularly with games.
[deleted]
&gt; It’s not like people just woke up to copyright infringement. Yes, it happens — but not like this.

Uhm... Let's face it: a lot of household names on the web got big initially with the help of copyright infringement. Think YouTube. YouTube walked a very thin line to get on the right path and they mostly managed thanks to Google's deep pockets.

&gt; Can’t think of a reason to have a box like this next to the video itself unless you know copyright infringement is part of your business plan.

You're right. I cannot think of a single reputable site that has a [button](http://i.imgur.com/OyYpyVo.png) to [report](http://i.imgur.com/keONYLa.png) copyright [violations](http://i.imgur.com/nubnzNf.png) next to their videos.

&gt; The stalling is galling. A physical signature. A request for identification of the work even though they themselves included a link to it.

This is the DMCA. Nothing to do with Udemy. Udemy is following the necessary legal procedure for dealing with reported copyright infringement.

Yes, the whole DMCA takedown process sucks and is greatly in need of an overhaul, but Udemy is doing a good job in making sure that they're actually sticking to it. Protects them, and protects rights holders.


---


I get that you're angry about the situation. But you're lashing out at Udemy, who, as far as I can tell, has not really done anything wrong. They cannot and should not be held responsible for vetting every single piece of content that people upload - that's getting into SOPA/PIPA/etc territory.
I am wondering how do other training sites like Lynda, Pluralsight or Tutsplus handle this problem.
I see a lot of comments here about how this isn't Udemy's responsibility. Which could be technically/legally (if not _ethically_) true.

It is true that the OP in this case has a contract with a company that could charge its customers a higher price in order to fund more agressive anti-priracy activity, and/or the OP could spend more of their valuable individual time cruising all the unethical parts of the Internet to do their own DMCA takedowns.

As a content creator myself, who has had all my content pirated over the past 20 years, I know the frustration. And I know that there's no easy answer. An individual could spend far more time fighting the evil people than creating new content - which is obviously counter-productive.

My personal approach has been to (correctly) assume that _most_ people are basically decent, and that those people won't _knowingly_ deprivate the content creator of their proper compensation. Especially in the educated world, people aren't dumb enough to think that content creators spend hundreds/thousands of hours creating this content for free - not when we need to pay bills, eat, put our kids through college, etc. just like everyone else.

So I haven't used DRM for example, because that punishes honest people and does virtually nothing to stop criminals from pirating/consuming my work.

Nor have I used the DMCA (a poor law at best) to convince criminals to stop their criminal activity. They are _criminals_, what do they care about the law? Or even if they are legally protected (like Udemy), they are clearly unethical and so _still_ don't really care about the law - they'll game the law for their benefit, and to the harm of content creators - and so they are, at best, complicit in the criminal behavior.

Instead, I do two things. I keep producing content that honest people pay for, and which criminals steal. And when I encounter a shady place like Udemy that enables otherwise honest people to _unknowingly_ consume my content without proper compensation I use social media and other venues to educate those honest people about their mistake.

Ultimately the criminals and unethical players are bad guys. They are irredemable. If they are religious I'm sure they'll all go to their personal hells or whatever. There's nothing anyone can do for them.

My job, as content creator trying to make a living, is to make the lives of _honest_ people as good as possible, and to help honest people avoid being suckered into supporting criminal/unethical enterprises by educating them and redirecting them to above-board sources for the content where the content creator is properly compensated for their work.
Here's an idea: Udemy should force uploaders to also upload a verification video. The same person in the course video has to appear in a video saying it's licensed to appear on Udemy.

Simple system. Dead easy verification.

But they won't rise to that challenge, because they're fucking criminals. They want stolen content. They want to shift responsibility. They're fucking parasites.
Where can I find a link to the legit course? It looks interesting.
How the hell does this relate to programming? None at all. It is a copyright civil dispute about howto/training videos.
It's funny how everyone on Reddit is pro-piracy when it's something they themselves would pirate (movies, music), but when it's something they wouldn't pirate (courses) it's shameful.
Ok I get it, Udemy does not review all of the submitted courses and verify that the actual owner is uploading them, they don't seem to claim to.

That being said, what else should they be doing here? They followed all laws and pulled the course in &lt; 12hrs on a holiday weekend. Anyone have a reasonable, cost effective way a company could do more?
Not trying to be snarky, but what is stopping content creators from just uploading their stuff to Udemy first? Not just udemy, but any similar service. If we are all aware that piracy happens and are aware that there isn't any way of stopping the flow of information then why even try to fight it? This scenario isn't much different than the one movie studios are in.
Can someone explain me what the problem is? Isn't that exactly what youtube does (automated reporting)?

That "physical signature" stuff is shady, thought, but I would not expect a site like this to do anything more than an automated process for takedown.

I am slightly at loss about what the author expects. Does he believes that platforms should check for the copyright source of every upload?
This sucks for OP but they come off as a petulant child here. Shit happens, people will try to steal your videos. Take the proper steps by being vigilant, and quiet down. The world isn't out to get you and your content, it's just the way the world works.

If it was that easy of a problem to solve, any type of piracy wouldn't exist anymore.
Throw a bunch of lawyers at them and ask for compensation, its the only way to deal with this kind of shit.
Its not Udemy's fault this guy doesn't know how to write a DMCA takedown notice. I did that shit in college when somebody stole pics off my facebook.
How I solved Part 1: Copy the text into notepad, replace ( with +1, replace ) with -1. In Chrome open javascript console and put the text from notepad into variable s.

    s="+1+1+1+1+1-1+1+1 . . ."
    eval(s)
    &gt; 74

Part 2: Use the same variable s. Now this will find the position where it evaluates to -1.

    for (i=2; i&lt;=s.length; i+=2) if (eval(s.substr(0,i))==-1) {console.log(i/2);break}
Brainfuck (16bit cells):

    +[&gt;&gt;,&lt;+&gt;----------------------------------------&gt;+&lt;[&lt;&lt; - &gt;&gt;-&gt;-&lt;]&gt;[-&lt;&lt;&lt;+&gt;&gt;&gt;]&lt;&lt;&lt;]
    &gt;[&gt;&gt;+&gt;+&lt;&lt;&lt;-]&gt;&gt;&gt;[&lt;&lt;&lt;+&gt;&gt;&gt;-]&lt;&lt;+&gt;[&lt;-&gt;[&gt;++++++++++&lt;[-&gt;-[&gt;+&gt;&gt;]&gt;[+[-&lt;+&gt;]&gt;+&gt;&gt;]&lt;&lt;&lt;&lt;&lt;]&gt;[-]++++++++[&lt;++++++&gt;-]&gt;[&lt;&lt;+&gt;&gt;-]&gt;[&lt;&lt;+&gt;&gt;-]&lt;&lt;]&gt;]&lt;[-&gt;&gt;++++++++[&lt;++++++&gt;-]]&lt;[.[-]&lt;]&lt;

With comments: https://github.com/maciekmm/adventofcode/blob/master/day-1/basement.bf
First 
Ctrl-F (
minus
Ctrl-F) 

Second
var z = 1;
for( var i=0;i&lt;a.length;i++)
{
    z +=(a.charAt(i)=='(' ? 1 : -1);
    if( z == -1 )
    {
        console.log('First is at '+i+' '+z);
        break;
    }
}
I'm trying to learn Pyth, a code golfing language based on python.

It took some time, and this can probably be golfed down a bit, but here's my solution for the first one (14 characters):

    -lhKczxSz\)leK

Try it [here](https://pyth.herokuapp.com/) by adding the code to the top window and all the parentheses to the bottom window.

Will edit this later for an explanation of the code.

Edit: Added some comments:

    -lhKczxSz\)leK
           S		Sort
            z		Input
          x  		Index of
             \)		Literal ")"  (So far we have the index of the first ")" after sorting) 
        cz			Chop input at the index we found. We now have a two element array with all the )'s and ('s
       K			Save it in the variable K
      h				First element
     l				Length
                eK  	Last element (of K)
               l    	Length
    -				subtract (the length of the last element from the length of the first element)
Nice touch to give different input for each user.
Quick one liner JS function for Part 1.

```
function calc(s) { return s.split('').reduce((acc, e) =&gt; acc + (e == '(' ? 1 : -1), 0); };
```
Second part in Scala:

    inputStr.scanLeft(0)((a, b) =&gt; if(b.equals("(")) a+1 else a-1).takeWhile(_ != -1).length
I don't like having to link an account from another website to be able to play.
Day 1, part 1:

Open MS Word. Copy the input. CTRL + F for values "(" and ")". Subtract total of "(" input from ")" input = answer.

:o lol
Day 1 Part 1 - JS right in the console (open up dev tools on the input page)

    var inputText = $('pre').innerText, floor = 0;
    for(var i = 0, len = inputText.length; i &lt; len; i++) {
       if(inputText[i] === '(') {
          floor++;
       } else if(inputText[i] === ')') {
          floor--;
       }
    }
    
    console.log(floor);
    #/bin/sh
    sed -e 's/(/1+/g' -e 's/)/-1+/g' -e 's/+$//g' | bc
Less formalism:

    a = $('pre').textContent; (a.match(/\(/g).length * 2) - a.length
Java part 1:

    public class SantaTest1 {
        public static void main(String [] args) {
            int floor = 0;
            for (char c : args[0].toCharArray()) {
                if (c == '(') 
                    floor++;
                else if (c == ')')
                    floor--;
            }
            System.out.println(String.format("floor: %d", floor));
       }
    }

And part 2:

    public class SantaTest2 {
        public static void main(String [] args) {
            int floor = 0, index = 0;
            char [] chars = args[0].toCharArray();
            for (int i=0; i &lt; chars.length; i++) {
                if (chars[i] == '(') 
                     floor++;
                else if (chars[i] == ')')
                    floor--;
                if (floor == -1) {
                    index = i;
                    break;
                }
            }
            System.out.println(String.format("floor: %d, %d", floor, index+1));
        }
    }


Hey. I decided to make a sub to share solutions: r/adventofcode/
This looks really fun. I figure I'd post my solutions to the first problem to get the discussion rolling

    findFloor :: String -&gt; Int
    findFloor "" = 0
    findFloor ('(':ss) = 1 + findFloor ss
    findFloor (')':ss) = (-1) + findFloor ss
    findFloor (_:ss) = findFloor ss
    
    basement :: String -&gt; Int
    basement s = f s 0 0
      where f _ (-1) i = i
            f "" _ i = i
            f (x:xs) n i = case x of '(' -&gt; f xs (n+1) (i+1)
                                     ')' -&gt; f xs (n-1) (i+1)
                                     _ -&gt; f xs n (i+1)

This is probably not the best way to do it but it's pretty straight foward.

edit. I realise a simple fold would be better suited here

    findFloor :: String -&gt; Int
    findFloor = foldl (\a b -&gt; case b of '(' -&gt; (a + 1)
                                         ')' -&gt; (a - 1)) 0
Solved part 1 like this with bash:

    $ expr (grep -o -w "(" lel | wc -w) - (grep -o -w ")" lel | wc -w)


D

    auto result = args[1].dup.map!(a =&gt; a == '(' ? 1 : -1).reduce!("a + b");
    sum ({$[x=")";-1;1]}each a)
in q / KDB
How knowledgeable in coding do you have to be to try solving these? I am currently learning Python and Java but I feel I am still very much at a beginner level...
I'd definitely try this if I didn't have finals coming up. 

Also anyone who likes these challenges should try /r/dailyprogrammer
I like the site design and I always appreciate more practice problems. Solution below.

Pretty straightforward using the JS console: 

    var lisp = document.querySelector('pre').innerHTML;
    var count = 0;
    for (var i = 0; i &lt; lisp.split('').length; i++) { 
        if (lisp.split('')[i] == '(') { count++; } 
        else { count--; } 
        if (count == -1) { console.log(i + 1); } // part 2, take first instance
    }
Here is some Elixir Code. Still in the beginner stage though.

    defmodule Advent1 do
      def part1("(" &lt;&gt; rest, count) do
        part1(rest, count + 1)
      end

      def part1(")"&lt;&gt; rest, count) do
        part1(rest, count - 1)
      end

      def part1("", count), do: count

      def part2(_, -1, pos), do: pos - 1

      def part2("(" &lt;&gt; rest, count, pos) do
        part2(rest, count + 1, pos + 1)
      end

      def part2(")" &lt;&gt; rest, count, pos) do
        part2(rest, count - 1, pos + 1)
      end
    end

    
My C# solution for day 1.

    public static int Solve(string input)
    {
        return input
            .ToCharArray()
            .Select(x =&gt; x == '(' ? 1 : -1)
            .Sum();
    }

Made a [GitHub repository.](https://github.com/tucker87/AdventChallenge2015)
Answers in Excel VBA:

Part 1:

	Public Function moveFloors(startFloor As Integer, sequence As String) As Integer
		Dim cur As Integer
		Dim steps As Integer
		Dim c As String
		
		cur = startFloor
		steps = Len(sequence)
		
		For i = 1 To steps
			c = Mid(sequence, i, 1)
			Select Case c
				Case "("
					cur = cur + 1
				Case ")"
					cur = cur - 1
				Case Else
					cur = cur
			End Select
		Next i
		moveFloors = cur
		
	End Function

+ Input parameters are:
    + startFloor (Integer) the floor to start on
    + sequence (String) - the character sequence
+ Return value
    + final floor (Integer)

----

Part 2:

	Public Function getFirstCharBasement(startFloor As Integer, sequence As String) As Integer
		Dim cur As Integer
		Dim steps As Integer
		Dim c As String
		Dim basement As Integer
		
		
		cur = startFloor
		steps = Len(sequence)
		
		For i = 1 To steps
			c = Mid(sequence, i, 1)
			Select Case c
				Case "("
					cur = cur + 1
				Case ")"
					cur = cur - 1
				Case Else
					cur = cur
			End Select
			If cur = -1 Then
				basement = i
				Exit For
			End If
		Next i
		getFirstCharBasement = basement
	End Function

+ Input Parameters same as for Part 1
+ Output Parameter: Position of first char to reach floor -1

For Part 1 I didn't even use any code. I just used atom.io to count the "(" and ")" in the string and substracted. 
Second half i've done in js, because it was the first thing available: 

    var text = "&lt;very long string&gt;"
    var floor = 0;
    for(var i = 0; i &lt; text.length; i++) {
        if( text.charAt(i) == "(") {
            floor++;
        } else {
            floor--;
        }
        if (floor == -1) {
            console.log(i);
            break;
        }
    };

after that I had to fix the of by one error, because text.charAt(0) returns the first character. 
I've made a github repo and a page for solutions:

- https://lab.xpaw.me/adventofcode/
- https://github.com/xPaw/adventofcode-solutions
This is brilliant.  Done my two today!
Part 2:

    [&lt;EntryPoint&gt;]
    let main argv = 
        let basement_count = 
          System.Console.ReadLine() 
            |&gt; Seq.scan (fun fl ch -&gt; if ch='(' then fl+1 else fl-1) 0 
            |&gt; Seq.takeWhile (fun fl -&gt; fl&gt;=0) 
            |&gt; Seq.length 
        printf "Steps to basement: %d" basement_count
        0 

Initial state goes into result of Seq.scan, so adding 1 to Seq.length is not needed.
Crystal :-)

    input = "(((())))("

    floor = 0
    input.each_char do |char|
      case char
      when '('
        floor += 1
      when ')'
        floor -= 1
      end
    end
    puts floor

Both parts of day one:

&amp;nbsp;

    function adventDayOne (str) {
        var count = 0,
            posit = [];
            
        for (var i = 0; i &lt; str.length; i++) {
            if ((count += str[i] == '(' ? 1 : -1) == -1) {posit.push(i+1)}
        }
        console.log({level:count,basement:posit[0]}); 
    }
My simple solutions in Python 3.4, no error handling though.

https://gist.github.com/ribolzisalvador/382cd4e34cd6c7777682
My solution: http://codepen.io/venerated/pen/MKgYvX

JavaScript n00b, so forgive me if it could be way better.
If anyone is interested going to be putting all my advent of code answer written in java on my github www.github.com/robertwildman/Advent-of-Code
Rust solution, without file reading stuff:

    use std::path::Path;

    fn which_floor_at_the_end() -&gt; Option&lt;i32&gt; {
        read_input_file(Path::new("input"))
            .map(|s| s.chars()
                      .map(|x| if x == '(' { 1 } else { -1 })
                      .fold(0, |acc, x| acc + x))
    }
    fn main() {
        println!("{}", which_floor_at_the_end().unwrap());
    }

I first wrote imperative version, but then tried to implement functional one based on some comments.
Very cool.

Blogged my solutions to day 1 using Clojure.

http://charliegriefer.github.io/2015/12/01/adventofcode-001/
I did mine in C#

Part 1:

    int floor = 0;
    char[] parenInput = File.ReadAllText(input).ToCharArray();
    foreach(char paren in parenInput)
    {
        if (paren == '(')
            floor++;
        else if (paren == ')')
            floor--;
    }
     return floor;
    
Part 2:

    int floor = 0;
    char[] parenInput = File.ReadAllText(input).ToCharArray();
    for(int i = 0; i &lt; parenInput.Length; i++)
    {
        if (parenInput[i] == '(')
            floor++;
        else if (parenInput[i] == ')')
            floor--;
        if (floor == -1)
            return i + 1;
    }
     return -1;
Part 2 in R

    which(cumsum(match(strsplit(scan("input.txt","character"),"")[[1]],c(")","","("))-2)==-1)[1]
For part2, paste text into emacs, prepend a left paren, and let it match that paren.
Haskell

    import Data.Char (ord)
    f = sum . map (negate . pred . (*2) . subtract 40 . ord)
    main = print $ f "()((()())()())))"

I've been learning haskell for a few weeks, my take on the first puzzle:
    
    input = --place your input here 
    
    counter :: String -&gt; Char -&gt; Int 
    counter str c = length $ filter (==c) str 
    
    output :: Int 
    output = (counter input '(') - (counter input ')') 



EDIT:
   output = 
		let replace = map (\c -&gt; if c=='(' then 1 else -1 )
		in sum (replace input)    

In Elixir using doctests, using this site to play with a new language. Did each part as an individual solution to beat Elixir into my mindset.
    
    defmodule AdventOfCode.Day1 do
      @doc ~S"""
        Part 1
    
        Figures out which floor Santa needs to visit based on given `str`
    
        `str` contains two chars, both have meaning:
        "(" means add one
        ")" means minus one
    
        This method will figure out the `floor` when given a stream of `(` and `)`
    
        ## Examples
    
            iex&gt; AdventOfCode.Day1.floor("(())")
            0
    
            iex&gt; AdventOfCode.Day1.floor("()()")
            0
    
            iex&gt; AdventOfCode.Day1.floor("(((")
            3
    
            iex&gt; AdventOfCode.Day1.floor("(()(()(")
            3
    
            iex&gt; AdventOfCode.Day1.floor("())")
            -1
    
            iex&gt; AdventOfCode.Day1.floor("))(")
            -1
    
            iex&gt; AdventOfCode.Day1.floor(")))")
            -3
    
            iex&gt; AdventOfCode.Day1.floor(")())())")
            -3
    
      """
      def floor(str) do
        str
        |&gt; String.length
        |&gt; with_negatives(str)
        |&gt; compute_floor
      end
    
      @doc ~S"""
        Part 2
    
        Now, given the same instructions, find the position of the first character
        that causes him to enter the basement (floor -1). The first character in
        the instructions has position 1, the second character has position 2, and so on.
    
        ## Examples
    
           iex&gt; AdventOfCode.Day1.position(")")
           1
    
           iex&gt; AdventOfCode.Day1.position("()())")
           5
    
    
      """
      def position(str) do
        on_position(str, 0, 0)
      end
    
      defp on_position(_str, position, -1) do
        position
      end
    
      defp on_position(&lt;&lt;"(", rest::binary&gt;&gt;, position, total) do
        on_position(rest, position + 1, total + 1)
      end
    
      defp on_position(&lt;&lt;")", rest::binary&gt;&gt;, position, total) do
        on_position(rest, position + 1, total - 1)
      end
    
      defp with_negatives(total_length, str) do
        negatives_length =
          str
          |&gt; String.replace("(", "")
          |&gt; String.length
    
        {total_length, negatives_length}
      end
    
      defp compute_floor({total_length, total_negatives}) do
        (total_length - total_negatives) - total_negatives
      end
    end
  
[edited] added part 2
My intial try for the second part in plain old Java

    public class Main {
        public static void main(String[] args) {
            String floors = "&lt;input from website&gt;";
            char[] charArr = floors.toCharArray();
            int floor = 0, index = 0;
            while (floor != -1) floor += ("(".equals("" + charArr[index++]) ? 1 : -1);
            System.out.println("Santa reaches basement at position " + index);
        }
    }
Guys! You can access the string with array notation (read-only) in JS, no need to '.split()' the whole thing. Mine was a recursive take on the first problem:

    var input = "((...))()";

    //Object used to map string input to -1 or 1 depending on which character is used.
    var map = {
        "(": 1,
        ")": -1
    }

    //A solution to 1-1, recursive.
    function HelpSanta(string, index, charaMap) {
        if (string[index+1] != null) {
            return charaMap[string[index]] + HelpSanta(string, ++index, charaMap)
        } else {
            return charaMap[string[index]]
        }
    }

    //Calculate solution to the first problem:
    var output1 = document.createElement("p")
    output1.innerText = HelpSanta(input, 0, map).toString();

Also, these regex solutions are amazing.

Second problem: I couldn't figure out how to do it recursively, since you kinda have to keep some sort of state to check a condition at each step, which the first function didn't do:

	//A solution to 1-2, iterative. 
	//If there's a way to evaluate each step of a recursive function for the "stop" condition I'll be glad to hear it!
	function TheseDirectionsAreSilly(string, charaMap) {
		var value = 0;
		var index = 0;
		while (value &gt; -1) {
			value += charaMap[string[index++]];
		}
		return index;
	}

	//Calculate solution to the second problem:
	var output2 = document.createElement("p")
	output2.innerText = TheseDirectionsAreSilly(input, map).toString();

Looking forward to tomorrow's! I'll probably have to batch some up though and work through what I can. Overtime sucks.
In R:
#part 1

    elevDrcs&lt;-readChar("location\day_one_input.txt", file.info("location\day_one_input.txt")$size)

    floor &lt;- 0
    for(i in 1:nchar(elevDrcs)) {
      char &lt;- substr(elevDrcs, i, i)
      if(char == "(") {
        floor &lt;- floor + 1
      } else if(char == ")") {
    floor &lt;- floor - 1
      } 
    }
    print(floor)
# part 2
    floor &lt;- 0
    i &lt;- 1
    while(floor &gt;= 0) {
      char &lt;- substr(elevDrcs, i, i)
      if(char == "(") {
        floor &lt;- floor + 1
      } else if(char == ")") {
        floor &lt;- floor - 1
      }
      i &lt;- i + 1
    }
    print(i - 1)
PHP:

    $a='(()()()';
    for(;$a[$i];){$a[$i++]&lt;')'?$p++:$p--;$f?:$p&gt;-1?:$f=$i;}echo$p.' '.$f;

produces a couple of notices but runs properly, no error checking too
Recursive for part 2 returns naturally with the answer.

    var input = "((((()(()(((((((()...etc";

    console.log(process(input.split(''), 0));

    function process(input, depth) {
        var c = input.shift();
        return c === "(" ? 1 + process(input, depth+1) : c === ")" ? (depth == -1 ? 0 : 1 + process(input, depth-1)) : 0;
    }
Day 2:

    let rec input() = 
      seq {
        let line = System.Console.ReadLine()
        if line &lt;&gt; null then
          yield line
          yield! input()
      }

    let parseLine (line:string) = 
      match line.Split('x') |&gt; Array.map System.Int64.Parse with
      |[|l; w; h|] -&gt; (l,w,h)
      |_ -&gt; raise &lt;| new System.Exception("Wrong input format")

    let paperArea (l,w,h) =
      let sides = [l*w; w*h; l*h]
      (List.sumBy ((*)2L) sides) + (List.min sides)

    [&lt;EntryPoint&gt;]
    let main argv = 
      let result = input() |&gt; Seq.map parseLine |&gt; Seq.sumBy paperArea
      printf "Total: %d" result
      0

&gt;Java solutions:

https://github.com/randreucetti/advent-code
Too bad my work's web filter is blocking it today.

Threat Type: spam 
Threat Reason: Domain has unusually high traffic volume for a very recent registration. 
Website source has a message to users :P
no need to write code, just use terminal for p1:

    cat input.txt | grep '(' -o | wc -l
    cat input.txt | grep ')' -o | wc -l
A day late to the party but here's some Haskell

    -- Part 1
    parensToInts :: String -&gt; [Int]
    parensToInts = map (\p -&gt; if p == '(' then 1 else -1)   

    whatFloor :: String -&gt; Int
    whatFloor = sum . parensToInts  

    -- Part 2
    whatPosition :: String -&gt; Int
    whatPosition = length . takeWhile (&gt;= 0) . scanl (+) 0 . parensToInts

C# 

    void Main()
    {
    	string input = "()()(()()()(()()((...";	
    	int floor, position;
    	getFloorPosition(input, out floor, out position);
    	floor.Dump();
    	position.Dump();
    }
    
    // Define other methods and classes here
    public void getFloorPosition(string inp, out int floor, out int position)
    {
    	floor = 0;
    	position = 0;
    	for(var i = 0; i &lt; inp.Length; i++)
    	{
    		if(inp[i] == '(')
    			floor++;
    		else
    			floor--;
    		if(floor == -1 &amp;&amp; position == 0)
    			position = i + 1;
    	}
    }

After seeing /u/tucker87 answer, I decided to shorten my own answer

    int floor = 0;
    int position = 0;
    
    void Main()
    {
    	var input = "()()(()()()....".ToCharArray().ToList();
    	position = input.FindIndex(FindPosition);
    	floor = input.Select(x =&gt; x == '(' ? 1 : -1).Sum();
    	floor.Dump();
    	(position + 1).Dump();
    }
    
    private bool FindPosition(char inp)
    {
    	floor += inp == '(' ? 1 : -1;
    	if(floor == -1 &amp;&amp; position == 0)
    		return true;
    	return false;
    }
#Day01

part1: `sed -E 's|(.)|\1\n|g' input.txt | awk 'BEGIN{ans=0;} /\(/{ans++;} /\)/{ans--;} END{print ans;}'`

part2: `sed -E 's|(.)|\1\n|g' input.txt | awk 'BEGIN{ans=0;idx=0;} /\(/{idx++;ans++;} /\)/{idx++;ans--;if (ans&lt;0){print idx;exit;}}'`

...or copy to/paste from clipboard; or `curl` with session cookie...

(aside: noticed the character symbols making up the ASCII X'mas tree change on frontpage reload?)

(and the ads are annoying!!)
Very lightly generalized Haskell:

    {-# LANGUAGE NoImplicitPrelude #-}
    {-# LANGUAGE LambdaCase #-}
    module Advent.Day1 where
    
    import BasePrelude

    -- Explicit accumulator so we can start on a floor other than 0
    day1Helper :: Integer -&gt; Char -&gt; Integer
    day1Helper acc = \case
      ')' -&gt; subtract 1 acc
      '(' -&gt; acc + 1
      _ -&gt; acc

    genericElemIndex :: (Eq a, Integral i) =&gt; a -&gt; [a] -&gt; Maybe i
    genericElemIndex x xs =
      listToMaybe $
      map fst $
      filter snd $
      zip [0..] $
      map (== x) xs

    day1Part1 :: String -&gt; Integer
    day1Part1 = foldl' day1Helper 0

    -- Parameterize the target floor so we're not limited to just the first basement level
    day1Part2 :: Integer -&gt; String -&gt; Maybe Integer
    day1Part2 targetFloor = genericElemIndex targetFloor . scanl' day1Helper 0

day 3, my solution in java: http://pastebin.com/12Asc2rM
These are a lot of fun. Thanks for making this.
day 4, java: http://pastebin.com/WNZGF1nm
In C
    
    #include &lt;stdio.h&gt;
    
    int main()
    {
        int step, c, foundBasement,
            floor = 0;
    
        for (step = 1; (c = getchar()) != EOF; step++)
        {
            floor += c == '(' ? 1 : c == ')' ? -1 : 0;
            if (floor == -1 &amp;&amp; !foundBasement)
            {
                foundBasement = 1;
                printf("Santa entered the basement on step %i.\n", step);
            }
        }
    
        printf("Santa ended up in floor %i.\n", floor);
    
        return 0;
    }
    
https://github.com/ttsda/advent-of-code/blob/master/src/1/main.c
    Day one: 

    &lt;?php
    $inputs = str_split('())(........');
    $floor = 0;
    foreach($inputs as $input) {
        if($input == "(") {
            $floor++;
        } else {
            $floor--;
        }
    }
    echo $floor;
    ?&gt;
Part 1

    expr `grep -o \( input.txt | wc -l` - `grep -o \) input.txt | wc -l`

Part 2

    awk -v FS="" '{ for(i=1; i&lt;=NF; i++) { tot+= $i=="(" ? 1 : -1; if (tot&lt;0) { print i; break; } } }' input.txt
So simple, in PHP:

	$input = '(all the brackets input)';

	echo "\nAnswer 1: ".(substr_count($input, '(') - substr_count($input, ')'));
	
	for($floor=0,$i=0;$i&lt;strlen($input);$i++)
	{
		if($input[$i] == '(')
			$floor++;
		else
			$floor--;
		if($floor == '-1')
		{
			echo "\nAnswer 2: ".($i+1);
			break;
		}
	}
Day 1.1 in Javascript (ES6 flavor) 

    const allFloors = 'YOUR_HUGE_PARENS_STRING';

    function countFloors(floors) {
      return floors
      .split('')
      .map(floor =&gt; {
        switch (floor) {
        case '(': return 1;
        case ')': return -1;
        default: return;
        }
      })
      .reduce((previous, current) =&gt; previous + current);
    }
    countFloors(allFloors);
In Python 2.7, Part 1:

    print sum([1 if x=='(' else -1 for x in in_string])

Part 2:

    floor = 0
    for i in range(0, len(in_string)):
        if in_string[i] == '(':
            floor += 1
        else:
            floor -= 1
        if floor == -1:
            print i + 1  # Advent calendar uses 1-based index, not Python's 0
            break

Any ideas for how to do part 2 in a more elegant way?
solution in Go, just cause.
    
    package main

    import (
        "fmt"
    )

    func main() {
        brackets := "the brackets"
        pos := 0
        ind := 1
        found := false
        for _, bracket := range brackets {
            if bracket == '(' { 
                pos++ 
            } else if bracket == ')' { 
                pos-- 
            }
	
	        if pos == -1 &amp;&amp; !found { 
	            found = true 
	        } else {
	            ind++
	        }
        }

        fmt.Println(ind)
        fmt.Println(pos)
    }
My C++ solution - probably overly verbose, but looping over a string is simpler than recursively doing it in C++.

        #include &lt;iostream&gt;
    #include &lt;string&gt;
    
    int stairs(const std::string&amp; brackets)
    {
    	int result = 0;
    	for(size_t position = 0; position &lt; brackets.size(); ++position)
    	{
    		if(result == -1)
    		{
    			std::cout &lt;&lt; "Entered basement at: " &lt;&lt; position &lt;&lt; std::endl;
    		}
    		(brackets[position] == '(') ? result++ : result--;
    	}
    	return result;
    }
    
    int main()
    {
    	std::string brackets;
    	std::cin &gt;&gt; brackets;
    	int floor = stairs(brackets);
    	std::cout &lt;&lt; "Santa goes to floor: " &lt;&lt; floor &lt;&lt; std::endl;
    	
    	return 0;
    }
I like F#.

    [&lt;EntryPoint&gt;]
    let main _ = 
        let floor = 
          System.Console.ReadLine() 
            |&gt; Seq.fold (fun fl ch -&gt; if ch='(' then fl+1 else fl-1) 0
        printfn "End floor: %d" floor
        0 

In C:

    #include &lt;stdio.h&gt;
    int main(void)
    {
    	char current;
    	int santasFloor = 0;
    	int counter = 1;
    
    	while((current = getchar()) != EOF){
     		santasFloor += ((current == '(')*2) -1;
    		if(santasFloor == -1){
    			printf("Santa first enters the basement at instruction %d\n", counter);
    		}
    		counter++;
    	}
    
    	printf("The instructions point to floor %d\n", santasFloor);
    	
    	return 0;
    }

Yes, I know it prints every time he enters the basement.
This is all you need (Python) where instr is a string containing the input string:

eval(instr.replace('(', '+1').replace(')', '-1'))

Or if you dont want to use the "eval" (because it feels like "cheating")

sum(1 if (c == '(') else -1 for c in instr)
Javascript ES6 solutions (input is defined as the mess of parens)

Part1:

	function getFinalFloor(input) {
		return input
			.split('')
			.reduce((floor, val) =&gt; { 
				return floor += (val == '(') ? 1 : -1; 
			}, 0);
	}

or

	function getFinalFloor(input) {
		return input.match(/\(/g).length - input.match(/\)/g).length;
	}

Part2:

	function getFirstPositionAtFloor(input, floorNo) {
		let found = false; 
		let position = 1; 
		let floor = 0;
		input
			.split('')
			.forEach(val =&gt; {
				floor += (val == '(') ? 1 : -1; 
				if (floor === -1) {
					found = true;
				}

				if (!found) {
					position++;
				}
			});
	}



Done in AS3 and open to suggestions to improve.

    import flashx.textLayout.utils.CharacterUtil;
    
    var floor:int = 0;
    
    var parString:String = "((((()(()(((";  //Parenthesis entered here, cut down for post clarity
    
    var myArray:Array = parString.split("");
    trace("Start");
    for (var i:int = 0; i &lt; myArray.length; i++) {
    	if (myArray[i] == "(") {
    		floor +=  1;
    	} else if (myArray[i] == ")") {
    		floor -=  1;
    	} else {
    		trace("error");
    	}
    }
    trace(floor);
1st part

    String text="puzzle-input-here";
    int len = text.length();
    int Floor=0;
    for(int i=0; i&lt;len;i++){
      Floor = (text.charAt(i) == '(') ? Floor+1 : Floor-1;
    }
    print(Floor);

Basic &amp; Easy to read java implementation

I'm sticking all my solutions on [Github](https://github.com/KristianFenn/AdventOfCode) if anyone is interested. I'm super interested in seeing anyone else's efforts also, especially if they're using a not-so-common language.

As for day 1:

    def m(s):
    	return s.count('(') - s.count(')')

Shortest I could make it in Python with 15 minutes effort. Part 2 is way more verbose:

    def Main(s):
    	floor = 0
    	index = 0
    	for char in source:
    		if (char == '('):
    			floor += 1
    		elif (char == ')'):
    			floor -= 1
    		index += 1
    		if (floor &lt; 0):
    			return index

I'll probably have a go at shortening that later.
Erlang solution, I've only been writing it for ~2 months so I'm open to suggestions.  

    -module(solution).
	-export([main/0]).
	-import(lists, [partition/2]).

	main() -&gt;
		In = io:get_line("") -- "\n",
		{Left,Right} = partition(fun(X) -&gt; X == $( end, In),
		io:format("~p~n", [length(Left) - length(Right)]).

EDIT: solution for part 2 as well: (doesn't consider the case that he never enters the basement)

	-module(sol2).
	-export([main/0]).

	main() -&gt;
		In = io:get_line("") -- "\n",
		Basement = find_basement(In, 0, 0),
		io:format("~p~n", [Basement]).

	find_basement(_, -1, Pos) -&gt;
		Pos;
	find_basement([H|T], Count, Pos) -&gt;
		case H of
			$( -&gt; find_basement(T, Count+1, Pos+1);
			$) -&gt; find_basement(T, Count-1, Pos+1)
		end.

Input is totally unique per user cause this didn't work to get my input :(

    import urllib2
    floors = urllib2.urlopen("http://adventofcode.com/day/1/input").read();
Part 1:

    t.count('(') - t.count(')')

Part 2:

    next(i + 1 for i in range(len(t)) if (t[:i + 1].count('(') - t[:i + 1].count(')')) == -1)
I don't mind Firefox enforcing scanning and signing (although I agree with this author that it is largely ineffective) but it's really irritating that they simultaneously took away ANY ability to run unsigned extensions.  When Google Reader went under I wrote an RSS reader extension for Firefox for my own personal use and have happily used it ever since.  But now I won't be able to use an extension I wrote myself?  That's nuts.

I fought the good fight trying to argue this point with the developers, but as the author notes, there's no winning that argument.

So I went through the mildly painful process of getting my extension signed and hosted on AMO.  (And I guess [26 other people](https://addons.mozilla.org/en-US/firefox/addon/rsstler/) are using it too.)   But how free and open is Firefox if you can't even easily add an extension to your own personal copy?
This post confirms and verbalizes all the bad directions I have felt Firefox develop into in the last decade. Pretty damn sad.
https://bugzilla.mozilla.org/show_bug.cgi?id=1227605

Our automated tool doesn't work so let's block the evidence? -sigh- Maybe it's time to consider a browser change.
i actually feel sad for the zotero guys but in essence i agree

i also think that the manual review process is actually a good way, its just handled badly, for instance we as a company recently had an app reviewed which took nearly 6 months (!) and it was really frustrating experience

what we would have liked to do is just pay/donate some money to mozilla to be reviewed faster, sadly they dont offer that and AMO is still a completly volunteered process and thus not very effective.


And suprise suprise, this shit was hidden (removed?) in /r/firefox

https://www.reddit.com/r/firefox/comments/3u2jhh/automated_scanning_of_firefox_extensions_is

Edit:
Someone reposted it:

https://www.reddit.com/r/firefox/comments/3u8cbe/automated_scanning_of_firefox_extensions_is/
Did no one at the Mozilla Foundation actually go to school and get a CS degree? 

**This reduces to the Equivalence Problem.**

**The Equivalence Problem is undecidable.** 

See a much different way to do this would be to only issue signed certs to developers with a signed contract and proof of identity.

The contract of course says something along the lines of You are liable for any purposely built malware that you upload and get signed. You will pay a fee of $x

Then issue bug bounties to people. For finding extensions that breach these terms that get something like 90% of $x.

Your never going to stop people creating malware. But you will also encourage 10000's or people to hunt it down.

However enforcement of payment may be a problem.
&gt;I asked in February how the scanner would possibly catch things like this, and the response from Mozilla’s Add-ons Developer Relations Lead was that most malware authors are lazy and that he believed the scanner could be made to “block the majority of malware”.

Selection pressure if I ever saw it.
Just another case of incompetent project managers that have 0 technical understanding. Being a developer in mozilla must suck pretty bad.
XPost Subreddit Link: /r/linux 

Original post: https://www.reddit.com/r/linux/comments/3u2ksy/automated_scanning_of_firefox_extensions_is/
First time I've heard the term "Security Theater".  I love it!  Finally I have something to describe all the liberties that are taken away at places I work in the name of "security".
&gt; There is simply no way to detect malicious code like this in a dynamic language like JS through static analysis of the source code.

Oh, there is, but you have to restrict the language like ADsafe does.

The tl/dr is in the bullet points:

http://www.adsafe.org/
OT: zotero seems like as neat extension 

&gt; Zotero is the only research tool that automatically senses content in your web browser, allowing you to add it to your personal library with a single click. Whether you're searching for a preprint on arXiv.org, a journal article from JSTOR, a news story from the New York Times, or a book from your university library catalog, Zotero has you covered with support for thousands of sites.

Seems it's possible (only?) to store things locally too, instead of this cloud (privacy) nonsense most other players are doing.
Whitelisting known good extensions seems like a common sense solution to a serious problem.  A braindead automated scanning system can be tweaked and fixed on the fly once it goes live, so I'm not entirely sold on that being just "theater", but alienating the extensions community is not something Firefox should consider a valid path forward.  That's a path into oblivion.  

The mozilla people need to stop pretending that their overly simplistic approach to security completely solves the problem.  It doesn't.  Programming is full of people seeking the *one true path* and insisting that reality isn't real, problems don't exist, and compromise is for losers.  They're always wrong.  Adapting to the reality of the situation is *always* the right choice.  I'm all for blocking malicious sideloaded extensions, but whitelisting doesn't prevent that.  
That's literally the worst newsletter-popup I've ever seen.
Poorly coded animated nag makes me smash the back button.
Criminals are not keeping their promises. Who would have thought?
It's time to implement a more frequent backup solution at home as well. 

This is really bad for other criminals (and of course victims). People will be more likely to pay if they believe they will get their files back. A few interesting points on randsomeware (other than making decryption possible):

* Provide a free decryption key to unlock one file as an incentive
* Display a timer countdown which once expired will make decryption impossible
* Accept bitcoin (or equivelent anonymous payment) only

Also interesting to note that sometimes the randsomeware will delay lockout until the victim machine browses to a porn website and will then freeze the screen. People will be less likely to take it to a repair shop if the screen is locked on images of porn. [Here](https://www.youtube.com/watch?v=_dKBXeoLIFo) is a short tutorial which can help remove some basic (and known) forms of malware by booting in safe mode.
How do you even get ransomware? I have kept my computer unprotected without any antivirus for the past 3 years and I can't even find viruses online anymore, even if I looked for them.
This isn't programming, it's politics.
So, did they ask for their money back?
I think it's a little humorous that we're complaining about a malfunctioning virus.
From what I've seen of ransomware, they're not particularly complicated bits of malware. They tend to rely more on human error/social hacking than pure technical poweress. That said, I don't know why it's so hard for AV software to detect and kill these things. It must absolutely hammering the CPU while encrypting all your files, that's got to be suspicious activity right there. 
"poorly coded" as if that wasn't the exact purpose..
I wonder: if a lot of these started occurring, would people lose confidence and stop paying, and thus reduce the benefit of releasing ransomware? Or are people going to pay anyway because they have no choice?

[edit] I was unclear. I meant wonder if a lot of these started occurring: ransomware which claims to restore your files upon payment, but bad coding means you still don't get your files back.
Did any major ransomware ever give back control of the files willingly?
That's so unethical of them!
So I'm guessing this lines up with the [RC1 release of ASP.NET 5](https://github.com/aspnet/Home/wiki/Roadmap)? I'm still waiting to be able to use ASP.NET MVC 6 in production.

I've got to say, the new separated structure of .NET gets pretty confusing, especially when bits and pieces aren't stable.
Would this mean that Mono is no longer needed?
Having checked it out recently, this RC feels like a very early beta. I think it having an RC label is purely for PR reasons and you can expect a usable version late next year.
Before anyone asks: .NET has always been available for Linux etc using the Mono framework which is a solid product, I use it a lot. It's worth noting it implements 95% of the actual .NET framework used on Windows.

.NET Core is actually a cut down version so in a lot of cases Mono is the best option.

ASP.NET 5 can run on either the full .NET or .NET Core. There seems to be a lot of people thinking you have to use .NET Core for ASP.NET 5.

Yes, you can do C#/.NET development without Visual Studio (it's never been called Visual Studio**s** ). Why you would ever do that I have no idea, it's the best IDE.


The install instructions are weird. Why am I installing dnvm from a shell script when it is available in homebrew?

Also I am not fond of this sticking a new entry per runtime in my PATH.

Otherwise, this is pretty cool. I feel like C# is finally starting to make inroads to being an acceptable alternative to Java for actual cross platform development. I just wish the tooling support was better with regards to dependency management. NuGet isn't quite there. Which is worrisome because Microsoft is relying heavily on NuGet for their current strategy. 

One of the bigger missing pieces, in my view, is the lack of a good alternative to Java's *-sources.jar. Projects like [GitLink](https://github.com/GitTools/GitLink) could be a viable solution.
Reposting a question I asked elsewhere, since it seems more relevant here:

I have to say, on the one-year anniversary of the announcement that .NET is open-source, I'm really disappointed with the progress of crossplatform .NET/ASP.NET. At the rate they're going it'll be years before it's production-ready on Linux. MS is taking their sweet time. Their Core/ASP.NET teams seem bare-bones, why not add more developers? What's a couple more million a year for 10 top of the line developers? (yes, I know about the mythical man-month...but since we're talking years here, extra manpower would definitely accelerate things).
Can someone ELI5 why having .NET cross platform is beneficial? Isn't a lot of .NET's libraries geared towards hooking into a windows environment and various Microsoft Full stack? 
What's the toolchain like?  Do you have to use some form of visual studios?  Can you make open source or commercial projects with the free version?
this article gives one important piece of bad advice. use package.el! finding random elisp files on the internet is unnecessary these days. 
Coming soon: a VP's Guide to VI.
CEOs generally have to do some public speaking, and a deformed, claw-like left hand would probably be distracting. Therefore, CEOs should avoid Emacs.
When I saw it was a "CEO's Guide" I was expecting 5 to 10 bullet points where each bullet point was at most one sentence long.
Instead of having a .emacs file and a .emacs.d directory, I prefer to have an init.el inside my .emacs.d. It's easier to put in git this way.
Reading an article like that gives you a fascinating glimpse into how humans develop complex and self-contradictory religions.
Org-mode in Emacs is absolutely wonderful. I use it to track my daily work. I import automatically everything from my google calendar. But the best point is tracking tasks for me. Editing tasks lists in a text editor with a few special key combos is really a refreshing and rewarding experience compared to services like todoist, and out of the box it has time tracking and note taking. When I'm working in a task at work I clock in (C-c C-t), and when I encounter issues or make progress or get blocked on a task I take a note (C-c C-z) like a little commit message that I can refer to later with all the details and context of what worked, where I got stuck or who I'm waiting on. Org-mode automatically puts in all the details like when I clocked in, when I took the note, and I can put in predicted hours for all my tasks too and compare with actual time. As someone who works from home, often on a project solo it's important for me to have a trace of the work I did. Having all this in Emacs, an environment I know so well is the icing on the cake.
Cool I can only see black and white background, work on your blog's accessibility, because it can't show A SIMPLE WALL OF TEXT without java script :(
Yup, he compared his text editor to fucking old bicycle that "it's broken and you'll hate it but you'll love it".

I actually don't have any skin in the Emacs/Vi debate but goddammit this is insufferable.
The real benefit is the the CEO will be so busy trying to figure out EMACS that they will not have time to bother the dev team. This is assuming they don't give up after a few minutes.
As a "CEO's guide" this was awful.  
First time I used emacs was in 1982 on a DEC-20 running TOPS-20.  It's been engrained in my brain ever since.  I have moved on to shinier things for most of what I do, but I still do use emacs sometimes, and it makes me happy.  I lost all my config files and stuff I'd built up over the years, not even sure how/when, but I do remember the 90s when emacs was my IDE, my browser, my emailer, my games, my everything!
Who cares? Use whatever tools you want to
Black text on white background dual monitors. Does Mr CEO wear sunglasses inside? Or does "if it's not broke don't fix it" not apply to color schemes?
&gt;Emacs is like a box of classic Campagnolo parts and a beautiful lugged steel frame that's missing one crank arm and a brake lever that you have to find in some tiny subculture on the Internet. 

I'm out.
&gt;Also, most modern applications seem simultaneously patronizing and lacking in functionality and usability.5 The only applications I dislike more than office apps are the online versions.

Fuck you.

Sincerely,
Every coworker who's had to convert your fucking TeX masterpiece into a .docx file that a real CEO can fucking use.
Give sublime text a try. Much better.
I'd start with 50:10:1 because thinking of 100 ideas and writing them down sounds like a lot of work.
Ah! Apparently I've been using the 327:1:0 method for quite a while!

That notebook is more or less a place for ideas to go die. 

I used to code or make stuff for fun, but a career and family really put a damper on that. 
I appreciate where he's coming from but boy... what a way to suck the fun out of doing your own thing.
This rings close to how most musicians work on releases - write 35 songs, record 18, release 10
and does it work?
These days, I prefer:

&gt; 0:0:0


[dokuwiki](https://www.dokuwiki.org/dokuwiki#) can help you with maintaining the 100 list.

Try it, you'll like it!

*(yeah, I know, this is just the housekeeping stuff, but it's a start.)*

Sorry, but that's just dumb.  Here's the 1:1 method - find one project that already exists and you're interested in, and contribute to it.  No need to waste a lot of time on half-baked ideas and reinventing the wheel.  The best way to guarantee that a project will be successful is to make sure it's supported, and if you go where there's a community already you're most of the way there.
Very nice post. Enjoyed
&gt; you copy a dir from VFAT and want to turn off executable bits on files using `chmod -R 644`, that will fail to recurse as it removes the executable bits from dirs.

TIL. That never occurred to me but it makes perfect sense. I've always hated that directories are "executable."
Here's a fun one: `sort -R/--random-sort`. You probably think it sorts lines randomly, right? I mean, the option is literally named 'random sort', right? Wrong! It's not a random sort, because it is sorting by the *hash* of each line. If you want an actual random sort, or 'shuffle', you need to use `shuf`.

(They refused to document this in the man page [for over 4 years](https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=641166) because *apparently* you're not supposed to be reading the `man` page at all, you're supposed to be reading the `info` page.)
TIL about ltrace. That could be useful.


How much of this is gnu specific and how much also applies to, say, the various BSDs or other unix's implementations?
Here's one that gets me all the time. If your unix pipeline is meant to be doing stuff as items are moving through, but you see nothing...?

Make sure that if you're grepping that you use `grep --line-buffered`. Otherwise grep will wait for huge chunks of input before it passes the output onwards.

Debugging this is extra painful, as if you remove the processing that happens after grep to see if it is behaving correctly, buffering is turned off as it is connected to a TTY.

This has cost me hours of my life before.
And we're already at a stage in computing where we're unable to change this old behavior anymore? Why? Because it might break stuff? We break stuff all the time, why not once for things that would make sense?
The expr stuff is only confusing if you ignore the meaning behind those values:

In the context of the result of the expression - in a C-like way - zero is false and non-zero is true.

In the context of *command exit codes*, 0 is success (true) and non-zero is failure (false).

This means that if you do something like:

    if expr 1 &gt;/dev/null; then echo true; else echo false; fi

or

    expr 1 &gt;/dev/null &amp;&amp; echo true || echo false

you will see 'true' both times, which is correct as the expression value is non-zero.
&gt; A very common issue encountered is with the default ordering of the sort utility. Usually what is required is a simple byte comparison, though by default the collation order of the current locale is used. To use the simple comparison logic you can LC_ALL=C sort ... as [detailed in the FAQ](http://www.gnu.org/software/coreutils/faq/coreutils-faq.html#Sort-does-not-sort-in-normal-order_0021).

I got bit by this.  Functional tests I wrote sorted output for comparison, and I would get different results depending on the hosts the tests ran on.  It took me some time to realize that it was the locales that differed.
&gt; you copy a dir from VFAT and want to turn off executable bits on files using chmod -R 644, that will fail to recurse as it removes the executable bits from dirs.

Known issue. Simply do

    find path -not -type d -exec chmod 644 {} +

instead.

&gt; echo

His comments regarding echo are somewhat incorrect. GNU echo is posixly correct but does not implement the XSI option. POSIX states that echo must not do option parsing (thus `--` must not be recognized). Factually, it's only allowed to check if the first argument is `-n` and then perform special behaviour (like GNU `echo` does).

&gt; *sum

This is dangerous as file names may contain newlines (go figure).

One util the author does not talk about:

&gt; tr

This thing is broken for non-ASCII locales and has no wide-character support. Don't use it in such situations. For example:

    echo Döner | tr 'ö' '*'

yields `D**ner` on GNU `tr` because `ö` is interpreted as two bytes. GNU `tr` also mishandles ranges with respect to locales. The correct behaviour is unintuitive though, I can understand t hat design decision. Generally, do not use GNU `tr` outside of the C locale.
This is why the "Unix philosophy", or rather its implementation by those who are so defensive about it, sucks. We need something like systemd but to completely replace coreutils and all current shells (bash, zsh, fish, etc...) with something very strict and descriptive. Heck, as flawed as Powershell is, it's still much easier to use and better in general than the shitshow that we have with this unixy mess glued together from tiny pet projects.
Using find is more general and functional in this case: `find -type f -print0 | xargs -r0 chmod 644`

`find -type f -exec chmod 644 {} \;` is easier I think
The `tail` bit is annoying, because if the `tailf` manpage is to be believed, you have two incompatible sets of features here: `tail -F` will follow file rotations, while `tailf` will avoid disk accesses while the file is not changing.
&gt; The df, du, ls --block-size option is unusual in that appending a B to the unit, changes it from binary to decimal. I.E. KB means 1000, while K means 1024.

&gt; In general the units representations in coreutils are unfortunate, but an accident of history. POSIX species 'k' and 'b' to mean 1024 and 512 respectively. Standards wise 'k' should really mean 1000 and 'K' 1024. Then extending from that we now have (which we can't change for compatibility reasons):

&gt; - k=K=kiB=KiB=1024
- kb=KB=1000
- M=MiB=1024^2
- MB=1000^2
- ...

&gt; Note there is new flexibility to consider when controlling the output of numeric units, by leveraging the numfmt utility. For example to control the output of du you could define a function like:

&gt;     du() { env du -B1 "$@" | numfmt --to=iec-i --suffix=B; } 

And this is where the author is wrong.

From http://pubs.opengroup.org/onlinepubs/9699919799/

DU:

&gt; The -b option was added to an early proposal to provide a resolution to the situation where System V and BSD systems give figures for file sizes in blocks, which is an implementation-defined concept. (In common usage, the block size is 512 bytes for System V and 1024 bytes for BSD systems.) However, -b was later deleted, since the default was eventually decided as 512-byte units.
Still no native issue/pr voting...
So they moved back to top nav, after moving to the sidebar a few years ago
all I really need is a built in dark mode
I like this. The collapsing sidebar was annoying. I never paid attention to when it collapsed and when it didn't. Obviously, if you take a second to think about it it collapses when viewing files so you can have more width in the file viewer. But I don't really want to take a second to think about it.
I wish instead of moving the navigation bar from top to side and back again they would focus on the wiki feature, because it sucks. I don't see why:

1. To change the repository you have to get your PR approved but any idiot can come and take a shit on the wiki, no permission required.
2. I can't make a PR that commits simultaneously to the wiki and the project, how is the documentation supposed to be in sync with the code without this?

[deleted]
I wish that Github would stop focusing on optimizing the UI and add real project management tools like assigned PR reviewers and separation of PR and issues.
TLDR; We moved a few things but didn't really do anything much else to improve the broken stuff in the UI!
It says it is opt-in, but I can't find any setting to activate it.
Great! Now can we finally get a landing page for a project that is actually helpful to anyone but developers of the project?
All I need is fully supported console mode.
If someone wants to keep the new tabbed nav stickied to the top of the page, I wrote a Firefox add-on / Chrome extension to do so: https://github.com/tonglil/github-static-nav 
gitlab like. 
Great, because what I needed was *less* vertical real estate, in addition to 

* OS menu, 
* some hairline of gap where I can see my desktop,
* app window bar with chrome, 
* browser menu / tabs
* browser address bar, 
* bookmarks, 
* html body padding,
* clean spacious whitespace web designs,
* window footer chrome,
* some hairline of gap where I can see my desktop, and
* OS app dock thing, 

Now I also get to lose another `26 px` for a menu that worked just fine as a sidebar.

Thanks Obama

(edit; vertical.  and I still stand by my point)
&gt; new design

Removes a side panel.
When are they going to add the ability to subscribe to particular Repos by email rather having to set it globally?
It fucking looks the same, work on a fucking dark mode instead, you lazy githubers
Mercurial repositories when?
This has nothing to do with delayed ACKs, you are not even beginning to fill up the initial congestion window used for slow start.

The real issue, and what should be fixed, is that you are issuing two small sends. Nagle's algorithm is a sort of protection by your TCP stack against bad programmers, and it acted correctly here. There was no reason for it to believe that you weren't about to send even more small packets.
I think W. Richard Steven's *TCP/IP Illustrated* describes this functionality *really* well.
I could tell you a joke about TCP but I would have to keep telling you the joke until you get it.

I could tell you a joke about UDP but you wouldn't get it
It's interesting that Ruby splits requests into two packets even if they could fit into one. Anyone know why?
That's one good reason to learn TCP.  A more general reason is that TCP/IP is pervasive throughout software today.  If you do anything web based or creating an app that communicates with another computer, you are using TCP/IP.  The sad part is that I'm pretty sure most software engineers don't know much about it.
This certainly qualifies as "a little"
First Upsert, and now this! Postgres is putting smiles on my face. Now I just got to wait for this to make it into the AWS's supported version in RDS (maybe if I put it on my wish list I send to Santa this year).  
What is parallel query? What does it do?
Admittedly I haven't used it for much, but Postgres didn't have parallelism within a query before this? I'm actually kind of surprised by that.
Just want to point out that this feature has very limited usefulness for OLTP. When you have multiple users, each running multiple queries concurrently, PostgreSQL already scales really well (certainly better than many alternatives). What it lacked so far is parallelizing a single large query. That's useful for anaytics/reports/etc. (where you have a small number of large queries), but doesn't matter that much for OLTP (large number of small queries).
Yet another reason to avoid other databases.
Way to go!
I wonder how many interns it takes to power just this service.
This should be called the mechanical turk test
http://www.wired.com/2015/08/how-facebook-m-works/

This story is terrible. Facebook has been super clear about the fact that M is powered by humans and that the 'AI' is simplistic and technically uninteresting.

So quotes like this are frustrating:  "However, its non-instantaneous nature and the sheer unlimited complexity of tasks it can handle suggest otherwise. The opinion is split as to whether or not it’s a real AI, and there seems to be no way of proving its nature on way or the other."

There is a way of proving its nature.  Ask Facebook.  They said:  “The AI tries to do everything,” says Alex Lebrun, the founder of Wit.ai, a startup Facebook acquired to help build this smartphone tool. “But the AI is supervised by the people.”

In fact, they've also admitted that humans answer questions right now in order to try to train the AI.


I was under the impression that it was common knowledge M uses people to answer queries that can't be automatically handled? 
M needs to fucking chill with all the exclamation points.
Actually, Turing mentioned the opposite test case [in his paper](http://loebner.net/Prizef/TuringArticle.html):

&gt; If the man were to try and pretend to be the machine he would clearly make a very poor showing. He would be given away at once by slowness and inaccuracy in arithmetic.
Sounds like a mechanical turk. Probably combined with Siri type AI for simple stuff. Really smart, because it allows them to have the service active so that they can keep track and figure out what kind of requests people make, and as time goes on they can extend the AI to cover more and more types of tasks.

It's a smart strategy, allows for data collection while still providing an effective service. I was reading about another company recently that did something similar. They wanted to design an automatic personal assistant that could interact with your clients/customers via email to schedule meetings and things. In order to obtain the data to really understand the use case, they launched the service and powered it with people. This allowed them the time to build out the AI features, and do it correctly because it is powered by usage data.
I would love to see the source code, what are the odds of it being a mess?
Pretty neat even if human aided for now. Still not sure if I'll use it though 

Also still in beta and only available to 10,000 inside the Bay Area. 
If I were Facebook and designed this, I would have written an AI that operates autonomously by default, but can ask human employees for help when it is asked to fulfill a request outside of its limitations. They'd need a response delay so that the requests that go to a human don't seem to be taking "too long" compared to the instant ones you're used to.

Quite cool though. I suspect that they are probably doing some automated tracking of what sorts of requests go to humans and attempting to extend the AI to cover them as well.
Reminds me of Vinge's short story, The Cookie Monster.  High-res scans of people's brains are run on a quantum computer and are used to perform human-level tasks like translation, analysis, tech support, etc, but much faster than normal.
Facebook M is actually behind the curve. Wolfram has had their maths AI, that runs on captive mathematics undergraduates, for years.
Was there any interaction with the person who called?  Or did they just have her on mute until the caller hung up?  I haven't heard of this service before, but it's interesting FB is trying to hide what it is.
Facebook should hire this guy to speed it up:

https://www.youtube.com/watch?v=blB_X38YSxQ
This is the best tl;dr I could make, [original](https://medium.com/@arikaleph/facebook-m-the-anti-turing-test-74c5af19987c) reduced by 85%. (I'm a bot)
*****
&gt; Facebook M - The Anti-Turing TestFacebook has recently launched a limited beta of its ground-breaking AI called M. M&amp;#039;s capabilities far exceed those of any competing AI. Where some AIs would be hard-pressed to tell you the weather conditions for more than one location, M will tell you the weather forecast for every point on your route at the time you&amp;#039;re expected to get there, and also provide you with convenient gas station suggestions, account for traffic in its estimations, and provide you with options for food and entertainment at your destination.

&gt; The biggest issue with trying to prove whether or not M is an AI is that, contrary to other AIs that pretend to be human, M insists it&amp;#039;s an AI. Thus, what we would be testing for is humans pretending to be an AI, which is much harder to test than the other way round, because it&amp;#039;s much easier for humans to pretend to be an AI than for an AI to pretend to be a human.

&gt; So what we want to prove is not the limitations of the AI, but the limitlessness of the humans behind it.


*****
[**Extended Summary**](http://np.reddit.com/r/autotldr/comments/3sbn6z/facebook_m_the_antituring_test/) | [FAQ](http://np.reddit.com/r/autotldr/comments/31b9fm/faq_autotldr_bot/ "Version 1.6, ~13518 tl;drs so far.") | [Theory](http://np.reddit.com/r/autotldr/comments/31bfht/theory_autotldr_concept/) | [Feedback](http://np.reddit.com/message/compose?to=%23autotldr "PMs and comment replies are read by the bot admin, constructive feedback is welcome.") | *Top* *five* *keywords*: **human**^#1 **call**^#2 **AI**^#3 **nature**^#4 **prove**^#5

Post found in [/r/programming](http://np.reddit.com/r/programming/comments/3s9g8d/facebook_m_the_antituring_test/), [/r/MachineLearning](http://np.reddit.com/r/MachineLearning/comments/3sa7gq/facebook_m_the_antituring_test/), [/r/Cyberpunk](http://np.reddit.com/r/Cyberpunk/comments/3sape3/facebook_m_the_antituring_test/), [/r/technology](http://np.reddit.com/r/technology/comments/3sb1va/facebook_m_the_antituring_test/), [/r/hackernews](http://np.reddit.com/r/hackernews/comments/3s9fy2/facebook_m_the_antituring_test/), [/r/Newsbeard](http://np.reddit.com/r/Newsbeard/comments/3saw3e/developer_facebook_m_the_antituring_test/), [/r/mildlyinteresting](http://np.reddit.com/r/mildlyinteresting/comments/3sau79/facebook_m_virtual_assistant_might_not_be_ai/) and [/r/skeet_skeet](http://np.reddit.com/r/skeet_skeet/comments/3saeaf/facebook_m_the_antituring_test/).
It might just be me, but the term ground-breaking is being miss used in this article. 

This isn't ground-breaking AI. This is actually pretty basic AI at most, especially if it has human inputs. 

All of the features shown can be done using existing APIs. In fact, all it does is use existing API such as Google Maps.
This was entertaining.  The author was suspicious far before I was, but I still believe it is possible.
This is how it actually looks like: http://imgur.com/R0kvGYZ
This guy is grade-A douchebag.
&gt; 
    Linux-i386
    Linux-x86_64 (amd64)
    Linux-powerpc
    Linux-sparc
    Linux-ARM
    Win32-i386 (2000/XP, WinNT or later)
    Win64-x86_64 (XP or later)
    Wince-ARM (cross compiled from win32-i386)
    FreeBSD-i386
    FreeBSD-x86_64
    Mac OS X/Darwin for PowerPC (32 and 64 bit)
    Mac OS X/Darwin for Intel (32 and 64 bit)
    iOS (ARM and AArch64/ARM64) and iPhoneSimulator (32 and 64 bit)
    OS/2-i386 (OS/2 Warp v3.0, 4.0, WarpServer for e-Business and eComStation)
    Haiku-i386
    GO32v2-i386
    Nintendo Gameboy Advance-ARM (cross compile from win32-i386)
    Nintendo DS-ARM (cross compile from win32-i386)
    Nintendo Wii-powerpc (cross compile from win32-i386)
    AIX 5.3 and later for PowerPC (32 and 64 bit)
    Java JVM (1.5 and later) and Android Dalvik (Android 4.0 and later)
    Android (ARM, i386, MIPS) via cross-compiling.
    MSDos-i8086 (cross compiled from win32-i386 or Linux)
    Amiga, MorphOS and AROS


This is a very impressive list of supported platforms.
Pascal is slowly recovering lost terrain as the ultimate developer multiplataform :D .
A JVM backend, official Android backend, lots of optimizations...this is awesome!
Here are some links

Searchable library documentation:
http://docs.getlazarus.org/

Free Pascal 3.0 and Lazarus installers:
http://www.getlazarus.org/setup/
Funny as I recomended this compiler today to my friend.
Congratulations to the FPC team! I've sent in a few patches myself, and followed the progress on the core mailing list. These guys are doing great work! This is, by far, the best and most modern pascal compiler in the world!
The install script hasn't been updated for OS X El Capitan.  And it requires MacPorts.  I use homebrew.  Apparently there is a Caskroom in homebrew that can install Lazarus and FPC.
Congratulations to them.

Still missing some essential features (to me at least):

* Anonymous methods. (Last I heard they couldn't even agree on the design.)
* Custom attributes.
* Enhanced rtti.

Back when I worked on Delphi code, I used those features all the time and I wouldn't want to use a pascal dialect without these features.
Didn't Pascal invent an adding machine?
The link to 3.0.0 docs on the announcement post is broken - http://www.freepascal.org/docs-html/3.0.0/fpctoc.htm (404)
I remember "inventing" null terminated strings when I was first learning to program in motorola complex instruction set assembly in a robotics course. I called my friend and was like "So yea, this subroutine just walks along incrementing the pointer and shoving the contents of that location to the lcd display routine, and then when it hits a zero it exits, so now I can print arbitrary text!"

He was less impressed :P
Even though it's reinventing the wheel it always feels good to discover these things on your own and then when you find out you were using a good approach.

The one for me was the "observer pattern". I had to solve a problem with notifications going around a system and ended up reinventing this design pattern I wasn't aware of.
&gt; Besides, if people call Columbus the discoverer of America then I can sure as hell can call myself the discoverer of Treesort.    

A good analogy. Discovering is learning about something previously unknown to you, as opposed to inventing, which is learning something previously unknown to anyone. 
Next time, try to see if p=np.
I really liked the spirit of that article.
Feels good right? Now imagine that someone patented your discovery. You can't use it. Already happended several times (see [Carmack's shadow volume technique](http://www.geek.com/games/john-carmack-to-rewrite-doom-3-shadow-volume-code-to-avoid-lawsuit-1442325/))
Remember me when a fellow student explained me his discovery about doing a "better" encryption with XOR. According to him, no one ever thought about cycling on every byte of the encryption key (repeating key).

Many inventions were invented independently by different people. In general, new inventions are built on top of other inventions or other's failed experiments. Nothing is created in a vacuum.

And this is one of many reasons why patents suck.

A great show about inventions and how they came to be is [Connections](https://en.wikipedia.org/wiki/Connections_%28TV_series%29) with James Burke. You can find most episodes on YouTube. I highly recommend to watch a few episodes. It's pretty eye-opening.
Well done, who cares if you reinvented well know a well known algo, you learnt from it (how to code a complex data structure, not to google :D ) so you sound like you'll end up a better programmer than most of the shitty node.js hacks.

also, NIH is not exactly rare so you're in good company!
Completely agree with this.

I used to do this with mathematics and later programming. It was fun, and I learned a lot. I actually went through a data structures book this way -- learn the basics, and then sit pondering how to make things better.

Good on the professor for letting you go through with it though. I've had lots of fun with new programmers explaining me preexisting things they've "figured out" -- let them work on it, discuss it with them and flesh it out, and then finally tell them it its "real world name". Really awesome watching people figuring out beautiful concepts for the first time on their own.
Nice, it is important that you really understand something. Next step, learn about min/max-heaps and look if those data structures can inspire you again ;)
Now google counting sort. It blow my mind on the first time I seen it.
You can actually build a balanced binary tree from an array without doing balancing by writing a function that partitions the array based on being lesser and greater than the midpoint and recursively do that for the left and right sub trees.

Then you can fuse the reading out of the tree elements with the building of the tree in this way, and you have created QuickSort! QuickSort is really just a tree sort with the steps fused together and optimized.
I remember "inventing" the factorial function in grade school when we were learning about combinatorics. Of course, there was no Google or Yahoo, or even Altavista that I could have checked first.

I think the author did nothing wrong. It is always much more valuable to learn this way. As long as he doesn't go on and apply "NIH" everywhere.
I thought I was *so cool* when I thought of using the mean as the pivot in Quicksort, as a good approximation of the median (the optimal pivot).
The thing most people don't realize about trees is: it's faster to sort your data with some other algorithm first and then *bulk*-insert into the tree (less rebalancing is needed).

In particularly, inserting already-sorted elements into a tree one at a time is just about the worst case, compared to a proper bulk insert which will insert the middle element first, then the middle of each half, etc.
I came up with an interesting variation on AVL trees, and I actually haven't seen it implemented anywhere else.  Basically instead of keeping track of its height, each node keeps track of the node count on either side (really only needs the left, but does the right for balancing).  Has the minor but occasionally useful effect of letting you reach a particular node by *array* index, though in O(log N) rather than O(1) like an array.

When you want index x, start at the top node.  If x is greater than the current node's Lcount, then recurse right with x - Lcount - 1 (the 1 is for the current node).  If it's less than the Lcount, recurse left with the index unchanged.  If it's exactly equal to the Lcount, then you've found the right index.
As a self-taught Argentinian programmer/tester that just doubles your age: Bravo!

I wish I was that smart at 19. And the funny thing... people at that time thought I was! :-D

I wish you a great life!
Let me know if you want a job when you finish university :-)
This reminds me of the time when at work the internet connection was restricted and I started thinking how to swap two variables in C without using a third one. I solved the puzzle (the solution used the XOR operator) and, when at home, yep, someone else already "discovered" the same method.
I invented bubble-sort :D
By the way, I learned programming on Standard ML, so even bubble-sort is hard when you can't declare variables.
Its a good learning experience. Even if your solution is still sub-optimal, it usually gives you a jump start in understanding the better developed solution.
And all I ever did was program a calculator to do basic calculus. The good news is that the teacher stopped allowing calculators after he figured it out, but gave me extra credit in his programming class. Principle was less impressed by the 17000/1 grade, but well worth it.

Even if you are not the first to do something, it shows that you think about problems the right way. Keep at it and maybe someday you can put something new out.
Keep inventing, one day you may come up with something really unique!
Yeah I invented a queue-based producer-consumer model one time.
I "invented" the event delegation pattern that Reactjs use in their components using data attributes ids with jQuery as an event bus, it was fun, but since it not use a virtual DOM it make a lot of reads to it and also is pain in the ass to debug.
While studying b-tree's I got the idea that leaf nodes should be linked... to better support range queries... Yeah, that's called a b+tree :)
For an assignment we had to find minimum spanning trees, given a connection matrix. 

When I started the assignment I had no clue what I was even trying to get (I didn't know the concept of MSTs). I ended up "inventing" and implementing Boruvka's algorithm to solve it, later I also "invented" Kruskal's algorithm on paper. 

Coming up with a correct algorithm is a thrill, even though someone else has done it way before you did. To me it was rather humbling, however, I'd like to say: "great minds think alike", so my mind is doing pretty great so far. 


Keep up the creativity, it might not matter that much for your exams but it's way more fun. 
Are there any new computer science concepts that actually have been invented since the 1970s?
This is what CS Education is about.

Well, one of the things. 
Was the title of this post inspired in any way by "Dr. Strangelove"?
Still happens to me.  Every so often I'll come up with some technique I think is pretty cool.  For example, at my last job I had to solve this copy-on-write problem and I did so with what I would end up calling "pimplcow".  You just do this:



    struct whatchamit
    {
        whatchamit();

        void mutate();
        void no_mutate() const;

    private:
        struct impl;
        std::shared_ptr&lt;impl&gt; pimpl_;

        impl * pimpl();
        impl const* pimpl() const;
    };

    struct whatchamit::impl
    {
        void mutate() {}
        void no_mutate() const {}
    };
    void whatchamit::mutate() { pimpl()-&gt;mutate(); }
    void whatchamit::no_mutate() { pimpl()-&gt;no_mutate(); }

    whatchamit::impl const* whatchamit::pimpl() const { return pimpl_.get(); }
    whatchamit::impl * whatchamit::pimpl()
    {
        if (!pimpl_.unique())
            pimpl_.reset(new impl(*pimpl_));

        return pimpl_.get();
    }

https://crazycpp.wordpress.com/2014/09/13/pimplcow/

I had never seen anyone do this.  I get around a bit online too.  When to CppCon and someone did a presentation using this technique to implement type-hiding style inheritance.
This happened to me when I was working on a turn based team multi-player game. One of the mechanics is a voting system where the members of the team propose what to do next and then they get to vote on each player's proposal. In the end I came up with a neat proposal stacking system that was then validated by an array of unique id's that served as votes. 

A month later I'm told I basically implemented a block chain. 
Reading this, it occurs to me that reinventing algorithms is something I did a lot of when I was solving project euler problems.

So, OP, I'd recommend trying it out.  You'll constantly run into problems where you'll go "The simple bruteforce method looks like it'll take several hours to run, and I know for a fact there's a way to do it in a minute.. how do I get there from here?"

https://projecteuler.net/


It's actually good that you discovered it on your own. It'll come in handy with coding interviews, and you'll know the structure more than most others. 

That being said, it probably won't be useful outside the interview because you'll more than likely use a library or a compiler type instead of a custom implementation
The canonical example is a medical researcher who invented a method of calculating the area under a [glucose tolerance curve](https://fliptomato.wordpress.com/2007/03/19/medical-researcher-discovers-integration-gets-75-citations/). 
I invented binary exponentiation ;)
If your function takes an array and then makes a balanced binary tree and only then does the search it will be o(n) + o(log n) with the o(n) coming from the fact you have to build the structure which means visiting every node in the array first. But on large enough data sets still faster than O(n^2).
I'm surprised that there isn't a standard math lib for matrix, quat, etc by now.
I can vouch for this, I'm just starting to use it myself - if you're new to graphics programming like me, it will be a bit of a learning curve, but the documentation is decent and the developer is friendly in gitter :)
I love good looking C++ code.

I haven't touched C/C++ in a couple of years. Is CMake still the go-to build tool?
I was thinking a bit earlier today about cool stuff that could be done if I had a library like this, and here it is :)

Is there any chance of iOS as well. Is it porting effort, or is it not possible due to some platform limitations?
This looks nice. Good job!
Looks nice at least. Eclipse still struggles with a good looking dark theme, Visual studio's is perfect. I personally don't understand the extreme hate for Eclipse, I run it just fine on an old core 2 duo with 2 gigs of ram on CentOS 6 and it doesn't feel particularly slow. 
Now even slower 
No thanks. This doesn't seem to offer *any* advantages over the classic Eclipse while adding overhead and removing features available in cassic Eclipse.

The site is also misleading because this is just Codenvy's pet project which has very little to do with the actual Eclipse developers.
Give me IntelliJ or give me death. Also, Eclipse Che? IMHO, not a great name. 
&gt; Che transforms the workspace into a high performance environment accessible through RESTful APIs that can be controlled by any IDE. 

http://i.imgur.com/baEXpDi.gif
The prejudices is strong within this thread. 

Even though I am sceptical I will give it a try.  

It's cool to see some new stuff to IDE development though! 
The dark theme almost like atom/sublime defaults... Can i get that in eclipse? I downloaded some theme addon but it only changed parts of eclipse to that theme, some parts stayed light.
Am I the only one who gets annoyed by projects that don't build out of the box or when you follow the instructions they give?


    E:\dev\eclipse&gt;git clone https://github.com/codenvy/che
    Cloning into 'che'...
    remote: Counting objects: 7890, done.
    remote: Compressing objects: 100% (340/340), done.
    remote: Total 7890 (delta 150), reused 0 (delta 0), pack-reused 7501
    Receiving objects: 100% (7890/7890), 1.06 MiB | 506 KiB/s, done.
    Resolving deltas: 100% (2718/2718), done.
    
    E:\dev\eclipse&gt;git checkout 4.0
    fatal: Not a git repository (or any of the parent directories): .git
    
    E:\dev\eclipse&gt;cd che
    
    E:\dev\eclipse\che&gt;git checkout 4.0
    Branch 4.0 set up to track remote branch 4.0 from origin.
    Switched to a new branch '4.0'
    
    E:\dev\eclipse\che&gt;cd assembly-sdk
    
    E:\dev\eclipse\che\assembly-sdk&gt;mvn clean install
    [INFO] Scanning for projects...
    Downloading: https://maven.codenvycorp.com/content/groups/public/org/eclipse/che/depmgt/maven-depmgt-pom/4.0.0-M5-SNAPSHOT/maven-metadata.xml
    Downloading: https://maven.codenvycorp.com/content/repositories/codenvy-public-snapshots/org/eclipse/che/depmgt/maven-depmgt-pom/4.0.0-M5-SNAPSHOT/maven-metadata.xml
    [WARNING] Could not transfer metadata org.eclipse.che.depmgt:maven-depmgt-pom:4.0.0-M5-SNAPSHOT/maven-metadata.xml from/to codenvy-public-repo (https://maven.codenvycorp.com/content/groups/public/): java.lang.RuntimeException: Could not generate DH keypair
    [WARNING] Could not transfer metadata org.eclipse.che.depmgt:maven-depmgt-pom:4.0.0-M5-SNAPSHOT/maven-metadata.xml from/to codenvy-public-snapshots-repo (https://maven.codenvycorp.com/content/repositories/codenvy-public-snapshots/): java.lang.RuntimeException: Could not generate DH keypair
    Downloading: https://maven.codenvycorp.com/content/groups/public/org/eclipse/che/depmgt/maven-depmgt-pom/4.0.0-M5-SNAPSHOT/maven-depmgt-pom-4.0.0-M5-SNAPSHOT.pom
    Downloading: https://maven.codenvycorp.com/content/repositories/codenvy-public-snapshots/org/eclipse/che/depmgt/maven-depmgt-pom/4.0.0-M5-SNAPSHOT/maven-depmgt-pom-4.0.0-M5-SNAPSHOT.pom
    [ERROR] The build could not read 1 project -&gt; [Help 1]
    [ERROR]
    [ERROR]   The project org.eclipse.che.sdk:assembly-sdk:[unknown-version] (E:\dev\eclipse\che\assembly-sdk\pom.xml) has 1 error
    [ERROR]     Non-resolvable parent POM for org.eclipse.che.sdk:che-sdk-parent:4.0.0-M5-SNAPSHOT: Could not transfer artifact org.eclipse.che.depmgt:maven-depmgt-pom:pom:4.0.0-M5-SNAPSHOT from/to codenvy-public-repo (https://maven.codenvycorp.com/content/groups/public/): java.lang.RuntimeException: Could not generate DH keypair and 'parent.relativePath' points at wrong local POM @ org.eclipse.che.sdk:che-sdk-parent:4.0.0-M5-SNAPSHOT, E:\dev\eclipse\che\pom.xml, line 16, column13: Prime size must be multiple of 64, and can only range from 512 to 1024 (inclusive) -&gt; [Help 2]
    [ERROR]
    [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
    [ERROR] Re-run Maven using the -X switch to enable full debug logging.
    [ERROR]
    [ERROR] For more information about the errors and possible solutions, please read the following articles:
    [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException
    [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/UnresolvableModelException
    E:\dev\eclipse\che\assembly-sdk&gt;
IntelliJ fan here. I just tried the IDE on my Mac, and while running into some issues in the beginning (mostly because I misread the instructions; you need to go into the docs to find out you need to start Docker first), I'm quite impressed. Yes, it's an alpha, so yes, there are a lot of missing features and some bugs, but overall, it worked pretty nice!

The performance wasn't as bad as I expected. Browsing files is easy, editing files is fast (saving is a little less fast, due to the 'updating dependencies' message every time). But the think I liked the most, is that it's built on Eclipse, but looks and behaves likes IntelliJ. IntelliJ look and feel, IntelliJ keymaps, etc.

I've made a small video, which displays basic navigation and simple refactoring: https://youtu.be/zh2WxNYOFAA (renaming in the JSP file doesn't work yet)

Overal verdict, especially for an Eclipse product: not bad so far!


If they could just make the actual Eclipse look like that, I might enjoy using it again.  Although it still needs a competent HTML/CSS/JavaScript editor.  And if you tell me to just use Aptana, you probably haven't used Aptana.

For now I'm just running Eclipse in the background with "Poll for external changes" selected, then doing all my front end work in Visual Studio, because it's leagues ahead of Eclipse.
Can someone tell a good reason to put an IDE on the cloud?
We're having a fire sale on UI elements!  
We've got side bars, horizontal tab bars, vertical tab bars, buttons that look like buttons, buttons that look like normal text!

We've got drop downs in multiple styles, left to right expanding explorer things, icon menus, mini maps, status bars, normal menus, tiny buttons and big buttons!

We've even got your name in there in case you forget who you are!
Why does an IDE need docker to run exactly?
So anyway, is it really related to eclipse or is it just a "trademark"?

Does it use eclipse backend? How smart its editor is? 
The only Che I have heard of killed lots of people and kept prisoners in work camps, IIRC. Not a perfect brand name.
a web app? could somebody explain to me what is wrong with people nowadays? 
This looks a lot like Typesafe Activator...
Sure, this'll be great on my company's mandated IE10 browser.
How is behind this product? I'm pretty sure that this is not a "just for the fun OOS project". Couldn't find anything in the documentation.
I had a look at the sources and what I found didn't make me happy. It seems to really be a complete rewrite from scratch. Since I'm a developer of an IDE for Eclipse I would like to know some things:

- The editor seems to be completely new and it is written in Java. It looks like it is compiled to JS through GWT. Does this means that it runs completely in a JS runtime? Or is there a split in the editor model, i.e. the document model is kept in Java on the server and the JS runtime only holds the presentation layer?
- The API is probably completely different, not just the one for the editor. Does that mean that we would have to rewrite half of our plugin (or at least all of the parts that access the Eclipse API) if we want to make it compatible to Che?
- I could see a fork or at least a lot of copied sources from JDT. Do you plan to run your own fork of JDT or is there a plan to merge back changes? Why the copied sources in the first place? Is there no way to make JDT compatible to your needs?
- Since I'm a IDE developer I had to fight the Eclipse API for my entire time as a IDE dev. Eclipse just doesn't provide an API that makes it easy to develop IDEs for different languages, it more or less started as a Java only IDE and still is. Every programming language I know that has IDE integration in Eclipse either had to fork or rewrite JDT in order to get IDE functionality. Is cross lang support a goal at all or is the goal to keep it the way how Eclipse did it?
- Git integration seems to call native git. At least I couldn't find JGit integration. Isn't JGit integration suitable to your needs? If not, why not?

There are some other technical things I don't understand:

- In the comments here or on hackernews, it was stated that Che has localhost and docker integration, which probably means that one can run everything on their desktop but always move it to the cloud when there is the need. In that regard I don't understand what makes docker so special. Is it just the container format you use right now or is everything built on top of docker? I'm asking because to me it would make more sense to have an API that abstracts from the storage layer and therefore allows to provide different implementations, where docker is only one of them.
- It was said that Che tries to solve the setup problem of workspaces and it also tries to make contributions easier for everyone. However I can't see how it solves it - to me it looks more like it hides the complexity from the user but former or later everyone has to deal with the complexity if they want to or not. If I want to run a test I have to know which test framework is used. This is even more true for all other frameworks and libraries that are used by a project. Sure, the docker approach may save time to setup the workspace and may allow people from everywhere to just open the workspace in their browser but people still have to know all of the technical decisions a project did in order to contribute something meaningful. Everything that requires a change of the build/workspace/project configuration needs to understand how to do so - which excludes a lot of meaningful contributions. This whole argument sounds more like a marketing argument than a technical one. I can't see how Che contributes anything new to the table - I would at least have expected the definition of some protocols and APIs that allow tool developers to standardize on and encourage them to care for a flawless integration into the API.
- This point is more a general critique of cloud IDEs and not per se unique to Che: What exactly has an IDE to do with the cloud? Users don't care if their stuff is stored in the cloud, in the company network or on localhost. They care about storing their stuff and not to get bothered with it. To me it looks like Che joins - as all the others - the group of cloud IDEs that bind users to a specific cloud instead of giving them the choice of which one to use. Do you provide any API that abstracts away the cloud into some virtual file system or storage layer that (which is similar to abstracting away the docker integration) or do you not?
"We gave up.  We're just copying the VS team now."
Literally no one wants this. In fact, a significant reason Eclipse has managed to cling to relevance is, love 'em or hate 'em, [RCP](https://wiki.eclipse.org/index.php/Rich_Client_Platform) applications. How can this be the next generation when it leaves those behind?
It still looks like an overengineered mess.
There are both Subversion and Got entries on the tool bar.. 
PHP isn't a framework...
I quite enjoy all the Eclipse memes and jokes. I worked with Eclipse for a long time, and I have seen amazing bugs, so I feel vindicated with these jokes. I am not sure we can do the "Mozilla=&gt;Firefox" transition here,  IDE's are huge and slow for a reason, ... IDE's is probably the one thing that require CPU's that don't exist yet, so they do less than what they can do. 


It will be exceedingly tough to compete with JetBrains IDEs and smaller ones like Atom/VSCode and others being developed... Eclipse is just being left behind.
I **really** hope resources that could've been used improving the desktop application didn't went into this...

On the other hand - maybe this will make it easier to use eclipse as a backend from other editors (I know there is/was a vim-plugin project that ran eclipse in the background for instance)
What is the difference between Eclipse Che and Eclipse Orion? They seem to be very similar projects.
&gt; submitted 9 hours ago by **_Garbage_**
So Eclipse Che like the man it was named after aims to impose world wide poverty and revels in torture?
Eclipse Ech - Jontron edition.
I am running the [caddy](http://caddyserver.com/) web server, and they have already integrated to where all sites are served automatically over https using let's encrypt certificates.

Literally zero setup, instant tls in seconds. Yaay internet!
Ran the let's encrypt auto setup expecting it not to work for my 5+ domains or mangle all my Apache configs. 

It worked perfectly and created really clean configs.

I likes it.
For those concerned with the official client requiring `sudo`: there are already many alternative clients that are compatible with the Let's Encrypt server, [mine included](https://github.com/barosl/letsencrypt-simple).

I made my own client because I wanted to know what's exactly going on during the certificate issue process. I tried to make the code as simple as possible, so [take a look](https://github.com/barosl/letsencrypt-simple/blob/master/letsencrypt-simple.py) if you have time! It's a simple single file script.
There was a webclient for letsencrypt, posted sometime back. Does any one know the URL?
I've built a .net client at https://github.com/Lone-Coder/letsencrypt-win-simple that supports iis but it could use simple plugins for this like aws azure Apache if anyone would like to help.
I wonder if this will significantly affect the profits of companies like verisign. Seems like the service will have to be around for a while before bigger corporations will consider using it. 
Do I understand it correctly? Is it free SSL for my website?
I tried it and it seems to work great, except both Safari for iOS and Chrome for Android gave me a security warning when I visited my site.

EDIT: Retrying an hour later and the problem is completely gone. I'm so confused.

EDIT2: I followed the tutorial a friend used and its all better now. I think I was using the wrong certificates, like cert.pem instead of fullchain.pem? I don't know I was just flailing my way through this before. Probably not the best way to go about security.
is it possible to bake one of these into an Oracle wallet? having issues getting the format of the csr and private key out of the wallet to jive with let's encrypt. I was trying to feed it through https://gethttpsforfree.com/ to get some certs back that I can set back into the wallet.

Weird that his example is 

    for(auto it = x.begin(); it != x.end(); i++)
    {
       it-&gt;something();
    }    

When one of my other favorite C++11 things makes this do the same thing:

    for(auto it : x) {
        it.something();
    }
Strongly typed enum was one of the best new C++11 features, imo.
For me, the biggest thing in C++11 is rvalue references and move semantics. So good.
One thing about lambdas:

    []() { }

is not the shortest form of a no-op lambda (ignoring whitespace).
You can leave out the argument list braces when there are no arguments:

    [] { }
Although it is only possible in C++14, some of the examples could be updated.

For example, you can now initialize local variables in capture lists.

    my_func ([x = 4]() {
          //...
    });


Question: In the lambda section, the `mutable` keyword lets you modify variables that have been captured by value.

Since Uppercase is captured by reference, the `mutable` keyword can be eliminated, and the example shown is valid.

Correct?
There's some gotchas with "auto" that this blog glosses over.  Such as, when using C++ 11's "uniform initialization" feature:

    int a( 42 );
    auto b{ 42 };

Here, a results in an int with value 42.  You'd expect b to be the same, but instead, you get a std::initializer_list&lt;int&gt;.

Effective Modern C++ by Scott Meyer (yes, the Scott Meyer) is a great book to get caught up on C++11 / 14.  http://www.aristeia.com/books.html
What is for_each()? Is it part of the language or some macro used by the article writer?
Isn't the functionality described in the article for unique pointers already available by not using the new keyword when creating an object?
`static_assert` without `static_if` is like a screwdriver handle without a screwdriver head.
C++14 is crying itself to sleep right now, then?
    else:
        try:
            raise SystemError from SystemError
        except Exception as e:
            try:
                raise SystemError from e
            except Exception as e:
                raise SystemError from e

Nevermind the political satire, my sides have attained Satori.
    print("You have attempted to obtain illegal material, and as such you have been classified a pedoterrorphile.")
    print("GHCQ agents will arrive at your location shortly to ship you on an all expenses paid vacation to Guantanamo Bay.")

Top lel.
This is funny and on point, most likely, but it would be really helpful to include a link or two in the README.md where people can read about the new law and educate themselves at the same time.
All encryption is breakable, did they specify time requirements?
Is this a joke? Can't tell...
Digital Fortress 
[deleted]
TL;DR: Strings are complex and forcing default behaviour means that anything outside this default gets messy and inconvenient fast. By not having a default, swift makes a tradeoff; Slightly inconvenient behaviour at all times, instead of mostly nice with terrible edge cases.

I can respect that.
Because *strings are hard*. They're complicated in a way that rivals data structures like date times.
TL;DR: Swift is doing it correctly, unlike almost all other languages.
&gt;Incidentally, I think that representing all these different concepts as a single string type is a mistake. ...  I'm not aware of any language or standard library that does this, though.

You can do this via some mechanism more or less equivalent to typedefs in many languages, but what you can't do is enforce some rules for converting them safely.  I think that QueryExpression should be a type, say, and you should only be able to inject stuff into it if it's been sanitized. Then you could be correct by design.   

If anybody could get this right, it would be something like Scala or Haskell.
Because Unicode is a semantic disaster area.
&gt; Rather than include a separate code point for the flag of every country on the planet, Unicode just includes 26 "regional indicator symbols." Add together the indicator for C and the indicator for H and you get the Swiss flag.

So everyone trying to display this shit salad needs not only tiny images for "the flag of every country on the planet," but also code to decode it.  Every time I think Unicode can't possibly get worse, it does.
That article would have been a fair bit better if it weren't relying on my browser to render all these obscure unicode strings.
&gt; Some languages represent strings as a sequence of Unicode code points. Python 3 works this way, as do many C implementations of the built-in `wchar_t` type.

**Stop**

`wchar_t` is 32 bits on Unix but only 16 bits on Windows; thus on Windows it's insufficient to use to carry code points (it will not work with Asiatic characters...). If you wish for a portable code point, use `uint32_t` (and wrap it...).
I'm taking this article with a huge grain of salt in reaction to "Swift has the best String manipulation."

Perhaps for a UI-focused language. If I'm building an app for iOS, yes my UI has to work with every language on the planet and making that actually simpler to deal with can make sense.

But then I get back to the back-end and use some library to parse all the incoming/outgoing JSON and I don't give a flying crap anymore. Now I want an easy default.

TL:DR - different languages are better for different goals
I think "String" should not exist in new languages, or be an abstract parent class. There should be a clear distinction between AsciiString, UTF8String, etc.
The article proves that Unicode is a bad solution for the problem of managing text. Programming languages have a hard time dealing with Unicode because Unicode is too complex.

The main problem of Unicode is that it likes to combine things...combine two code points for producing é, combine two characters for producing a ...flag, etc.

It is stupid...ē, ê, ë, è, and é should be single code points. The swiss or mexican flag should be its own code point. There should be no combining whatsoever, either at codepoint level of grapheme cluster level.

We are making our lives difficult for no reason...
&gt;A design that never ships

&gt;Always runs at zero MIPS.

Simply beautiful. 
This is awesome.  I laughed, I cried, I recognized my life being spent in a similar way.  My wife doesn't really enjoy these kind of war stories much for some reason. So then the guy deletes the semaphore object, then he signals it.....   
This is amazing!

    Hard we won the knowhow to
    Pay the fiendish scheme its due:
    Exorcize it in a final cleansing chop.
    Should it yet more cycles steal
    In a war room sweetheart deal?
    Haunt us more in version 4?  When would it stop?

I may have cried a little here ...
*Completely out of topic:* Whenever I try to read English poems, I fail at getting the rhythm right so I never appreciate them. Can someone sing this poem for me? Please.
&gt; Anirudh Sahni is a software engineer and a former longtime member of the Windows development team. He lives in Seattle and *believes that engineers need not restrict their creativity to engineering alone*.

This is awesome!
Okay. WTF?
&gt;Visual Studio 2015 Support

Awesome
Unreal adds a *lot* of stuff between releases!
Is it a good idea to use this for making my first game? As a hobby.
But i just fownloaded UE4.9!  

Here goes another 5 gigs
&gt; Although it is possible to build a standalone APK containing your application, doing so right now is difficult. If you’re feeling brave, you can see how we build the Stocks.apk in examples/stocks. Eventually we plan to make this much easier and support platforms other than Android, but that work still in progress.

I'd have thought building a final deployable artifact would be the first thing on such a frameworks todo list :/
This is basically a clone of Qt Quick without all the features. Qt Quick also has a native fast OpenGL rendering engine, is cross-platform (with desktop as well), uses reactive ui techniques, is declarative, uses javascript and has way more functionality built in.
This looks quite promising. What is the hivemind's opinion of Dart as a language? If we can pull off native iOS level performance with this while also being able to compile for Android it could be a big game changer for building Minimum Viable Products.
Is this a follow-up to the [Sky-project](http://www.androidauthority.com/google-dart-sky-android-apps-605970/) or something completely unrelated? 
Would love to see pics of widgets, and code. Couldn't after 30 seconds of clicking.  Fusetools is in the space too (and makes code and resulting apps easy to see). So is QML ([v-play.net](http://v-play.net/) have plenty of examples). I'd love to see [node.qml](https://github.com/trollixx/node.qml) gain traction though - the holy grail for me.
It's a shame Dart looks like it is mandatory. Pretty much dead on arrival I think. Happy to be wrong.
I can't stop to wonder if they are not reinventing haxe + openfl + haxeUI?
This looks like yet another alpha stage product from Google that may or may not ever be finished. I wish they would stop shipping incomplete stuff. It may feel great to the devs but they haven't really accomplished anything yet.

Consider that there are only a few widgets, no API documentation, no iOS support (even though it's supposed to be cross-platform), can only build a standalone app if you are "brave" and doesn't support development on Windows.

Have the devs at Google not heard of the 80/20 rule? I'll bet this project, if it ever reaches 1.0, will take another 2 years to get there.

Google, why do you do this? Do you not realize that we are on to you now? We have been burned enough times to make no assumptions at all about your announcements. They are too often just empty words. Please wait to announce a product until it actually can do what you are claiming it can do.
For those of us who hate Java, Swift, and JavaScript, this is awesome.
&gt; To get started, we need to set up Dart SDK

...and 90% of the would-be developers stopped reading. 
This looks great. The Dart ecosystem has really matured over the last year. Great tooling with the focus on Dart Analyzer in WebStorm / Atom / Sublime. Angular 2 Dart is look damn rock solid. The Redstone MVC framework continues to improve, the Shelf middlewear packages got a large quality update, and the language itself has really started to look spectacular with the async / await support. (no more callback pyramids) 

I really don't understand the negative reactions to Dart. Google has provided tremendous resources to the language and platform, and it's exciting to see Darts future getting some clarity in the mobile space. 
In 18 months they will shut it down with a weeks notice.
The syntax is react copied 1:1. I have to say it is a shame they did not do jsx-like syntax.
I think jsx-like syntax is much more readable for complex UI trees than regular function nesting.
Oh boy this looks good

&gt; Getting Started with Flutter  

\*click\*

&gt; Install the Dart SDK:  

\*close tab\*
I don't understand why this was not a Typescript project and/or is not leveraging Angular.

Ionic is already a fairly proven platform utilizing Angular and it is quite well suited as a mobile application framework. I know nothing about Dart and whether it is bad/good, but I think it already had its window of opportunity and a lot of people passed on it. The Angular team spent a lot of time trying to maintain Dart support in Angular 1/2 and I've never encountered anyone who uses it (not to say there's anything wrong with it). 

However, contrary to many of the commenters, I don't think the fact that there is little documentation/no standalone apk bundler available is a problem. It is an alpha...it's not as if you're going to distribute the APK to users anytime soon. 
Dart go home - nobody wants you.
Awesome.  I see you've done the first part of making a google library or toolkit:

- Unnecessarily reinvent the wheel with no improvements instead of contributing to the obvious existing OSS project that does the same thing.

What's the google step two with this one?:

1. Abandon it next year
2. Make a new version in a year that completely breaks everything
3. Fail to update it while continuing to insist that it's supported and relevant


&gt; Flutter uses Atom as its IDE

Uh oh, get prepared for everyone to LOSE THEIR MINDS.  I just got done uninstalling the latest Intellij fwiw though, it just had the fan blowing 100% all the time and was always choppy and slow.  Just no excuse for that on i7 8 gig machine.  Back to Eclipse.
It looks like another move against Microsoft.
Well, the GitHub link doesn't work... :(

And browsing github.com/Microsoft, I found [vscode-distro](https://github.com/Microsoft/vscode-distro), whose README says:

&gt; DO NOT MAKE THIS REPOSITORY PUBLIC

Uh oh...
[deleted]
Has anyone started a vscode vim extension yet?
Great. Tried a few weeks ago and it was really nice, specially beacuse it feels so light and fast, compared to e.g. Atom or full blown IDEs. It being closed-source was the main reason I ditched it.

With a parinfer extension and some maybe REPL integration it might make a great lightweight Clojure editor.
If I configure code as my git editor it doesn't seem to integrate correctly.

    [core]
      editor = 'c:/program files (x86)/Microsoft VS Code/code' --new-window

e.g.

    &gt; git add .
    &gt; git commit -e
    Commit rejected: &lt;msg from my git hook&gt;

The commit is rejected before code has had a chance to start up, let alone waiting for the code instance to exit.
~~Checking for updates with 0.9.2 does nothing. Does it require a manual update (again)?~~

**Edit:** It auto-updated after leaving it open for a while. Guess there is some throttling in place.
Is this any useful for writing regular desktop apps with c# under linux or is it just for asp.net stuff?
The sqrt one has a feel of smoothsort leonardo heaps: http://www.keithschwarz.com/smoothsort/, which I implemented in D here:
https://github.com/deadalnix/Dsort/blob/master/sort/smooth.d

It still differs in some interesting manner.

The complexity looks interesting, but it doesn't run that fast in practice, as the memory access pattern is just terrible compared to something like TimSort.
Second one is [here](http://forum.dlang.org/post/n3ifba$ulq$1@digitalmars.com). I'd appreciate thoughts and pointers to related work!
Looks like a variant of Fibonacci heaps.
There's a folklore data structure that kind of achieves sqrt(n) lookups and inserts. I vaguely remember a short report from INRIA (?) about it.

Take a sorted linked list. Assume you can sample nodes randomly (e.g., they're embedded in an array). Choose sqrt(n) random nodes without replacement, and start a linear search at the most promising node. The total expected search time is sqrt(n). Insertion is lookup + splice.

You can also handle deletions if there's a guarantee on density (e.g., at least 50% of the elements in the array aren't deleted).
It will be O(sqrt(N) log(N)) to insert into this data structure. Consider the scenario where you insert an element smaller than everything already present. This will displace the first heap (which is size 1). You'll have to insert that element into the next heap (which is size 4), which will cause it to spill over, and so on. All in all, you will need to perform a constant number of operations on every single heap. This results in an expression in the form:

    \sum_{i=0}^{sqrt(N)} log(i)

since heap insertions are logarithmic in the size of the heap. We can bound this above and below with integrals, so to get the O notation answer, we can consider:

    \int_{x=1}^{sqrt(N)} log(x) dx

The integral of log(x) is x * log(x) - x; substituting sqrt(N) for x yields something proportional to sqrt(N) * log(N) as N becomes large.

Edit: corrected the integral of log(x) from xlog(x) to xlog(x) - x; I mentally dropped the term since it doesn't affect big O and wrote it incorrectly.
[The site is down!](http://www.downforeveryoneorjustme.com/forum.dlang.org/post/n3iakr$q2g$1@digitalmars.com)

It seems that the forum is not reddit scale. Let's rewrite it in rust.
Happy Birthday!
The random slide is quite interesting. You could also sample the random numbers per a distribution that has a higher probability in the middle of k numbers to bring it more toward the middle each time or even skew it closer to the front. 

You could also keep track of the number of times an element has been accessed and swap elements accessed more often closer to the front as you search. Keeping track of this would also let you check adjacent elements of the randomly selected one so as to choose the least often accessed one for swapping.  

Actually there are a lot of additional optimizations that you can get by counting the number of accesses to an element.
It vaguely reminds me of quadtrees - one, then four, then 16, then 64, etc.
What's the advantage of your method over, say, a quadratic search algorithm? Say, maintaining the list sorted, and then just checking every sqrt(n)th element, then searching within the appropriate sublist.

I guess *advantage* is not really a meaningful term here....
Further in the linked thread there is a question on how to find k smallest elements of an array in randomized linear time - you can do it with modification of quicksort algorithm, which roughly goes as follows.

First, choose a pivot element and partition around it, as usual. After that, you know that all elements on the pivot are less than it. Let's say that pivot has index i. We have three cases:

1. i = k, you have k smallest elements on the left of the pivot
2. i &lt; k, you have i smallest elements, recurse on the right part of an array searching for (k - i) smallest elements
3. i &gt; k, you have i smallest elements and k smallest elements are obviously among them, recurse on the left part of an array

(there are some off-by-one errors in an algorithm above, but you probably get the idea)

In "the best" case, i would be in the center of an array, and overall complexity would be equal to sum by i of n * 2^(-i), which is O(n).
Here's an algorithm to compete with the second one. I don't know if it actually works well, but some tiny examples by hand makes me think that it might for geometrically distributed data (precisely the kind that this tool is useful for).

The aim of this algorithm is to form an array of values approximately sorted by access frequency. The algorithm requires an `n+k`-length array where `k` is some small count of sentinel values (eg. `log2 n`). I'm not yet totally sure of how large `k` needs to be, but it's surely not large.

These `k` slots are taken by a sentinel value. If you are unable to us an actual sentinel value, it's simple to just use a length-`k` secondary array to hold the positions of these. This secondary array will stay sorted since these sentinels never move past each other, so this is cheap.

Start your array as so:

    _ _ 1 2 3 4 5 6

with the sentinels in the "most frequent" side of the array. Lookup is a linear search, and when it finds the value it moves it past the most recently seen sentinel with 3 swaps. A search for 5 would thus give:

    _ 5 _ 2 3 4 1 6

The other key is that when moving a value puts the sentinel next to another sentinel (or the end of the array), swap that sentinel instead down to the left. For example, searching 5 in

    _ 2 3 4 1 6 _ 5

gives

    _ 2 3 4 1 6 5 _

which then goes to

    _ _ 3 4 1 6 5 2

Note that this strategy does not prevent pairs of sentinels in general, but that is OK.

I want to do some tests of this idea (and the others here) with some geometric data to see if it really does work as intended.
**Edit: I did a more thorough analysis [here](https://florianjw.de/en/andreis_container.html),** ignore the stuff below.

@/u/andralex : Highlight, so you see this update.

----
Let's call these arrays buckets.

`n` Buckets can hold `\sum_{i+1}^{n} i^2 \in O(n^3)` Elements. That implies that we need `O(\sqrt[3](n))` Buckets to save `n` Elements.

Since the size of a bucket is in `O(\sqrt{2})` And we can do a binary search to find the correct bucket, finding an element is in `O(\log{\sqrt[3]{n}} + \sqrt{n}) = O(\sqrt{n})`

Insertion: We find the first bucket for which the new element is not the largest one, insert it there and extract the largest. If `m` is the size of the bucket, that is an `O(\log{m})`-operation. The we take the extracted element and go to the next bucket, where it will be the smallest and repeat the process. We can expect, that we will have to go through `O(\sqrt[3]{n})` Buckets, a sufficient amount of them holding `O(\sqrt{n})` elements. This would imply a complexity in `O(\sqrt[3]{n} \times \log{\sqrt{n}})`.

~~Deletion: A lot worse since we have to do a linear search for the smallest element in each bucket, implying a runtime in `O(\sqrt[3]{n} \times \sqrt{n}) = O(\sqrt[6]{n^5}` which is barely sublinear. I am however positive, that there are ways to make the deletion of multiple elements at once a lot faster.~~

~~Creation: This one is interessting. At a first glance we might assume that this is basically just quicksort but stopping earlier followed by `make+heap`-operations (All of those will be in `O(n)` for the entire sequence, so we can ignore them for the start, since the partial sort cannot take less time). The largest bucket will have a size in `O(\sqrt{n})` implying the existence of `O(\sqrt{n}!)` permutations, while the whole array has `n!` possible permutations. Using a binary comparator this implies that we need at least `\Omega(\log[2]{\frac{n!}{\sqrt{n}!}})` comparissions, which even [Wolfram alpha](http://www.wolframalpha.com/input/?i=log_2%28%28n!%29%2F%28sqrt%28n%29!%29%29) doesn't manage to simplify.~~
Awesome data structures idea. Nice to see you active or Reddit Andrei!
Oh I hope you don't take the same approach for rolling your own crypto ;)
&gt; The Visual Studio editor now provides built-in **syntax highlighting** and basic **IntelliSense** support **for** languages including **Go, Java, Perl, R, Ruby, and Swift**. We support the TextMate bundle 
model for language grammars and snippets, allowing you to extend this with support for other languages.

http://blogs.msdn.com/b/visualstudio/archive/2015/11/30/visual-studio-update-1-rtm.aspx

Unexpected and welcome addition.
I recommend downloading the .iso
https://www.visualstudio.com/en-us/downloads/download-visual-studio-vs.aspx

Web installer freezes for me with 0 disk/network activity.
&gt;Visual Studio license improvements. Signing in to unlock the IDE with your subscription is one of those features that you do not want to see interrupting your workflow. Towards this goal, Update 1 has improvements that will ensure the IDE stays unlocked for a year or more after signing in as long as you have regular access to the internet to keep the license renewed in the background. More improvements to reduce sign ins are still to come.

Today is a good day.
C# REPL and scripting sound cool
Thank mr microsoft for giving us a different icon from 2013.
"Go to definition" is the one I was waiting.
EDIT: "Go to Implementation", my bad as /u/bananaboatshoes pointed out.
Does anyone else experience lagging\non-responsiveness when starting up visual studio?

Takes maybe between 5 - 10 minutes to free up and start working.. 
There is something about clang support? 
Guys can someone tell me if the ISO contains Universal App SDK and Emulator? It seems everything you have to download seperately, how come 4GBs ISO doesn't have Windows SDK and Emulator?
BTW, Microsoft, **please** fix "Go to Definition" from C# code to F# code which does not work, and instead shows me the stupid autogenerated metadata, for *my own code inside my own solution*. OMG...
Does it have auto-sized enums yet?
Anybody having problems with razor views after updating? I installed the update this morning and my razor views were not working, had to reinstall the visual studio.
This release also should fix the issue when deploying silverlight apps to Windows 10 Mobile.

Source: http://blogs.windows.com/windowsexperience/2015/11/18/announcing-windows-10-mobile-insider-preview-build-10586/
I'm reading stuff about 'get' and 'properties' under the C++ section and can't find any mention of the '/experimental' flag that was mentioned in the video a week ago.
Good job on requiring me to restart the OS after applying an update to an IDE. Feels so 2000.

On a positive note, the update itself is nice.
Hm, it links to the NuGet 3.3 release notes, which aren't available yet.

EDIT: They're up now.
This release also fixes a nasty issue where you couldn't deploy apps to the Lumia 950 and 950 xl.
"Almost every application that is being made ends up lagging in schedule, regrettably" - So not much has changed in 42 years then!
&gt;[projected deadlines] end up being far too optimistic

phew, glad we got that all worked out 

good job, everyone
So this is how Linus grew up.
"Is computing the field of future? No, even though they always claim so".
They all sound like they've been made to read out a statement from their kidnappers. Maybe that's just Finns?
I never learned Finnish.  I guess things have changed a little bit.

&gt; making the program continues even after finishing it
&gt; "The work is very tiresome and stressful"

Thank God we are way past that.
Bet you a nickel customers haven't changed since the 70s.
They seem to hate what they do :(
Wow, can you imagine actually meeting customers! We've come a long way!
Fuck.  It won't ever get better.  Well, at least I didn't have to learn Finnish.
not so much changed in 40 years, or the subtitle is fake :)
The first guy looks like a serial killer
As a non-Haskeller, what is strict Haskell and how is it useful?
I dunno, I already had a strict haskell called Idris...
How the heck did they implement that with ~500 lines of code??? (excluding documentation)
I thought the opposite of lazy evaluation was eager evaluation, not strict evaluation.
I-it wasnt strict enough?
Still a newb to haskell, thus this question: Will that flag propagate to other prebuilt "dependencies": i.e. forcing a rebuild?
Sounds like this would be useful during debugging too.
Is it going to be available in the next release of GHC?
neat syntax 10/10 would follow along vaguely again
Man, I'm totally using that quote with "attitude" swapped with "socialism" from now on.
Every LinkedIn article or quote is always terrible.  It always reeks of that really shitty advice you got in high school from a counselor whose greatest accomplishment in life was becoming a public high school counselor.
why don't you make a "stop being creepy and stealing my contacts from gmail even when I say not to" program for linkedin?

Lol, during my PhD I used the kuzmin-kutuzov distribution function. I just knew there was something about it : )
&gt;insecticide

I knew it.
Equating whole numbers with percents? There's just so much wrong with this "motivational" quote
I guess they aren't encoding in ASCII
The mathmetician in me is frustrated.  Nobody write 'K+N+O+W+L+E+D+G+E'.  We write 'KNOWLEDGE'.  It's value should be 11*14*15*23*12*5*4*7*5 which equals 446,292,000.  'ATTITUDE' on the other hand would be 30,240,000.  So by my calculations knowledge is greater than attitude.  

The chemist/physicist in me though says "yeah, what are the units?  Whatever they are, they suck for this comparison."

The tired old man in me just says "stupid quote!"
&gt; but breaking a batch of a million 160-bit ECC keys costs a total of just 290

I read that article, but I'm still not clear on something.

Is this a batch of arbitrary keys, or a batch of keys that are "similar" somehow.
TDD is fucking hard. For me personally and a lot of developers I work with it does not mesh with the natural habit of most programmers.

That being said, I am fully on board with a good TDD. It is so incredibly useful, and when we are in a good flow (we pair program) we only end up writing the bare minimum of code we need.

The thing I struggle with the most is writing worthwhile tests. I think it is just one of those things that come with practice.
Doing lots of planning ahead doesn't usually work nor does doing no planning ahead. There's some sweet spot in the middle. I don't understand agile advocates that promote the minimal amount of planning ahead possible.
EDIT: sorry, was meant be a reply to [/u/Euphoricus post](https://www.reddit.com/r/programming/comments/3ssg0s/mindless_tdd/cx049jy)

&gt; That is why the 3rd step exists : Refactor.

No, the article covers why this is not the saving grace, and I happen to agree with it:

&gt; we end up with a load of tests that are coupled to the implementation

If you have to refactor your tests when you refactor your code, *your tests are effectively worthless*.

A big problem is that people don't understand what a "unit" is. A unit doesn't mean a single class or a method, it means a use-case or behavior that can be tested in isolation. It's a "unit" of the public API of a &lt;thing&gt;. If that &lt;thing&gt; encapsulates 800 classes, 6000 methods, but exposes just one behavior, guess what? Your unit test should only test that one behavior and not let the implementation details of all those classes and methods leak out into the test. If they would leak out into the test, they would leak out into an application, which is precisely the problem that designing through TDD aims to avoid.

TDD and unit testing was *never* about testing the smallest pieces of code, it was about testing the smallest independent pieces of *behavior*, regardless of how much code might be needed to achieve it.
That is why the 3rd step exists : Refactor. Many people forget about this step. And one of the important parts of refactoring is removing unneeded complexity. I'm sure with proper refactoring, the Pin class would soon be removed while tests are being written.

The problem is not people not doing TDD properly. The problem is people not refactoring properly.

And you built a strawman by saying that it would take 15-20 minutes to build that code. Even novice programmer would take only 5 minutes to do that, including booting up VS.

Next thing to point out that your examples are already well known. You know what the whole algorithm is going to look like. What inputs and outputs the algorithm will have and all the edge cases. This makes it easy and safe to say "lets start with little more complex first test, so we speed things up". But real world is not so clear. In may cases, you have only fuzzy idea about all the inputs and output, and rarely know any of the edge cases. Starting from simplest input and output and building on top of it is easiest way to reach good solution. And building on top of what I said about refactoring. It is not uncommon to completely rework the whole algorithm as you write more tests and add more use cases. From my experience, it is rare for me to end up with relatively same kind of code I had in the beginning.
&gt;He said he wouldn’t be able to help. He would only slow us down. The team (us) seemed to know what it was doing and simply needed time to get the work done. A project this ambitious was inevitably going to hit some tough moments. The Board would have to accept that.

Hah. That's the most polite way I've ever heard anyone say, "Welp, you're fucked, and I want no part of this sinking ship."
I will gold anyone who posts a similarly sized writeup, told from Pranab's POV (ala Ender's Shadow), about how he was secretly keeping the whole project together and sticking up for Lawrence, his personal hero and role model, the entire time.
&gt; Then came what can be described as the Big Pivot. Hinton, Milburn and Zach (our lead sales guy), decided we should get rid of the Android app and switch to a pure chat app. We would now use Natural Language Processing for everything. Instead of the user clicking a button to edit old Contacts, they would type, “I want to edit the info about my contact Jenny Hei,” and we would then find the info about Jenny Hei and offer it to the user so they could edit it. That was the new plan.
&gt;
&gt;This was a brilliant idea.

You know, there is subtle but important difference between "*is* a brilliant idea" and "*seems* like a brilliant idea."
This is why I would never want to be in a position of responsibility in a tech company where business people are calling the shots on technical decisions.

Traditional wisdom for salespeople and managers is that the best managers and CEOs can move mountains by sheer force of will. That's why they get the 7-figure salaries. They can wrangle the people, rally morale, send the memos, pass down the edicts, silence the naysayers, and lay off the dead weight until their division or company is profitable.

Anyone who tells them "no", or gives less than 110% because they have become disgruntled or complacent, gets replaced with someone who will tell them "yes."

And this works in industries where people are building things on an assembly line, moving things from place to place, or filling out paperwork. Any person, no matter how experienced, can be replaced with several less experienced people who are willing to work hard. As cruel as it can be to some individuals, it works, and makes many companies successful, creates many jobs in the long run, and pays dividends to the investors.

But it doesn't work that way for industries with any creative element. Picasso could not be replaced by 100 art students. The senior engineer who has architected a dozen successful products cannot be replaced by 100 CS interns. 

Good ideas and solutions appear on their own to a mind that is free to explore possibilities and trained to recognize them. They vanish when someone is shackled to a singular task or purpose. Intimidation cannot force people to come up with good ideas faster. Many bad ideas are no substitute for one good idea. And the people who can tell the difference and then *make them a reality* are orders of magnitude more valuable from people who don't yet have the experience or talent for it.

Any manager who doesn't understand that this is the nature of software development will always fail, because all of their instincts for how to make a business productive will betray them every time. The more they try to make their business succeed through traditional methods where employee "effort" is the only currency that matters, the more irrational they will appear to their subordinates who simply can't think of and implement solutions any faster than they already are. And as a result, they will drive all the best human capital out of their company, and their capacity to build great software along with it.
Why some people choose to stay in toxic environments like this will forever be a mystery to me. 
"We spent 20 minutes talking about whether a user should type yes or Yes to confirm an edit."  


Story of my life right there....
Sounds like Milburn is not man enough to admit he has failed.
There should have been a spec from the start. Also, how do you look at that and not see a too complex project for a 3 man dev team? It's not gonna happen in any reasonable timeframe. 
 
The signs were there, this wasn't going to work, not consulting the actual team for big changes, no expectation of estimates from the start also are a huge red flag. 
  
Investors don't just give money away and expect results "whenever they are ready", you certainly expect a ROI during a certain timeframe. The fact that no delivery date discussion ever happened should have alerted everyone.

YIKES.  The first time that that board of director guy called me I would have been out of there.

The software market in the US for experienced devs who know what they are doing is far too good to put up with micromanaging idiots like that.  
The real problem here is a small team taking on a stupidly ambitious project.

They should have insisted on at least releasing the first Android app before working on anything else.
Author, interestingly enough, never does blame himself for anything more than having produced some bugs here and there, so I'll help him.

1) Joined a startup where nobody involved had done this before.  A 22 year old "CEO" who is providing neither money, technical expertise, a personal relationship with the team, zilch.  

2) Joined a startup that was self-friends-and-family funded for a huge amount.  $1.5MM is too much to seed a team of three (even though this concept needs a mature, experienced team of 20) and is a huge warning flag that none of the early investors have any idea what they're doing.  When it comes time for a real series A, no fund is going to want to be coinvested with a random clown car that has that big a stake.

3) As a technical team lead, did not display an ability to consider business impacts of technical decisions.  I don't want to get into a holy war, but using Scheme or Lisp or Scala or whatever totally beautiful and perfect language that your last enterprisey job wouldn't let you use for your startup is pants-on-head dumb.  You know what I can hire tomorrow?  Python, Java, .NET, Objective-C.  Bonus points for those people can (relatively easily) learn each other's stack and meaningfully contribute to reviews, etc, assuming there is an a priori reason to actually have three different languages used by three different developers.  It sounds like Pranad was not a strong developer, but maybe his effectiveness was further limited by the fact your side of the application looked like deep black magic to him?  

A quick moral I guess for developers (especially those w/o work experience or in college): If you're going to join a startup, join one that has a team that's done this before, and where leadership does not consist of self-described "vision guys".  Ability to execute trumps cash, contacts, and code -- every time.
Any suggestions why "Pranab" wasn't fired?
&gt;A line such as, “But I guess that doesn’t interest you, does it?” is never meant in any literal sense; it is always thrown out there to show that the person speaking has the power to throw it out there.

I don't know why, but I am completely allergic to this stuff. I've parted ways with long-term friends over single instances of lines like this.

Just yesterday I almost got in a physical fight with a teammate on a class project over something like this. I'm still shaking with rage.
Thank you for this. It was a wonderful read.
Maybe I'm stupid, but I don't see any need for a Finite State Machine. Why not just store incomplete requests in a persistent database, iterate through the fields, and send a message for each field that the user hasn't specified yet (or specified with errors)?

I'm guessing there must be more complexity to their app, but I feel like if they're maintaining their state machine by hand, they're doing it wrong. Especially in a Lisp.
I'm not sure what the point of this post was—other than to make the author feel good about getting shit off of his chest.

Airing dirty laundry is never a good way to endear yourself to future employers. 
If anyone else is curious who the company is, I'll save you the first step. The author scrubbed them from his linkedin page. There's a conspicuous gap of 2015 work experience.

EDIT:  Found it. Google has cached an earlier version of his linkedin page and uses that earlier version for the search preview. The author's clearly trying not to incriminate them, so I won't mention the name, but if you understand the edit you should be able to find it with little problem. 
Anyone else take issue with the tech lead not knowing each applications language and tooling? What kind of tech lead are you that waits until integration to find out one app is written in clojur one in Java and both using different libs?

Even if you expect your team to be experts in thier domain your job is still to look over thier shoulder from time to time and ensure things are going welll. How did you not recognize in the first month that a w3 year old college grad was writing bug filled code! 
&gt;Instead of the user clicking a button to edit old Contacts, they would type, “I want to edit the info about my contact Jenny Hei,”

Started laughing about here. Didn't stop.

What a dreary piece of half starry-eyed, half Microsoft-bred corporate horseshit this is. It's like any book by Jesse Liberty.
What happens? Gigantic wall of text aside.
This Milburn character sounds like he's got narcissistic or borderline personality disorder.
The author also also pretty whiny and very naive. I say that because I also have this naivety. You will always lose in such situations if you straight talk, be honest and direct and show your cards. Blaming others always makes you look like a loser, regardless how true it is. And maybe this Pranab guy had some connections to someone on the board. 

Lesson 1: Don't blame directly, make superiors see it. 
Could have made a presentation/demo the author knew would fail in Pranabs code (and knew where already). During the demo, in which of course Pranab, Hinton and Milburn must be present, you intentionally trigger the error. They get mad. You do something that looks like debugging, then calmly call over Pranab and show it happens in his module. Ask him to fix it. Of course it should not be easy to find and fix. After 5 minutes, jump and and fix it yourself. Makes him look incompetent and you like a savior. Rinse and repeat. This will form a mental image of both of you in the other guys minds. If you want someone fired, trap him.

Lesson 2: Always have enough money on the side
So you can walk away any time and keep your standard of living for at least half a year.

Why? That you don't have to tolerate Milburns shit. Keeping him connected over conference or microphone? WTF? Just cut him off. Blame it on a bad network. He will have his doubts, but that's exactly the point. If you get fired about it, at this stage of the project would be his problem.

And the thing about not calling him. He repeating the same sentence 10 times. Just walk away man, If I get disrespected, I disrespect you. The whole conversations sounds like a woman talking to a child. Never ever explain yourself. That's what he wants. Explaining yourself = you are at fault. Regardless if true or not.
Or you could just say. "Do you want to fire me? Then do it or stop harassing me and stop you bullshit. What is it?" Now he is under serious pressure because I'm pretty sure he isn't that dumb and knows the project will be even later or fail if you leave. If he doesn't fire you, you are now actually holding the position of power regardless if he is your boss.

That's why you need savings (or balls of steel). The greatest power of a man, is the power to walk away at any time.

Lesson 3: Don't do unpaid overtime

This is it. Don't do it. Given the circumstances salary was probably also crappy. Now this is hard in a startup and depends on what the promise you in shares. Personally I would never go for shares. They are overestimated. Most startups will fail and it is a weapon to make you work harder. If they don't offer a proper wage for proper hours, look somewhere else. It already means they don't value you enough and probably don't understand the technical complexity required. Also this is directly related to Lesson 2. If you are not afraid of getting fired, they can't guilt you into overtime. It puts the pressure back on management. Replacing someone in a small time is much harder and more painful than in a large team /company. I don't think they will actually fire you, IF you are actually competent. And if they do, it probably was a bad place to work anyway.
&gt; whereas all our other apps were written in Clojure

&gt; The number of errors were endless: cast errors, regex errors, dependency issues and more.

Maybe they should have used a language with types instead?
Sounds like fiction but I can belive this is how things can turn out. The thing is that if everything is good and profitable everyone is happy (not overly happy thou). If things don't go as good as previously even with more effort put things go sour... Business people are ruthless, will delight you one day, and chew and spit you out next. To be a good manager means you need to be able to use people and be good with them on the other hand.. 
Sadly, I see both Pranab and Hinton in my personality all while Feeling the rage and disgust of Lawrence.
&gt;  I’ve skipped over most of the computer programming details

Should this post belong here?
Begins to panic? Board of Directors are always panicking or pretending to panic to manipulate their charges, like movie directors.
That's a lot of hot clown on clown action there...  Insane inept management, slapdash "engineering", non existent planning at any useful level.  Yikes.  
Fiction?
It sounds like they had a lot of problems obeying API contracts.  Instead of JSON maybe they should have used something like protobuf where the message schema is enforced? No idea what the protobuf support in clojure is like though.

That's but a small problem though, it sounds like there are all kinds of problems with that company.
That's the size of maybe 2 or 3 of Stephen King's short stories. Without spending more than 2 minutes glancing over it, I am certain that reading 2 or 3 of Stephen King's short stories is going to be more pleasurable _and_ more useful in the short and the long run.
From the /r/rust thread:

The goals of Redox are to create a useable, secure, Unix-like operating system. Redox is built on top of a microkernel-like collection of "schemes" that provide seperate file namespaces for everything from networking to audio to windowing. Redox is not a pure microkernel, but very close. Each scheme may accept and handle a number of messages, including the standard open, read, write, seek, close file syscalls. Schemes are isolated, so crashes in one scheme will not affect the operation of others or the kernel.

In Redox, everything is a file with a scheme attached to indicate the protocol being used.

* Windowing is achieved by opening window:/x/y/width/height/title, which returns a window file where pixels can be written and events can be read. Windows will eventually be moved and the title could be changed using the rename syscall
* TCP connections can be started by opening tcp://10.0.0.1:80, for an outgoing http connection to 10.0.0.1 for example. A listener can be binded with tcp:///80. Reading from the fpath syscall will return the remote address and port upon a connection, like tcp://10.0.0.1:32634/80
* Audio devices can be used by opening audio:/device/sample_rate/bitrate/channels. Reading will record sound, writing will output sound. Nonblocking reads and writes will eventually be supported. Supported arguments, sample rates for example, could be found by doing a readdir on audio:/device/

Rust was chosen as the kernel language and the preferred userspace language (although it is not required to be used in userspace) because its design makes kernel development very expressive and, in many cases, safer.

There is a degree of posix support, we are using a hand-crafted libstd that directly calls the Redox syscalls, many of which are ABI-compatible with Linux. (lrs reminds me of our work on libredox, but lrs is probably more feature complete). We also have a newlib port for C program support. So far I have ported SDL, zlib, libpng, freetype, lua, and I am working on tar. The first screenshot shows Droid Sans text being rendered in an SDL program.
Roughly how much is `unsafe` code? 
awesome! this is a great thing. as long as the OS is still written in C, C/C++ will continue to rule systems programming. an OS running in Rust will be necessary for Rust to eventually take over this space
An honest question from a UNIX fan: is UNIX really state of the art of the OS design? I would hope there have been new ideas worth exploring, beyond UNIX.




Here is the github repository: https://github.com/redox-os/redox
This is a crosspost from /r/rust: https://www.reddit.com/r/rust/comments/3sm4oq/redox_is_serious/
What niche is this OS supposed to fill?
The next TempleOS?
Time to fork the language. I call this new Swift. Swift++
I wonder how many people will just add in some operator overloading to get them back.
&gt; Swift += operator cannot be used in-line, since (unlike C) it returns Void.

What's the reasoning behind x += 1 returning void in Swift?

This entire Reddit thread strikes a solid 1, possibly even a 2, on [the Wadler scale](https://wiki.haskell.org/Wadler's_Law)
`s/too/to/g`
In C++, there is the concept of a forward iterator, which can be increased by one but not by an arbitrary amount. That is, it implements the ++ operator, but not the + operator.

Of course this can also be implemented without using operators, but it's worth noting that ++ and += 1 are not interchangeable in all contexts.
why? I read the reasoning, specifically the disadvantages of the operators and i'm wondering why? the reasons seem kinda meh for just removing something from the language which I thought would be a big deal.
I'm not a super duper programmer or anything but I'm curious.
Based on some of the logic in the argument, I propose threading / dispatch switching should be removed because beginning users might get confused by it.
I bet that it will generate a lot of swearing. I see no real advantage of removing it.
Last time I wrote Swift I didn't use these operators at all. As a matter of fact, it's been a while since I used them in any language. Hardly bothers me either way.
I'll just remind people of stupid interview questions like this:

&gt; What is the result of this program:

&gt;     #include &lt;stdio.h&gt;
&gt;     int main() {
&gt;         int a = 5;
&gt;         printf("%d\n", (++a * ++a));
&gt;         printf("%d\n", a);
&gt;     }

The correct answer, for what it's worth, is "a failed code review". The other correct answer is:

    stupid.c:4:18: warning: multiple unsequenced modifications to 'a'
      [-Wunsequenced]
        printf("%d\n", (++a * ++a));

 
But if you answered this:

    42
    7

Then you're technically correct, the best kind of correct.
Hmm... I haven't seen any discussion on the origain advantage of the "++". This used to be an interview question... The ++ (in C) would get compiled to an auto-increment machine instruction, the others may or may not. Unfortunately, that level of programming, for the most part, has gone away with more powerful CPUs. But, it is still valuable to understand how they work...
Did Guido Van Rossum move to Apple, or something? :)
There reasons can be pretty much said just for about any language feature. Belowe I give a sarcastic example.

&gt; These operators increase the burden to learn Swift as a first programming language - or any other case where you don't already know these operators from a different language

Lets remove operator as it increases the burden for learning swift as a first programming language. or any other case where you don't already know these operators from a different language (e.g. objective-c, java)

&gt;  expressive advantage is minimal - x++ is not much shorter than x += 1.

Same with operator overloading. x+y on not much shorter than x.add(y).

&gt; Swift already deviates from C in that the =, += and other assignment-like operations returns Void (for a number of reasons). These operators are inconsistent with that model.

WTF? this is not even a disadvantage or even related to the issue at hand.

&gt; Swift has powerful features that eliminate many of the common reasons you'd use ++i in a C-style for loop in other languages, so these are relatively infrequently used in well-written Swift code. These features include the for-in loop, ranges, enumerate, map, etc.

ok +1 for this

&gt; Code that actually uses the result value of these operators is often confusing and subtle to a reader/maintainer of code. They encourage "overly tricky" code which may be cute, but difficult to understand.

 Code that actually uses operator overloading  is often confusing and subtle to a reader/maintainer of code. They encourage "overly tricky" code which may be cute, but difficult to understand.

&gt; While Swift has well defined order of evaluation, any code that depended on it (like foo(++a, a++)) would be undesirable even if it was well-defined.

While Swift has well defined order of evaluation, any code that depended on it (like foo(a+b, b+a)) would be undesirable even if it was well-defined.

&gt; These operators are applicable to relatively few types: integer and floating point scalars, and iterator-like concepts. They do not apply to complex numbers, matrices, etc. 

operator overloading are applicable to relatively few types: integer and floating point scalars, and numerical-like concepts. They do not apply to views, images, constraints etc. 

&gt; Having to support these could add complexity to the potential revised numerics model

Ditto for operator overloading.

With that said these are nearly the same reason Java doesn't have operator overloading. Just for kicks operator overloading will only be defined for `String` type to do concatenation but will not be overrideable to prevent misuse and unintuitive abstractions. 


Good move. Also += -= and such create a silent cast in the C family. It's a bit ugly when you think about it.
The list of disadvantages sounds like someone trying to convince themselves. #2 isn't even a disadvantage - it's an advantage worded just so.
I really like go's way of dealing with this. `x++` can be used as a statement, but not as an expression.
Good. In principle, there's nothing wrong with an operator that increments and does nothing else, but C ruined that option with its inane return-the-old-value- then-increment system.
I found myself agreeing with the author. The biggest disadvantage **to me** would be finding it annoying to not be able to use something I use pretty often, coming from a C/C++ background. This whole thread resembles bike-shedding. 
Considering this proposal is by the guy who invented the language...

And considering swift already went through some code breaking changes from version 1 to 1.2 to 2.0 (the current), and now open source as well...

So... isn't it kind of too late to be changing language features again?
[deleted]
It is funny how simple little concept like that can fuck with programmers
I think that it is easier to avoid off by one errors by using ++ operators. You can often remove an entire line of code by using ++ in accessor methods or similar, this way you preserve invariants instead of temporarily breaking it until you fix it in the next line.
[deleted]
Why did they add them in the first place? They created a new language ffs! No need to copy the horrible parts of other languages. Sigh.
++x makes  sense ...people who use it are usually pedants because they know the difference. (and confuse the hell out of people when they do use it) 

x++ on the other hand is used nearly every time someone writes a loop. there is an exception and that's if they have a foreach loop in Swift with the ability to *get the current index* which many programming languages lack. 
Given enough time all programming languages evolve to become Pascal


Lol, macfags just couldn't deal.
That's just a proposal that I am sure will be dropped. I still think dropping the prefix version only is a good idea.
[deleted]
One more: Conversion ctors should be `explicit` by default, with a new `implicit` keyword in cases where it's needed.
There is one issue: 3rd-party libraries.

Today, I can compile a C++98 (oh craziness) library header as part of my C++11 (or C++14) code and it "just works". Applying the magic wand to those 3rd-party headers however is a maintenance burden... actually, it means *forking* those 3rd-party headers... and it may very well not even be *legal* without authorization of said 3rd-party.
One problem that he doesn't address is that sometimes (unfortunately all too often) code needs to be compiled with older toolset. Especially in the slower moving embedded world dealing with toolset oddities are a given. If that code only needs to execute on those targets that's fine, just don't run the magic wand on it. But again that's not so simple. Many professional organisation have millions of lines of code in various libraries that get reused over multiple projects/platforms/... If they're unlucky enough to have to deal with external libraries over which they don't control the source the issue can even get trickier. Breaking eggs sounds fine, but you can't seriously pretend that an automatic tool checker/changer is a sufficient answer for a software development shop dealing with legacy that's 15-20-30 years old. I hate it enough as it is...
Python tried this, it still hasn't worked out.
If you break all the eggs in C++, the language you have is no longer C++. Feel free to do it, but just be aware of that. One of the most important features of C++ is compatibility.
I'd really like to get rid of implicit generation of copy/move ctors and assignment operators in the case where at least one of them or the destructor is user-declared. The existence of the "rule of three" basically means that we got our defaults wrong.
For now, I'd just deprecate them - raise warnings when people use the old versions.

And then in 10 or 20 years, when people have had time to adjust, replace those warnings with errors. (Maybe have a compiler flag that lets you revert to the old behaviour, for those working with very old code).
Why not go crazy and remove support for header files, malloc, and all the other legacy C functions. I'm sure auto_ptr is frowned upon, that can go too.
"To fix the issues in [language], just create a new version of [language] by [breaking compatibility]!"

That's not how programming languages work once they're past v1.0.0, unfortunately.
Why not.  Imagine how good and dominant C++ would be now if you could just mulligan the unnecessary parts.
So basically re-invent D?

I'm completely serious: Try D. It's basically C++ without the "features".
&gt; So let's take a page out of D's book (in particular, page 30 of The D Programming Language) and zero-initialize built-ins by default, but specify that void as an initial value prevents initialization:
&gt;
&gt;     int x;              // always zero-initialized
&gt;     int x = void;       // never zero-initialized

Yeah no. If anything, not providing an initializer should be a compiletime error.

    int x = 0; // 0-initialized
    int x; // error
    int x = void; // uninitialized
Are his 'o's supposed to be zeroes? because IDKWTF is going on if not.
Does this mean that SQLite will be part of .NET, so instead of including your own version of sqlite3.dll, you can just do something like

using System.Database.Sqlite3;

?
That's might be good for sqlite too, they will contribute to their source code and it can be better in future hopefully. Not that sqlite is bad now, but who knows. 
Good for MS.
LOL. After struggling for a day trying to figure out LocalDB I ended up ditching it and going to SQLite, seems like even Microsoft agree with me. Maybe ditching SQL Server Compact was a bad idea.. they don't even want it in their own product.
I'm curious as to what this means for SQL Server Compact Edition. Is this a replacement or just another option? 
Unfortunately the latest SQLite (v3.9) does not support fixed-point type similar to SQL standard type [NUMERIC(p,s) or DECIMAL(p,s)](https://en.wikipedia.org/wiki/SQL#Data_types). This is the type usually used to represent money/currency values and used in financial/accounting solutions and will hence round values as used in such kind of solutions.

Will be interesting to see if/how Microsofts API to SQLite solves that!
Embrace.
&gt;Up till Windows 8.1, we had a different state store for Desktop, Phone, and Xbox. Moreover, each of these was based on different combinations of database technologies like the registry, the Extensible Storage Engine, and XML files. To have a single state store for all our devices, we needed a unified data layer ideally built on a single database technology.

How the **fuck** are registry and XML database technology!?

ESENT is, fine. Are they saying that SQLite is better than ESENT? ;-)
Cool story, bro.

Too many programmers will tell you how great OO is for encapsulation and then futz around with the internal state of other objects anyway, as if doing it via some accessor method somehow makes it acceptable.
&gt; During the day you need to borrow some money from a colleague. You pick up your colleague, reach into their trousers and pull out their wallet. You open the wallet and pull out some cash.

I used this exact example to explain some colleagues why they shouldn't getX().getY().doZ(); I think it was not enough for them to understand it...
Well, to be fair, not everyone dies at the end.  If you're one of the lucky few (or maybe because an immortal reached into your trousers, took your wallet, and your wallet has your photo ID, which can also magically summon you anywhere), you end up not dying and being moved to the Tenured Home.
This post is clever and fun. Bravo OP!
I'm trying to decipher the metaphors in this:

* Body parts pushed into you = Dependency Injection
* Abstract concept of vehicle = Asking a factory for a thing, the thing is abstract
* Telling yourself it's a motorcycle = Wrong typecast in a try/catch
* Taking engine/pedals/starter out = ????
* Pick up colleague and pulling out trousers = getters? bad API? 
* Two copies of wallet = Concurrency by sharing memory
* Wallet explode = Concurrency issue because of timing
* Friends forgetting about you = References
* Refuse disposal = Garbage collection
* Organs donated = ???? Possibly an object pool?

Can anybody help me out on this?

This is the best thing i've read this week, for sure.
It's beatiful. I mean it.
Not gonna lie, but that story was stupid.  Learn many languages and use what is best for the task at hand.  As a side note, there are much better stories about Java then that one.
But what if x is a data object. For example appConfig.getDatabaseConfig().gethost()?  

For ease of use, you save the object for future references

DBConfig = appConfig.getDbConfig()
Host = Dbconfig.getHost()
Username = Dbconfig.getUsername()  

Dosomething is probably bad, but consider  appConfig().getDbConfig().connectReadOnly() (vs connectReadWrite() ) which returns some sort of SqlConnection that is abstracted from PostgresSqlConnection or MsSqlConnection.  

Finally having multiple objects that share appConfig / dbConfig because they do different things but all have something to do with the database.  

Ok, mutation is definitely bad. Unless it's intended to be mutated. Like some producer / consumer thread safe job queue.  

I can make some argument for the other side too, but it seems already there in this thread. Just my 2c on how analogy can only go so far
I haven't read that much sci-fi, but the writing style seems reminiscent of a certain author. Is that the case? 
I love it how OO proponents dislike analogies to OO-design. "No, it shouldn't work like that, that's stupid. We have this rule...". Yeah. Well. That's not how any OO code I've come across looks like. And I'm pretty sure an analogy of that would sound even stupider.
That is the stupidest thing I have read today.
A description of an anemic society if you ask me.
wat
This is kind of cheating, isn't that mod just an interface for externally run code?
This belongs in either /r/feedthebeast or /r/opencomputers. Just because you've written a simple script doesn't make it "of high quality" that the guidelines outline for the sub.

He should have tried to download and install Minecraft on that computer. Than do the same thing to the in-game-in-game computer. Then again. I wonder on which iteration he would have get a stackoverflow exception.
Good job!
Elm is rapidly becoming what I wish Haskell was. I haven't used it for anything serious yet, but it seems like the type system really hits a sweet spot between power and complexity, and the syntax feels like 'Haskell's greatest hits'.
Those compiler messages are great! The hints would be super useful for anyone learning to program.
Elm enabled fearless code change for me in a way that no other programming language did. Now, I just change my code, please the compiler and, almost magically, everything works as I expected it to work. 
I recommend watching this cool talk about the design decisions in Elm/its tools and how to optimize for approachability: [Let's be mainstream! User focused design in Elm](https://www.youtube.com/watch?v=oYk8CKH7OhE)
&gt; Well, there are no more cascading errors in Elm 0.16 thanks to Hacker News! When we announced our first effort to improve Elm’s error messages, someone on Hacker News commented with a very simple yet specific description of how to avoid cascading errors.

The link he gave says:

&gt; Many years ago I wrote the front end of a compiler-like system (it was for formal specifications, not for runnable code) and dealt with some of these problems. Whenever a type problem was detected, the error was reported and the **type of the failed object was changed to an internal error type.**

I did this a compiler I wrote a few months back. It just randomly popped in my head; I thought at the time that this was just a "toy compiler" thing!
I've played with Elm a bit and it's really cool, though the syntax is a bit annoying to read (I'm sure that's just because I'm accustomed to C-like languages)
Excellent improvements, as expected.

I tried installing Elm a few months ago, on Linux, but I got stuck in cabal hell. I tried installing it via node, but I'm on a 32-bit system, and it seems like the node binaries are 64-bit ...

Are there any plans to make the Elm compiler self-hosting?

Keep up the good work!
&gt; union types

I read that, then checked the docs to see if you supported union and intersection types, but you don't. Do you mean sum types? It's very bad to use existing terminology to mean something other than its established meaning.
Yep, I am *still* wondering what an email client release was doing in this subreddit when I saw the headline.  
Every couple of months I see an article about the impostor syndrome from a person who can't seem to understand its most basic premise. Impostor syndrome is, by its definition, the inability to internalize positive feedback.

Insecurity caused by insufficient positive feedback has nothing to do with the impostor syndrome.

I know people who, however, disregard positive feedback as "I know that's not true, this is just a facade", so just like many other things, there are no true "the one cure". 
Inevitably this is going to point to the stereotype that millenials need constant feedback. Can't win.
I think this i a good way of dealing with the problems. The challenge will be to get feedback if you don't have a system for it. Setting up a simple routine with good collegues will do it I guess. 
Lovely tutorial! Would be nice if it covered `create does&gt;` which is considered by many to be the pearl of Forth, they are used to define defining words. Mixing compilation and interpretation state, and exposing compilation to the user is something that makes Forth so appealing. `create` is used for `create`'ing something at compile time, and `does&gt;` is used for defining what the word does with it at runtime. For instance you could define the words for a `struct` like so:

    : struct  0 ;
    : field  create over , + does&gt; @ + ;
    
    ( usage example, a packet header)
    struct
      4 field length
      1 field type
      2 field tag
    constant /header

http://wiki.laptop.org/go/Forth_Lesson_18
    
-----

Also, for [an array of cells](http://skilldrick.github.io/easyforth/#arrays) I usually just do something like:

`create myarray 1 , 2 , 3 , 4 , 5 ,`
stack underflow, you don't see that too often.
Weird, we were just talking about Forth at lunch today. Haven't used it in 20 years.
Why would I ever want to write a stack based language? It seems incredibly unintuitive and I don't see any particular benefit. 

(Serious question, not just hating on the post.)
Nice tutorial.

The inline interpreters were a little annoying - Instead of having to type code in multiple areas, I wish there was just a single place, and I wish there was a text editor from which code can be evaluated in a REPL, instead of just a REPL.

I've been playing around with the snake game a bit, and I noticed that you can store any number into graphics memory, not just 0 or 1.

So, instead of allocating 1000 cells to store the body, and managing that, you could just write the pointer to the new head pixel as the color of the old head pixel.

That would effectively give you a linked list from the tail to the head, and then you could simply move like this:

    : move cut-tail grow-head ;

PS: Looking at `: convert-x-y ( x y -- offset )  24 cells * + ;` - Why is "cells" there? 24 should be multiplied by the value on the stack, which is y ... Right?
Nicely done!
Whatever I write in the site's interpreter I get "dict.find is not a function". Am I doing something wrong?
(when is the emacs mode coming?)

I'm seriously impressed. This is amazing.

Paredit always ended up being a requirement for me, but this takes it to the next level. What other editing problems out there could have excessive hotkey usage replaced by something automatic with the same level of power, I wonder? This is a great example of an area where user interaction experts can really have a lot of impact, and I think shows why engineers ought to be more accepting of disciplines like that.
Just tried this out. It's *dreamy*. I've never been so turned on by an editor plugin. &lt;3

Thank you so much! Exemplary work on the website too!
I've been convinced to learn a Lisp.
One of those things where, once you see it, you can't understand why it hasn't always been this way.
Oh my god suddenly Clojure doesn't feel like a giant uphill battle thank you kind stranger.
Can someone explain to me, as somewhat of a novice, why people consider lisp so powerful? I've been learning Scheme for my first CS class after python, and it just seems so... inefficient. The only thing I can see it being better than python is the built in cons/car/cdr recursive list stuff, but even then you can make your own in python.
great, integrating it at work asap!
I seriously need to check this out. This is AMAZING!
These seem pretty nice for even an ordinary programming language.
TypeScript's importance should not be underestimated. Finally, typesafe scripting with widespread adoption thanks to Google and Microsoft. 

I'm pretty sure browsers will support TypeScript directly pretty soon, so we won't need the extra compile step of today. This would make the Inspect/debug plugins in browser sane as well.

Heck, the thing even has union types and flow typing. Well done!
Oh, async/await. Nice. 3 thumbs up!

Speaking of "fluent APIs", would be neat if they'd copy Dart's cascade operator.

    var button = querySelector('#button');
    button.text = 'Confirm';
    button.classes.add('important');
    button.onClick.listen((e) =&gt; window.alert('Confirmed!'));

Same thing with some cascade magic:

    querySelector('#button')
      ..text = 'Confirm'
      ..classes.add('important')
      ..onClick.listen((e) =&gt; window.alert('Confirmed!'));
Anyone using Typescript in real world apps? 
There is no fucking way i'll write any more javascript now.
What are the advantages over [Scala.js](http://www.scala-js.org/)?
I don't know that much about Typescript but I can I use Typescript along-side Babel or does Typescript itself implement ES2015 to target ES5 browsers?
Perl6 is the only language in the last 5 years which really impresses me. 

* It is general enough to support different concepts for building large programs (OO, traits/roles, functional), without enforcing a specific  model on your programs *cough*Go*cough*
* It is optionally typed. Which is a really great feature, introduced in the last years by Facebook's Hack (I know it's not the first language to support this). I think we will see more optional typed languages in the future. 
* Expressiveness of the language improves greatly the readability of the program.
* While the expressiveness reminds one of ruby, perl6 has a proper language spec and tests.
* Grammars &gt; Regex
* Sane and helpful errors!

There also are some things which are not perfect yet:

* The startup of the MoarVM is **slow**, but they have not started optimizing yet. If you need performance you can use JVM as backend.
* Lots of modules and functionality is missing, but you can reuse Perl5, Lua and Python libraries via [Inline](http://modules.perl6.org/?q=Inline%3A)

Author, please don't use md5 as a password example. Use it for trivial file integrity or duplicate checking, but not passwords.

Grep/map examples would be handy too.
Actually looks quite nice. 
I want camel book to be rewritten for Perl 6. Maybe a book along the lines of Higher Order Perl would be great too, incorporating more from the statically typed FP techniques this time. 
Perl 6 sounds like a really nice language.

My only problem with it is the consistently derpy naming scheme:

`my` for variables? `$@%` to denote arrays and hashes?

Or the type names `Mu`, `Cool`, and `Rat`?

On the other hand, the formal way of listing the operators as In-/Pre-/Post-/Circum-/Postcircum- fix is REALLY nice.

Also: "kebab-case"
A bit off topic, but asciidoctor creates beautiful docs ;)
Does it still have $_ ? I liked $_ back when I was doing perl5...
Looks compelling. I may try it some time. Initial thoughts:

- Love the multiple subroutines (reminds me of Julia) and the static typing

- Didn't see much about functional programming or immutability, though 

- some of the examples was a bit of a turn off because it showed bad practice in programming. For example, who would have a method that sets the value of an attribute?  
Too much slurping and spurting for my tastes, but I am not going to judge what goes in and out of perl programmers.
I'll stick with Perl 5 until Perl 6 proves itself properly outside of its implementors' circlejerks, thanks
I think Perl was invented at a time when people believed that it was or might be a great idea to do everything with one programming language.  This belief led to the philosophy of There Should Be More Than One Ways to Do It.

Unfortunately, many of us eventually found out that while TIMTOWTDI gave us momentary satisfaction at the time something was created, it would come back and haunt us and would cause excruciating agony.

Perl 6 is an extension of Perl 5 not just in features but also in concepts.  Beyond including many ways to express something, Perl 6 aims to include as many features that they think would be useful as possible.  I think it's not a very good idea.


This is my project, if anyone wants to ask me anything...
This is amazing dude. How long have you been working with Raspberry pis? 
I'm interested in the hardware you have added to the camera module.

Do you have links for that lens? And are those IR LED attachments custom or pre-made? How about a parts list? :)

I had planned on just buying a game camera, but hell I have enough Raspberry Pi's sitting around I could easily do something like this to capture some outdoor wildlife time lapses. I'd just need to weatherproof an enclosure for it.

Edit: I also thought that was one of my Pis at first, as I have the same case &amp; WiFi dongle. Guess they're popular.
I can see you using this for your PhD dissertation and after all this awesomeness the cranky chairman having only one question after an uncomfortable silence--"What is the size of the banana?"
How do you power?
You can try it out here: http://joelbox.s3.amazonaws.com/citd/index.html

It activates after 200 characters I think
Wow, that looks both fun and seriously annoying...
Cool, yet not particularly useful :P

The Github comments after it gets popular however do perfectly represent why there really needs to be a like/agree/I-have-this-issue-too button on Github.
Good job Javascript monkeys.
Awesome. I'm going to use this for a presentation :)
[Oh fuck the hell yes](https://www.youtube.com/watch?v=aBSU4SBrNJU)
