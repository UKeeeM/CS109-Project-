{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Find Your Expert</h1>\n",
    "<br>\n",
    "The purpose of our application is to identify the best comments for a user defined article. In order to provide the best and most relevent comments for articles that have somehow escapped the reddit verse, find similar articles that have been posted on and we present the best comments. We identify the best comments through a prediction model.\n",
    "<br>\n",
    "<h2>Prediction Strategy</h2>\n",
    "<img src=\"PredictionStrat.png\">\n",
    "The above proccess identifies our prediction strategy but we requrie a slightly different approach. In order to train our model we need to collect articles that *Do* exist in the reddit verse and the top comments related to that article. We can then use this data to extract the features from the comments and the relation to the article, and construct a model to predict the score of the comments. \n",
    "<br>\n",
    "<h2>Training Strategy</h2>\n",
    "<img src=\"TrainingStrat.png\">\n",
    "<br>\n",
    "<br>\n",
    "The collection strategy was designed to maximise effective data collection while adhearing to the <a href=\"https://www.reddit.com/wiki/licensing\">**Reddit API Guidlines**</a>.\n",
    "In order to provide the most effective training data the collection strategy was designed with the following coniderations\n",
    "<br>\n",
    "* Only collect comments directly in response to an orginal article post. We restrict our collection to first level responses in an effort to avoid collecting comments not directly related to the orginal article.\n",
    "* Collect from specific sub-reddits where the orginal content is usually an external source ie. r/news, r/TIL\n",
    "* Avoid sub-reddits where there is no orginal content like AMA's, or ELI5\n",
    "* Avoid multi-media subreddits like r/pics, r/videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Things!</h1>\n",
    "\n",
    "The Reddit API consits of several *things* (the base class). The API provided documentation on several different <a href='https://github.com/reddit/reddit/wiki/JSON'> *Things!*</a>; however we are most interested in the following:\n",
    "\n",
    "<h2>Links</h2>\n",
    "Links are the orginal post, all of the comments we are interested in our in response to a specific link. A link object has several tags that can all be found in the <a hfref= 'https://github.com/reddit/reddit/wiki/JSON'> *things documentation*</a>. The most relevent tags to our collection strategy are:\n",
    "\n",
    "|**Type**|**Tag**|**Description**|\n",
    "|--|--|-------------------------------|\n",
    "|String|id|this links identifier, e.g. \"8xwlg\"|\n",
    "|String|name|Fullname of link|\n",
    "|String|author|the account name of the poster.|\n",
    "|String|domain|the domain of this link. Self posts will be self.subreddit while other examples include en.wikipedia.org and s3.amazon.com\n",
    "|boolean|over_18|true if the post is tagged as NSFW. False if otherwise|\n",
    "|int|num_comments|the number of comments that belong to this link. includes removed comments.|\n",
    "|String|permalink|relative URL of the permanent link for this link|\n",
    "|int|score|the net-score of the link. note: A submission's score is simply the number of upvotes minus the number of downvotes. If five users like the submission and three users don't it will have a score of 2. Please note that the vote numbers are not \"real\" numbers, they have been \"fuzzed\" to prevent spam bots etc. So taking the above example, if five users upvoted the submission, and three users downvote it, the upvote/downvote numbers may say 23 upvotes and 21 downvotes, or 12 upvotes, and 10 downvotes. The points score is correct, but the vote totals are \"fuzzed\".|\n",
    "|String|subreddit|subreddit of thing excluding the /r/ prefix. \"pics\"|\n",
    "|String|title|the title of the link. may contain newlines for some reason|\n",
    "\n",
    "<br>\n",
    "<h2>Comments</h2>\n",
    "Comments are the actual comments made in regard to a specific link. The most relevent tags to our collection strategy are:\n",
    "\n",
    "|**Type**|**Tag**|**Description**|\n",
    "|--|--|-------------------------------|\n",
    "|String|id|this comment identifier, e.g. \"8xwlg\"|\n",
    "|String|name|Fullname of comment|\n",
    "|String|author|the account name of the poster.|\n",
    "|String|body|the raw text. this is the unformatted text which includes the raw markup characters are escaped.|\n",
    "|int|gilded|the number of times this comment received reddit gold|\n",
    "|String|link_id|ID of the link this comment is in|\n",
    "|String|parent_id|ID of the thing this comment is a reply to, either the link or a comment in it|\n",
    "|int|score|the net-score of the comment|\n",
    "\n",
    "\n",
    "<h2>Listings</h2>\n",
    "The last *Thing* we need is the listing class. In order to facilitate bulk data collection we can make use of the listing class to request list of the things we are interested in. The listing class literaly consists of a list of other things. A listing object is restricted to 100 Things so we also make use of the paging concept to get longer lists of things we want. The paging concept of the listing class is controlled by the following tags:\n",
    "\n",
    "|**Type**|**Tag**|**Description**|\n",
    "|--|--|-----------------| \n",
    "|String|before|The fullname of the listing that follows before this page. null if there is no previous page|\n",
    "|String|after|The fullname of the listing that follows after this page. null if there is no next page.|\n",
    "\n",
    "\n",
    "<h2>Header Information</h2>\n",
    "In addtion to the JSON objects themselves we also make use of the response headers to stay under the rate limiting. Reddit nicely asks to make no more than 30 requests per minute. We use the following response headers to ensure we play by the rules. \n",
    "\n",
    "|**Header Tag**|**Description**|\n",
    "|----|----------| \n",
    "|X-Ratelimit-Used|Approximate number of requests used in this period|\n",
    "|X-Ratelimit-Remaining| Approximate number of requests left to use|\n",
    "|X-Ratelimit-Reset|Approximate number of seconds to end of period|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create a Reddit API Request Token</h2>\n",
    "<br>\n",
    "Before we can start looking at *Things!* we need to we need to create a scripting application and an authorized token. The application is our vehicle for making authorized requests to the reddit API. The application **ExpertCollector/0.1 by cs109-2015** was created using https://github.com/reddit/reddit/wiki/OAuth2-Quick-Start-Example and named using reddit user agent naming conventions.\n",
    "<br>\n",
    "Once the application was created we can use the user and application credentials to request a token. User and application credentials are not to be shared and will only be distributed as required. The credentials text file is of the following format:\n",
    "<br>\n",
    "<br>\n",
    "client_id: application id<br>\n",
    "username: username<br>\n",
    "pw: password<br>\n",
    "secret_id<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import requests.auth\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys\n",
    "import newspaper\n",
    "import nltk\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getToken(creds):\n",
    "    client_auth = requests.auth.HTTPBasicAuth(creds['client_id'], creds['secret_id'])\n",
    "    post_data = {\"grant_type\": \"password\", \"username\": creds['username'], \"password\": creds['pw']}\n",
    "    headers = {\"User-Agent\": creds['user_agent']}\n",
    "    response = requests.post(\"https://www.reddit.com/api/v1/access_token\", auth=client_auth, data=post_data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        print 'Credentials Verified: Token Recived'\n",
    "    else:\n",
    "        print 'Invalid Creds'\n",
    "\n",
    "    auth = response.json()['token_type']+' '+response.json()['access_token']\n",
    "    return {\"Authorization\": auth, \"User-Agent\": creds['user_agent']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a created token and our application name to construct the authorized header for application requests. Our request token is valid for 1 hour so we may have to do this more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credentials Verified: Token Recived\n"
     ]
    }
   ],
   "source": [
    "with open(\"creds.txt\") as f:\n",
    "    creds = dict([line.strip().split(':') for line in f])\n",
    "token = getToken(creds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the created token to make authorized requests, through our application, directlty to the reddit API. As an example let's see how active our cs109-2015 reddit account has been."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'comment_karma': 0,\n",
       " u'created': 1448151333.0,\n",
       " u'created_utc': 1448122533.0,\n",
       " u'gold_creddits': 0,\n",
       " u'gold_expiration': None,\n",
       " u'has_mail': True,\n",
       " u'has_mod_mail': False,\n",
       " u'has_verified_email': False,\n",
       " u'hide_from_robots': False,\n",
       " u'id': u's9gd4',\n",
       " u'inbox_count': 1,\n",
       " u'is_gold': False,\n",
       " u'is_mod': False,\n",
       " u'is_suspended': False,\n",
       " u'link_karma': 1,\n",
       " u'name': u'cs109-2015',\n",
       " u'over_18': False,\n",
       " u'suspension_expiration_utc': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(\"https://oauth.reddit.com/api/v1/me\", headers=token)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Collecting Training Links</h2>\n",
    "<br>\n",
    "Now that we can make requests, our first step is to find links (articles) we can use for our training data. In order to get a broad set of relevent training data we aim to collect the Top *n* Links, in a specified time period,for a set of specific sub-reddits we are interested in. The following function lets us do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "GetTrainingLinks(subreddit_list,n_links,time_window)\n",
    "\n",
    "Description:\n",
    "############\n",
    "Collectes the top n links, in the specified time window, from each subreddit provided.\n",
    "\n",
    "Runtime(seconds) = n_links/50 * length(subreddit_list)\n",
    "\n",
    "Parameters:\n",
    "###########\n",
    "subreddit_list: a list of subreddit names\n",
    "\n",
    "n_links: total number of links per sub-reddit to collect\n",
    "\n",
    "time_window: one of ('hour', 'day', 'week', 'month', 'year', 'all')\n",
    "\n",
    "outfile: path to output location\n",
    "\n",
    "Outputs:\n",
    "#########\n",
    "Writes a single JSON per line to the outfile. \n",
    "\n",
    "Returns:\n",
    "##########\n",
    "\n",
    "\n",
    "'''\n",
    "url = \"https://oauth.reddit.com/r\"\n",
    "\n",
    "def getTrainingLinks(subreddit_list,n_links,time_window,outfile):\n",
    "    token = getToken(creds)\n",
    "    for subreddit in subreddit_list:\n",
    "        n=0\n",
    "        while n < n_links:\n",
    "            if n == 0:\n",
    "                query= 'top?limit=100&t={0}'.format(time_window)\n",
    "            else:\n",
    "                query= 'top?limit=100&t={0}&after={1}'.format(time_window,after)\n",
    "            request_url= \"/\".join([url,subreddit,query])\n",
    "            response = requests.get(request_url, headers=token)\n",
    "            after = response.json()['data']['after']\n",
    "            n+=100\n",
    "            if not os.path.exists(os.path.dirname(outfile)):\n",
    "                os.makedirs(os.path.dirname(outfile))\n",
    "            with open(outfile,'a') as link_file:\n",
    "                   for link in response.json()['data']['children']:\n",
    "                        json.dump(link['data'],link_file)\n",
    "                        link_file.write('\\n')\n",
    "            clear_output()\n",
    "            sys.stdout.write('{0} r/{1} Links Collected'.format(n,subreddit))\n",
    "            sys.stdout.flush()\n",
    "            time.sleep(2) # So we respect the rate limits! \n",
    "    \n",
    "    clear_output()\n",
    "    print 'Done.'\n",
    "    return outfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Sample Collection</h2>\n",
    "Here we specify our collection parameters, this is just a sample. The actual training collection prameters will be more extensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "subreddit_list = ['science','news','worldnews','dataisbeautiful','todayilearned','politics',\n",
    "                  'technology','space','internetisbeautiful','nottheonion','gadgets',\n",
    "                  'documentaries','upliftingnews','programming','europe','datascience',\n",
    "                  'uspolitics','ukpolitics','CanadaPolitics','explainlikeimfive','liberal','conservative',\n",
    "                  'nba','soccer','nfl','food','SubredditSimulator','askscience','askhistorians']\n",
    "\n",
    "n_links = 1000\n",
    "time_window ='year'\n",
    "outfile ='./links.txt'\n",
    "\n",
    "training_links = getTrainingLinks(subreddit_list,n_links,time_window,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34798, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>domain</th>\n",
       "      <th>downs</th>\n",
       "      <th>gilded</th>\n",
       "      <th>is_self</th>\n",
       "      <th>likes</th>\n",
       "      <th>media</th>\n",
       "      <th>id</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>num_reports</th>\n",
       "      <th>over_18</th>\n",
       "      <th>permalink</th>\n",
       "      <th>score</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>title</th>\n",
       "      <th>ups</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the_phet</td>\n",
       "      <td>1.447239e+09</td>\n",
       "      <td>ibtimes.co.uk</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3se6lu</td>\n",
       "      <td>1073</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/science/comments/3se6lu/algae_has_been_gene...</td>\n",
       "      <td>6705</td>\n",
       "      <td></td>\n",
       "      <td>science</td>\n",
       "      <td>http://b.thumbs.redditmedia.com/y1CGKgl69hKw-s...</td>\n",
       "      <td>Algae has been genetically engineered to kill ...</td>\n",
       "      <td>6705</td>\n",
       "      <td>http://www.ibtimes.co.uk/algae-genetically-eng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>skoalbrother</td>\n",
       "      <td>1.448903e+09</td>\n",
       "      <td>phys.org</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3uvg0o</td>\n",
       "      <td>2216</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/science/comments/3uvg0o/researchers_find_ne...</td>\n",
       "      <td>6777</td>\n",
       "      <td></td>\n",
       "      <td>science</td>\n",
       "      <td>http://b.thumbs.redditmedia.com/hZrhEdBoJp22oE...</td>\n",
       "      <td>Researchers find new phase of carbon, make dia...</td>\n",
       "      <td>6777</td>\n",
       "      <td>http://phys.org/news/2015-11-phase-carbon-diam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>godsenfrik</td>\n",
       "      <td>1.447707e+09</td>\n",
       "      <td>news.stanford.edu</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3t2exx</td>\n",
       "      <td>703</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/science/comments/3t2exx/when_scientists_fal...</td>\n",
       "      <td>6259</td>\n",
       "      <td></td>\n",
       "      <td>science</td>\n",
       "      <td>http://b.thumbs.redditmedia.com/FQPskQP8EejVh9...</td>\n",
       "      <td>When scientists falsify data, they try to cove...</td>\n",
       "      <td>6259</td>\n",
       "      <td>http://news.stanford.edu/news/2015/november/fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>avogadros_number</td>\n",
       "      <td>1.447108e+09</td>\n",
       "      <td>phys.org</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3s6xe6</td>\n",
       "      <td>657</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/science/comments/3s6xe6/dispersants_did_not...</td>\n",
       "      <td>6146</td>\n",
       "      <td></td>\n",
       "      <td>science</td>\n",
       "      <td>http://b.thumbs.redditmedia.com/mFQzb4d2QNiyaE...</td>\n",
       "      <td>Dispersants did not help oil degrade in BP spi...</td>\n",
       "      <td>6146</td>\n",
       "      <td>http://phys.org/news/2015-11-dispersants-oil-d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Letmeirkyou</td>\n",
       "      <td>1.44787e+09</td>\n",
       "      <td>popularmechanics.com</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3tbkv6</td>\n",
       "      <td>580</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/science/comments/3tbkv6/scientists_have_dis...</td>\n",
       "      <td>6020</td>\n",
       "      <td></td>\n",
       "      <td>science</td>\n",
       "      <td>http://b.thumbs.redditmedia.com/spD7SnbKlAEY1y...</td>\n",
       "      <td>Scientists have discovered an exoplanet still ...</td>\n",
       "      <td>6020</td>\n",
       "      <td>http://www.popularmechanics.com/space/deep-spa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             author   created_utc                domain downs gilded is_self  \\\n",
       "0          the_phet  1.447239e+09         ibtimes.co.uk     0      0   False   \n",
       "1      skoalbrother  1.448903e+09              phys.org     0      0   False   \n",
       "2        godsenfrik  1.447707e+09     news.stanford.edu     0      0   False   \n",
       "3  avogadros_number  1.447108e+09              phys.org     0      0   False   \n",
       "4       Letmeirkyou   1.44787e+09  popularmechanics.com     0      0   False   \n",
       "\n",
       "  likes media      id num_comments num_reports over_18  \\\n",
       "0  None  None  3se6lu         1073        None   False   \n",
       "1  None  None  3uvg0o         2216        None   False   \n",
       "2  None  None  3t2exx          703        None   False   \n",
       "3  None  None  3s6xe6          657        None   False   \n",
       "4  None  None  3tbkv6          580        None   False   \n",
       "\n",
       "                                           permalink score selftext subreddit  \\\n",
       "0  /r/science/comments/3se6lu/algae_has_been_gene...  6705            science   \n",
       "1  /r/science/comments/3uvg0o/researchers_find_ne...  6777            science   \n",
       "2  /r/science/comments/3t2exx/when_scientists_fal...  6259            science   \n",
       "3  /r/science/comments/3s6xe6/dispersants_did_not...  6146            science   \n",
       "4  /r/science/comments/3tbkv6/scientists_have_dis...  6020            science   \n",
       "\n",
       "                                           thumbnail  \\\n",
       "0  http://b.thumbs.redditmedia.com/y1CGKgl69hKw-s...   \n",
       "1  http://b.thumbs.redditmedia.com/hZrhEdBoJp22oE...   \n",
       "2  http://b.thumbs.redditmedia.com/FQPskQP8EejVh9...   \n",
       "3  http://b.thumbs.redditmedia.com/mFQzb4d2QNiyaE...   \n",
       "4  http://b.thumbs.redditmedia.com/spD7SnbKlAEY1y...   \n",
       "\n",
       "                                               title   ups  \\\n",
       "0  Algae has been genetically engineered to kill ...  6705   \n",
       "1  Researchers find new phase of carbon, make dia...  6777   \n",
       "2  When scientists falsify data, they try to cove...  6259   \n",
       "3  Dispersants did not help oil degrade in BP spi...  6146   \n",
       "4  Scientists have discovered an exoplanet still ...  6020   \n",
       "\n",
       "                                                 url  \n",
       "0  http://www.ibtimes.co.uk/algae-genetically-eng...  \n",
       "1  http://phys.org/news/2015-11-phase-carbon-diam...  \n",
       "2  http://news.stanford.edu/news/2015/november/fr...  \n",
       "3  http://phys.org/news/2015-11-dispersants-oil-d...  \n",
       "4  http://www.popularmechanics.com/space/deep-spa...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = []\n",
    "#Tags to keep\n",
    "tags= [ u'author',u'created_utc', u'domain', u'downs', u'gilded',u'is_self', u'likes', u'media', 'id',\n",
    " u'num_comments', u'num_reports', u'over_18', u'permalink',u'score', u'selftext', u'subreddit', u'thumbnail', u'title', u'ups', u'url']\n",
    "\n",
    "with open('./links.txt') as data_file:\n",
    "    for link in data_file:\n",
    "        links.append(pd.read_json(link,orient='records',typ='series')[tags])\n",
    "linkdf = pd.concat(links,axis=1).transpose()\n",
    "print linkdf.shape\n",
    "linkdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politics               1200\n",
      "Conservative           1200\n",
      "todayilearned          1200\n",
      "science                1200\n",
      "uspolitics             1200\n",
      "askscience             1200\n",
      "Documentaries          1200\n",
      "soccer                 1200\n",
      "food                   1200\n",
      "Liberal                1200\n",
      "CanadaPolitics         1200\n",
      "datascience            1200\n",
      "europe                 1200\n",
      "news                   1200\n",
      "ukpolitics             1200\n",
      "InternetIsBeautiful    1200\n",
      "nottheonion            1200\n",
      "UpliftingNews          1200\n",
      "worldnews              1200\n",
      "AskHistorians          1200\n",
      "dataisbeautiful        1200\n",
      "nfl                    1200\n",
      "space                  1200\n",
      "programming            1200\n",
      "SubredditSimulator     1200\n",
      "technology             1200\n",
      "nba                    1200\n",
      "explainlikeimfive      1199\n",
      "gadgets                1199\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print linkdf.subreddit.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Finding Usefull Links</h2>\n",
    "<br>\n",
    "Now we have a dataframe of potential links but before we can collect the comments we do some prunning. We remove links based on the following criteria:\n",
    "\n",
    "* Remove any potential NSFW links using the over_18 tag\n",
    "* Remove any links with no article using the is_self tag\n",
    "* Remove links with fewer than 10 comments\n",
    "* Remove any duplicate URLs. \n",
    "\n",
    "We write the list of unique article urls so we can go get the original article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_links = linkdf.loc[(~linkdf.is_self.fillna(False)) & (~linkdf.over_18.fillna(False)) & (linkdf.num_comments > 10),:]\n",
    "urls = training_links.url.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO Documentation\n",
    "\n",
    "def getArticle(url):\n",
    "    article = newspaper.Article(url, fetch_images = False)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    article.nlp()\n",
    "    return {\"url\":article.url, \"text\":article.text,\"keywords\":newspaper.nlp.keywords(article.text), \n",
    "            \"authors\":article.authors,\"summary\":article.summary,\n",
    "            \"publish_date\":str(article.publish_date)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'authors': [u'Derek Keats Flickr', u'Hannah Osborne', u'Marc Cirera'],\n",
       " 'keywords': {u'algae': 1.0436046511627908,\n",
       "  u'cancer': 1.0261627906976745,\n",
       "  u'cells': 1.0261627906976745,\n",
       "  u'chemotherapeutic': 1.0130813953488371,\n",
       "  u'drugs': 1.0305232558139534,\n",
       "  u'engineered': 1.0130813953488371,\n",
       "  u'genetically': 1.0174418604651163,\n",
       "  u'nanoparticles': 1.0130813953488371,\n",
       "  u'toxic': 1.0130813953488371,\n",
       "  u'tumours': 1.0130813953488371},\n",
       " 'publish_date': '2015-11-10 16:00:00+00:00',\n",
       " 'summary': u'Algae has been genetically engineered to kill cancer cells without harming healthy cells.\\nThe algae nanoparticles, created by scientists in Australia, were found to kill 90% of cancer cells in cultured human cells.\\nThe antibody binds only to molecules found on cancer cells, thus delivering the toxic drug specifically to the target cells.\\nIn turn, the antibody binds only to molecules found on cancer cells, meaning it could deliver drugs to the target cells.\\nResearchers genetically engineered the algae to produce an antibody-binding protein on the surface of their shells.',\n",
       " 'text': u'Algae has been genetically engineered to kill cancer cells without harming healthy cells. The algae nanoparticles, created by scientists in Australia, were found to kill 90% of cancer cells in cultured human cells. The algae was also successful at killing cancer in mice with tumours.\\n\\nNico Voelcker, from the University of South Australia, worked with researchers from Dresden in Germany to engineer diatom algae and loaded it with chemotherapeutic drugs. Publishing their study in the journal Nature Communications, the team also found that when they injected the nanoparticles into mice, tumours regressed.\\n\\nDiatom algae is a type of tiny, unicellular, photosynthesising algae. It measures just four to six micrometres in diameter and is enclosed within a porous skeleton made of silica. Because chemotherapeutic drugs are often toxic to healthy tissue, the researchers were able to hide the drugs inside the algae.\\n\\nResearchers genetically engineered the algae to produce an antibody-binding protein on the surface of their shells. In turn, the antibody binds only to molecules found on cancer cells, meaning it could deliver drugs to the target cells.\\n\\nVoelcker explained: \"By genetically engineering diatom algae - tiny, unicellular, photosynthesising algae with a skeleton made of nanoporous silica, we are able to produce an antibody-binding protein on the surface of their shells. Anti-cancer chemotherapeutic drugs are often toxic to normal tissues.\\n\\n\"To minimise the off-target toxicity, the drugs can be hidden inside the antibody-coated nanoparticles. The antibody binds only to molecules found on cancer cells, thus delivering the toxic drug specifically to the target cells.\\n\\nThe report authors sate: \"These data indicate that genetically engineered biosilica frustules may be used as versatile \\'backpacks\\' for the targeted delivery of poorly water-soluble anticancer drugs to tumour sites.\"\\n\\nAs algae mostly only needs water and light to grow, the team believes the technique could reduce the cost and waste of nanoparticle manufacturing and has huge potential for future cancer treatments. \"Although it is still early days, this novel drug delivery system based on a biotechnologically tailored, renewable material holds a lot of potential for the therapy of solid tumours including currently untreatable brain tumours,\" Voelcker said.',\n",
       " 'url': u'http://www.ibtimes.co.uk/algae-genetically-engineered-kill-90-cancer-cells-without-harming-healthy-ones-1528038'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO DOC\n",
    "url = urls[0]\n",
    "getArticle(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Collect ALL Articles and write each JSON to a new line\n",
    "#TODO doc\n",
    "def writeArticleJSON(urls,outfile):\n",
    "    if not os.path.exists(os.path.dirname(outfile)):\n",
    "                    os.makedirs(os.path.dirname(outfile))\n",
    "    with open(outfile,'a') as article_file:\n",
    "        for url in urls :\n",
    "            try:\n",
    "                json.dump(getArticle(url),article_file)\n",
    "                article_file.write('\\n')\n",
    "            except:\n",
    "                print 'error: ' + url\n",
    "    print 'done.'\n",
    "    return outfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inlvalid JSON\n",
      "(11472, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>summary</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Derek Keats Flickr, Hannah Osborne, Marc Cirera]</td>\n",
       "      <td>{u'toxic': 1.01308139535, u'cancer': 1.0261627...</td>\n",
       "      <td>2015-11-10 16:00:00+00:00</td>\n",
       "      <td>Algae has been genetically engineered to kill ...</td>\n",
       "      <td>Algae has been genetically engineered to kill ...</td>\n",
       "      <td>http://www.ibtimes.co.uk/algae-genetically-eng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>{u'diamond': 1.04316546763, u'laser': 1.008633...</td>\n",
       "      <td>None</td>\n",
       "      <td>If Q-carbon is harder than diamond, why would ...</td>\n",
       "      <td>This is a collection of 0.02, 0.03 and 0.04 ca...</td>\n",
       "      <td>http://phys.org/news/2015-11-phase-carbon-diam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Bjorn Carey]</td>\n",
       "      <td>{u'obfuscation': 1.01181102362, u'fraud': 1.01...</td>\n",
       "      <td>2015-11-16 00:00:00</td>\n",
       "      <td>Stanford researchers uncover patterns in how s...</td>\n",
       "      <td>Stanford researchers uncover patterns in how s...</td>\n",
       "      <td>http://news.stanford.edu/news/2015/november/fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>{u'oil': 1.05142857143, u'microbes': 1.0114285...</td>\n",
       "      <td>None</td>\n",
       "      <td>And that leads to more questions about where m...</td>\n",
       "      <td>Samantha Joye, a professor of marine sciences ...</td>\n",
       "      <td>http://phys.org/news/2015-11-dispersants-oil-d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[More From]</td>\n",
       "      <td>{u'star': 1.02124645892, u'forming': 1.0106232...</td>\n",
       "      <td>2015-11-18 06:00:00</td>\n",
       "      <td>Together, the two observations allowed the sci...</td>\n",
       "      <td>Shares Share\\n\\nTweet\\n\\nE-mail\\n\\n​Of the tho...</td>\n",
       "      <td>http://www.popularmechanics.com/space/deep-spa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             authors  \\\n",
       "0  [Derek Keats Flickr, Hannah Osborne, Marc Cirera]   \n",
       "1                                                 []   \n",
       "2                                      [Bjorn Carey]   \n",
       "3                                                 []   \n",
       "4                                        [More From]   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  {u'toxic': 1.01308139535, u'cancer': 1.0261627...   \n",
       "1  {u'diamond': 1.04316546763, u'laser': 1.008633...   \n",
       "2  {u'obfuscation': 1.01181102362, u'fraud': 1.01...   \n",
       "3  {u'oil': 1.05142857143, u'microbes': 1.0114285...   \n",
       "4  {u'star': 1.02124645892, u'forming': 1.0106232...   \n",
       "\n",
       "                publish_date  \\\n",
       "0  2015-11-10 16:00:00+00:00   \n",
       "1                       None   \n",
       "2        2015-11-16 00:00:00   \n",
       "3                       None   \n",
       "4        2015-11-18 06:00:00   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Algae has been genetically engineered to kill ...   \n",
       "1  If Q-carbon is harder than diamond, why would ...   \n",
       "2  Stanford researchers uncover patterns in how s...   \n",
       "3  And that leads to more questions about where m...   \n",
       "4  Together, the two observations allowed the sci...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Algae has been genetically engineered to kill ...   \n",
       "1  This is a collection of 0.02, 0.03 and 0.04 ca...   \n",
       "2  Stanford researchers uncover patterns in how s...   \n",
       "3  Samantha Joye, a professor of marine sciences ...   \n",
       "4  Shares Share\\n\\nTweet\\n\\nE-mail\\n\\n​Of the tho...   \n",
       "\n",
       "                                                 url  \n",
       "0  http://www.ibtimes.co.uk/algae-genetically-eng...  \n",
       "1  http://phys.org/news/2015-11-phase-carbon-diam...  \n",
       "2  http://news.stanford.edu/news/2015/november/fr...  \n",
       "3  http://phys.org/news/2015-11-dispersants-oil-d...  \n",
       "4  http://www.popularmechanics.com/space/deep-spa...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since We already Have some articles, lets not get duplicates.\n",
    "articles = []\n",
    "with open('./articles.txt') as data_file:\n",
    "    for article in data_file:\n",
    "        try:\n",
    "            articles.append(pd.read_json(article,orient='records',typ='series'))\n",
    "        except:\n",
    "            print 'inlvalid JSON'\n",
    "articledf = pd.concat(articles,axis=1).transpose()\n",
    "print articledf.shape\n",
    "articledf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_urls = articledf.url[~articledf.url.isin(urls)]\n",
    "len(new_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/paul/anaconda/lib/python2.7/site-packages/newspaper/parsers.py\", line 54, in fromstring\n",
      "    cls.doc = lxml.html.fromstring(html)\n",
      "  File \"/home/paul/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 706, in fromstring\n",
      "    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must download and parse an article before parsing it!\n",
      "error: http://www.rappad.co/freestyle\n",
      "done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/paul/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 600, in document_fromstring\n",
      "    value = etree.fromstring(html, parser, **kw)\n",
      "  File \"lxml.etree.pyx\", line 3032, in lxml.etree.fromstring (src/lxml/lxml.etree.c:68121)\n",
      "  File \"parser.pxi\", line 1786, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:102470)\n",
      "  File \"parser.pxi\", line 1667, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:101229)\n",
      "  File \"parser.pxi\", line 1035, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:96139)\n",
      "  File \"parser.pxi\", line 582, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:91290)\n",
      "  File \"parser.pxi\", line 683, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:92476)\n",
      "  File \"parser.pxi\", line 631, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:91904)\n",
      "XMLSyntaxError: line 1241: htmlParseEntityRef: expecting ';'\n"
     ]
    }
   ],
   "source": [
    "article_data = writeArticleJSON(new_urls,'./articles.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since some of the articles couldn't be downloaded (reasons could vary from the article no longer being available, rate limiting, bandwidth etc.) All of these issues could be explored but one of the joys of working with big data is being able to except a certain loss of data. Here we can see how many training threads we lost due to issues downloading the article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inlvalid JSON\n",
      "(11472, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>summary</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Derek Keats Flickr, Hannah Osborne, Marc Cirera]</td>\n",
       "      <td>{u'toxic': 1.01308139535, u'cancer': 1.0261627...</td>\n",
       "      <td>2015-11-10 16:00:00+00:00</td>\n",
       "      <td>Algae has been genetically engineered to kill ...</td>\n",
       "      <td>Algae has been genetically engineered to kill ...</td>\n",
       "      <td>http://www.ibtimes.co.uk/algae-genetically-eng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>{u'diamond': 1.04316546763, u'laser': 1.008633...</td>\n",
       "      <td>None</td>\n",
       "      <td>If Q-carbon is harder than diamond, why would ...</td>\n",
       "      <td>This is a collection of 0.02, 0.03 and 0.04 ca...</td>\n",
       "      <td>http://phys.org/news/2015-11-phase-carbon-diam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Bjorn Carey]</td>\n",
       "      <td>{u'obfuscation': 1.01181102362, u'fraud': 1.01...</td>\n",
       "      <td>2015-11-16 00:00:00</td>\n",
       "      <td>Stanford researchers uncover patterns in how s...</td>\n",
       "      <td>Stanford researchers uncover patterns in how s...</td>\n",
       "      <td>http://news.stanford.edu/news/2015/november/fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>{u'oil': 1.05142857143, u'microbes': 1.0114285...</td>\n",
       "      <td>None</td>\n",
       "      <td>And that leads to more questions about where m...</td>\n",
       "      <td>Samantha Joye, a professor of marine sciences ...</td>\n",
       "      <td>http://phys.org/news/2015-11-dispersants-oil-d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[More From]</td>\n",
       "      <td>{u'star': 1.02124645892, u'forming': 1.0106232...</td>\n",
       "      <td>2015-11-18 06:00:00</td>\n",
       "      <td>Together, the two observations allowed the sci...</td>\n",
       "      <td>Shares Share\\n\\nTweet\\n\\nE-mail\\n\\n​Of the tho...</td>\n",
       "      <td>http://www.popularmechanics.com/space/deep-spa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             authors  \\\n",
       "0  [Derek Keats Flickr, Hannah Osborne, Marc Cirera]   \n",
       "1                                                 []   \n",
       "2                                      [Bjorn Carey]   \n",
       "3                                                 []   \n",
       "4                                        [More From]   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  {u'toxic': 1.01308139535, u'cancer': 1.0261627...   \n",
       "1  {u'diamond': 1.04316546763, u'laser': 1.008633...   \n",
       "2  {u'obfuscation': 1.01181102362, u'fraud': 1.01...   \n",
       "3  {u'oil': 1.05142857143, u'microbes': 1.0114285...   \n",
       "4  {u'star': 1.02124645892, u'forming': 1.0106232...   \n",
       "\n",
       "                publish_date  \\\n",
       "0  2015-11-10 16:00:00+00:00   \n",
       "1                       None   \n",
       "2        2015-11-16 00:00:00   \n",
       "3                       None   \n",
       "4        2015-11-18 06:00:00   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Algae has been genetically engineered to kill ...   \n",
       "1  If Q-carbon is harder than diamond, why would ...   \n",
       "2  Stanford researchers uncover patterns in how s...   \n",
       "3  And that leads to more questions about where m...   \n",
       "4  Together, the two observations allowed the sci...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Algae has been genetically engineered to kill ...   \n",
       "1  This is a collection of 0.02, 0.03 and 0.04 ca...   \n",
       "2  Stanford researchers uncover patterns in how s...   \n",
       "3  Samantha Joye, a professor of marine sciences ...   \n",
       "4  Shares Share\\n\\nTweet\\n\\nE-mail\\n\\n​Of the tho...   \n",
       "\n",
       "                                                 url  \n",
       "0  http://www.ibtimes.co.uk/algae-genetically-eng...  \n",
       "1  http://phys.org/news/2015-11-phase-carbon-diam...  \n",
       "2  http://news.stanford.edu/news/2015/november/fr...  \n",
       "3  http://phys.org/news/2015-11-dispersants-oil-d...  \n",
       "4  http://www.popularmechanics.com/space/deep-spa...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = []\n",
    "with open('./articles.txt') as data_file:\n",
    "    for article in data_file:\n",
    "        try:\n",
    "            articles.append(pd.read_json(article,orient='records',typ='series'))\n",
    "        except:\n",
    "            print 'inlvalid JSON'\n",
    "articledf = pd.concat(articles,axis=1).transpose()\n",
    "print articledf.shape\n",
    "articledf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21268 Article URLs\n",
      "11420 Sucsessfull Downloads\n"
     ]
    }
   ],
   "source": [
    "print str(len(training_links.url.unique())) + ' Article URLs'\n",
    "linkdf = linkdf.loc[linkdf.url.isin(articledf.url)]\n",
    "print str(len(articledf.url.unique())) + ' Sucsessfull Downloads'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Collecting Training Comments</h2>\n",
    "\n",
    "Now that we know what links (articles) we are interested in. We need to go get the top comments. To maximise the relevencey to the original article we restrict our comment collector to the direct children of the article. We also want to control how many comments for each article we recive. The following function let's us do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "GetTrainingComments(link_list,depth, outfile)\n",
    "\n",
    "Description:\n",
    "############\n",
    "Collectes the first children for each provided link. \n",
    "\n",
    "Runtime(seconds) = len(link_list)/2\n",
    "\n",
    "Parameters:\n",
    "###########\n",
    "linkdf: a dataframe where each record is a link. \n",
    "        Required columns: subreddit, name\n",
    "\n",
    "outfile: path to output location\n",
    "\n",
    "depth: the max depth of comments to return. \n",
    "\n",
    "\n",
    "Outputs:\n",
    "#########\n",
    "Writes a single comment JSON per line to the outfile. \n",
    "\n",
    "Returns:\n",
    "##########\n",
    "\n",
    "\n",
    "'''\n",
    "url = \"https://oauth.reddit.com/r\"\n",
    "\n",
    "def getTrainingLinks(df,depth, outfile):\n",
    "    token = getToken(creds)\n",
    "    for subreddit, name in [tuple(x) for x in df[['subreddit','id']].values]:\n",
    "            query= 'comments/{0}/?depth={1}'.format(name,depth)\n",
    "            request_url= \"/\".join([url,subreddit,query])\n",
    "            response = requests.get(request_url, headers=token)\n",
    "            if not os.path.exists(os.path.dirname(outfile)):\n",
    "                os.makedirs(os.path.dirname(outfile))\n",
    "            with open(outfile,'a') as comment_file:\n",
    "                   for comment in response.json()[1]['data']['children'][:-1]:\n",
    "                        try:\n",
    "                            json.dump(comment['data'],comment_file)\n",
    "                            comment_file.write('\\n')\n",
    "                        except:\n",
    "                            print skipped\n",
    "    time.sleep(1) # So we respect the rate limits! \n",
    "    print 'done.'\n",
    "    return outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    }
   ],
   "source": [
    "training_comments = getTrainingLinks(linkdf,1,'./comment_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>body_html</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>created</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>downs</th>\n",
       "      <th>edited</th>\n",
       "      <th>gilded</th>\n",
       "      <th>id</th>\n",
       "      <th>likes</th>\n",
       "      <th>link_id</th>\n",
       "      <th>name</th>\n",
       "      <th>num_reports</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>replies</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>ups</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SirT6</td>\n",
       "      <td>The title sort of misses the point of the stud...</td>\n",
       "      <td>&amp;lt;div class=\"md\"&amp;gt;&amp;lt;p&amp;gt;The title sort ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.44728e+09</td>\n",
       "      <td>1.447251e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>cwwhtv7</td>\n",
       "      <td>None</td>\n",
       "      <td>t3_3se6lu</td>\n",
       "      <td>t1_cwwhtv7</td>\n",
       "      <td>None</td>\n",
       "      <td>t3_3se6lu</td>\n",
       "      <td>{u'kind': u'Listing', u'data': {u'modhash': No...</td>\n",
       "      <td>1359</td>\n",
       "      <td>science</td>\n",
       "      <td>1359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DrBiochemistry</td>\n",
       "      <td>Just want to point out that until I see a deli...</td>\n",
       "      <td>&amp;lt;div class=\"md\"&amp;gt;&amp;lt;p&amp;gt;Just want to po...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.447277e+09</td>\n",
       "      <td>1.447249e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>cwwgxle</td>\n",
       "      <td>None</td>\n",
       "      <td>t3_3se6lu</td>\n",
       "      <td>t1_cwwgxle</td>\n",
       "      <td>None</td>\n",
       "      <td>t3_3se6lu</td>\n",
       "      <td>{u'kind': u'Listing', u'data': {u'modhash': No...</td>\n",
       "      <td>3209</td>\n",
       "      <td>science</td>\n",
       "      <td>3209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Frogblood</td>\n",
       "      <td>It's an interesting idea but the in vitro and ...</td>\n",
       "      <td>&amp;lt;div class=\"md\"&amp;gt;&amp;lt;p&amp;gt;It&amp;amp;#39;s an...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.447276e+09</td>\n",
       "      <td>1.447247e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>cwwggxu</td>\n",
       "      <td>None</td>\n",
       "      <td>t3_3se6lu</td>\n",
       "      <td>t1_cwwggxu</td>\n",
       "      <td>None</td>\n",
       "      <td>t3_3se6lu</td>\n",
       "      <td>{u'kind': u'Listing', u'data': {u'modhash': No...</td>\n",
       "      <td>133</td>\n",
       "      <td>science</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mijn_ikke</td>\n",
       "      <td>Just waiting until somebody smarter than me co...</td>\n",
       "      <td>&amp;lt;div class=\"md\"&amp;gt;&amp;lt;p&amp;gt;Just waiting un...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.447276e+09</td>\n",
       "      <td>1.447247e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1.447249e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>cwwga6g</td>\n",
       "      <td>None</td>\n",
       "      <td>t3_3se6lu</td>\n",
       "      <td>t1_cwwga6g</td>\n",
       "      <td>None</td>\n",
       "      <td>t3_3se6lu</td>\n",
       "      <td>{u'kind': u'Listing', u'data': {u'modhash': No...</td>\n",
       "      <td>773</td>\n",
       "      <td>science</td>\n",
       "      <td>773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>awhitt8</td>\n",
       "      <td>Yes the title is sensationalized.\\n\\n&amp;gt;The m...</td>\n",
       "      <td>&amp;lt;div class=\"md\"&amp;gt;&amp;lt;p&amp;gt;Yes the title i...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.447285e+09</td>\n",
       "      <td>1.447256e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1.447259e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>cwwkopn</td>\n",
       "      <td>None</td>\n",
       "      <td>t3_3se6lu</td>\n",
       "      <td>t1_cwwkopn</td>\n",
       "      <td>None</td>\n",
       "      <td>t3_3se6lu</td>\n",
       "      <td>{u'kind': u'Listing', u'data': {u'modhash': No...</td>\n",
       "      <td>16</td>\n",
       "      <td>science</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           author                                               body  \\\n",
       "0           SirT6  The title sort of misses the point of the stud...   \n",
       "1  DrBiochemistry  Just want to point out that until I see a deli...   \n",
       "2       Frogblood  It's an interesting idea but the in vitro and ...   \n",
       "3       mijn_ikke  Just waiting until somebody smarter than me co...   \n",
       "4         awhitt8  Yes the title is sensationalized.\\n\\n&gt;The m...   \n",
       "\n",
       "                                           body_html controversiality  \\\n",
       "0  &lt;div class=\"md\"&gt;&lt;p&gt;The title sort ...                0   \n",
       "1  &lt;div class=\"md\"&gt;&lt;p&gt;Just want to po...                0   \n",
       "2  &lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s an...                0   \n",
       "3  &lt;div class=\"md\"&gt;&lt;p&gt;Just waiting un...                0   \n",
       "4  &lt;div class=\"md\"&gt;&lt;p&gt;Yes the title i...                0   \n",
       "\n",
       "        created   created_utc distinguished downs        edited gilded  \\\n",
       "0   1.44728e+09  1.447251e+09          None     0         False      1   \n",
       "1  1.447277e+09  1.447249e+09          None     0         False      0   \n",
       "2  1.447276e+09  1.447247e+09          None     0         False      0   \n",
       "3  1.447276e+09  1.447247e+09          None     0  1.447249e+09      1   \n",
       "4  1.447285e+09  1.447256e+09          None     0  1.447259e+09      0   \n",
       "\n",
       "        id likes    link_id        name num_reports  parent_id  \\\n",
       "0  cwwhtv7  None  t3_3se6lu  t1_cwwhtv7        None  t3_3se6lu   \n",
       "1  cwwgxle  None  t3_3se6lu  t1_cwwgxle        None  t3_3se6lu   \n",
       "2  cwwggxu  None  t3_3se6lu  t1_cwwggxu        None  t3_3se6lu   \n",
       "3  cwwga6g  None  t3_3se6lu  t1_cwwga6g        None  t3_3se6lu   \n",
       "4  cwwkopn  None  t3_3se6lu  t1_cwwkopn        None  t3_3se6lu   \n",
       "\n",
       "                                             replies score subreddit   ups  \n",
       "0  {u'kind': u'Listing', u'data': {u'modhash': No...  1359   science  1359  \n",
       "1  {u'kind': u'Listing', u'data': {u'modhash': No...  3209   science  3209  \n",
       "2  {u'kind': u'Listing', u'data': {u'modhash': No...   133   science   133  \n",
       "3  {u'kind': u'Listing', u'data': {u'modhash': No...   773   science   773  \n",
       "4  {u'kind': u'Listing', u'data': {u'modhash': No...    16   science    16  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = []\n",
    "#Tags to keep\n",
    "tags= [ u'author', u'body', u'body_html', u'controversiality', u'created', u'created_utc', u'distinguished', u'downs',\n",
    " u'edited', u'gilded', u'id', u'likes', u'link_id', u'name', u'num_reports', u'parent_id', u'replies', u'score',\n",
    " u'subreddit', u'ups']\n",
    "\n",
    "with open('./comment_data.txt') as data_file:\n",
    "    for comment in data_file:\n",
    "        comments.append(pd.read_json(comment,orient='records',typ='series')[tags])\n",
    "commentdf = pd.concat(comments,axis=1).transpose()\n",
    "\n",
    "#Remove ['removed'] comments\n",
    "commentdf = commentdf.loc[commentdf['body']!='[removed]']\n",
    "commentdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have links, articles and comments. \n",
    "\n",
    "Lets join links and articles are really just different features about the thread orgin. So lets join them.\n",
    "\n",
    "Now we have all of the data we need, we have articles and the corresponding comments. We created a few descriptive features for both the articles and the comments but we want to build a few more. But first we want to remove any incomplete records where we couldn't collect the article or the comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15611 Sample Articles\n",
      "293701 Sample Comments\n"
     ]
    }
   ],
   "source": [
    "#Subset For Complete Records\n",
    "#Remove links with no articles\n",
    "linkdf = linkdf.loc[linkdf.url.isin(articledf.url)]\n",
    "#Remove links with no comments\n",
    "commentdf['pid'] = commentdf.parent_id.apply(lambda x: str.split(str(x),'_')[1])\n",
    "train_links = linkdf.loc[linkdf['id'].isin(commentdf['pid'])]\n",
    "\n",
    "#Join Link Features and Article Features\n",
    "#Join on PID opposed to URL!!!\n",
    "train_articles = articledf.merge(train_links, on='url',how='left')\n",
    "\n",
    "train_comments = commentdf[commentdf.pid.isin(train_links['id'])]\n",
    "print str(train_articles.shape[0])+' Sample Articles'\n",
    "print str(train_comments.shape[0])+ ' Sample Comments'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets wrtie out training data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_articles.to_csv('./train_articles.csv',sep=',',index=False,encoding = 'utf-8')\n",
    "train_comments.to_csv('./train_comments.csv',sep=',',index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collection of the training data is complete, now move on the feature creation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
