{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Prediction Data Factory</h1>\n",
    "\n",
    "This notebook demonstrates how we take the keywords of a URL and create a feature data frame of comments that we can use to predict the highest scored in relation to the new article. This notebook is intended to demonstrate the back-end of the prediction process and is not intended to actually complete the prediction. \n",
    "<br>\n",
    "<br>\n",
    "These functions demonstrate how we complete the entire process preformed in the Collection and Feature Factory for Prediction Data.\n",
    "<br>\n",
    "<br>\n",
    "This is done so that we have comments from articles similar to the input URL. We then create features identical to the training data so that we can use our model to predict the most relevant out of these comments. The only difference here is that the similarities will be computed between the comment and the interest article opposed to the article they were originally about. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import requests.auth\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys\n",
    "import newspaper\n",
    "import nltk\n",
    "import operator\n",
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Get API Request Token</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credentials Verified: Token Recived\n"
     ]
    }
   ],
   "source": [
    "def getToken(creds):\n",
    "    client_auth = requests.auth.HTTPBasicAuth(creds['client_id'], creds['secret_id'])\n",
    "    post_data = {\"grant_type\": \"password\", \"username\": creds['username'], \"password\": creds['pw']}\n",
    "    headers = {\"User-Agent\": creds['user_agent']}\n",
    "    response = requests.post(\"https://www.reddit.com/api/v1/access_token\", auth=client_auth, data=post_data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        print 'Credentials Verified: Token Recived'\n",
    "    else:\n",
    "        print 'Invalid Creds'\n",
    "\n",
    "    auth = response.json()['token_type']+' '+response.json()['access_token']\n",
    "    return {\"Authorization\": auth, \"User-Agent\": creds['user_agent']}\n",
    "\n",
    "with open(\"creds.txt\") as f:\n",
    "    creds = dict([line.strip().split(':') for line in f])\n",
    "token = getToken(creds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define Required Functions</h2>\n",
    "\n",
    "All of these functions are utilized in the Collection Factory or the Feature Factory to create Training data. We use them to ensure the prediction data has an identical structure to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Gets the Article from the URL of the reddit thread\n",
    "def getArticle(url):\n",
    "    article = newspaper.Article(url, fetch_images = False)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    article.nlp()\n",
    "    return {\"url\":article.url, \"text\":article.text,\"keywords\":newspaper.nlp.keywords(article.text), \n",
    "            \"authors\":article.authors,\"summary\":article.summary,\n",
    "            \"publish_date\":str(article.publish_date)}\n",
    "\n",
    "#Creates Clean, Parsed Tokens (a bag of words) from a block of text\n",
    "def get_tokens(text):\n",
    "    lowers = text.lower()\n",
    "    clean = regex.sub('',lowers)\n",
    "    tokens=nltk.word_tokenize(clean)\n",
    "    return [w for w in tokens if not w in stopset]\n",
    "\n",
    "#Stems tokens to the root word\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "#Computes the Cosine Similarity between two bags of words\n",
    "import math\n",
    "def get_cosine(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "#Computes the Cosine between the Comment Keywords and the Article Keywords\n",
    "#Keywords are produced from the newspaper.nlp module\n",
    "def keyword_sim(comment_row):\n",
    "    try:\n",
    "        c_kw = comment_row['keywords']\n",
    "        a_kw = articles[articles['id']==comment_row['pid']]['keywords'].iloc[0]\n",
    "        sim = get_cosine(c_kw,a_kw)\n",
    "    except:\n",
    "        sim = 0.0\n",
    "    return sim\n",
    "\n",
    "#Computes Cosine between Comment Tokens and Article Tokens\n",
    "#Tokens are produced by the stem and get tokens fucntions defined above\n",
    "def token_sim(comment_row):\n",
    "    try:\n",
    "        c = comment_row['tokens']\n",
    "        a = articles[articles['id']==comment_row['pid']]['tokens'].iloc[0]\n",
    "        sim = get_cosine(c,a)\n",
    "    except:\n",
    "        sim = 0.0\n",
    "    return sim\n",
    "\n",
    "#Computes the cosine between all of a users tokens and the tokens of single article\n",
    "def user_vocab_tokens_sim(comment_row):\n",
    "    try:\n",
    "        c = user_vocab[comment_row.author]\n",
    "        a = articles[articles['id']==comment_row['pid']]['tokens'].iloc[0]\n",
    "        sim = get_cosine(c,a)\n",
    "    except:\n",
    "        sim = 0.0\n",
    "    return sim\n",
    "\n",
    "#Computes the cosine between all of a single users keywords and the keywords of single article \n",
    "def user_vocab_kw_sim(comment_row):\n",
    "    try:\n",
    "        c = user_vocab[comment_row.author]\n",
    "        a = articles[articles['id']==comment_row['pid']]['keywords'][0]\n",
    "        sim = get_cosine(c,a)\n",
    "    except:\n",
    "        sim = 0.0\n",
    "    return sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Description:\n",
    "############\n",
    "Collects comments from articles similar to 'keywords' and creates features for modelling.\n",
    "\n",
    "Parameters:\n",
    "###########\n",
    "subreddit_list: a list of subreddit names\n",
    "\n",
    "n_articles: Number of Articles to collect\n",
    "\n",
    "n_comments: number of comments per article\n",
    "\n",
    "time_window: one of ('hour', 'day', 'week', 'month', 'year', 'all')\n",
    "\n",
    "\n",
    "Returns:\n",
    "##########\n",
    "dict of feature dataframes:\n",
    "comments dataframe \n",
    "article data frame\n",
    "similarit data frame\n",
    "\n",
    "'''\n",
    "url = \"https://oauth.reddit.com/r\"\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "stopset = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def getPredictionData(keywords,n_links,n_comments,time_window):\n",
    "    #Get Token\n",
    "    token = getToken(creds)\n",
    "    #Create Search words\n",
    "    top3keywords = [x[0] for x in sorted(keywords.items(), key=operator.itemgetter(1),reverse=True)[:3]]\n",
    "    searchterms= \" \".join(top3keywords)\n",
    "    #Create Query String\n",
    "    query = \"all/search?limit={0}&type=link&t={1}&sort=relevance&q='{2}'\".format(n_links,time_window,searchterms)\n",
    "    request_url= \"/\".join([url,query])\n",
    "    #Make API Request to get Articles similar to the keywords\n",
    "    response = requests.get(request_url, headers=token)\n",
    "    tags= [ u'author',u'created_utc', u'domain', u'downs', u'gilded',u'is_self', u'likes', u'media', 'id',\n",
    "         u'num_comments', u'num_reports', u'over_18', u'permalink',u'score', u'selftext', u'subreddit', u'thumbnail', u'title', u'ups', u'url']\n",
    "    #Get The links for all of the articles\n",
    "    links = []\n",
    "    for link in response.json()['data']['children']:\n",
    "        links.append(pd.DataFrame.from_dict(link['data'],orient='index'))\n",
    "        linkdf = pd.concat(links,axis=1).transpose()[tags]\n",
    "    #Go get the actual articles\n",
    "    articles = []\n",
    "    for link_url in linkdf.url.unique():\n",
    "        articles.append(pd.DataFrame.from_dict(getArticle(link_url),orient='index'))\n",
    "        articledf = pd.concat(articles,axis=1).transpose()\n",
    "    #Go Get the comments for these articles\n",
    "    comments = []\n",
    "    tags= [ u'author', u'body', u'body_html', u'controversiality', u'created', u'created_utc', u'distinguished', u'downs',\n",
    "            u'edited', u'gilded', u'id', u'likes', u'link_id', u'name', u'num_reports', u'parent_id', u'replies', u'score',\n",
    "            u'subreddit', u'ups']\n",
    "    for subreddit, name in [tuple(x) for x in linkdf[['subreddit','id']].values]:\n",
    "            query= 'comments/{0}/?depth=1'.format(name)\n",
    "            request_url= \"/\".join([url,subreddit,query])\n",
    "            response = requests.get(request_url, headers=token)\n",
    "            for comment in response.json()[1]['data']['children'][:-1]:\n",
    "                try:\n",
    "                    comments.append(pd.DataFrame.from_dict(comment['data'],orient='index'))\n",
    "                    commentdf = pd.concat(comments,axis=1).transpose()[tags]\n",
    "                except:\n",
    "                    print 'skipped'\n",
    "    linkdf = linkdf.loc[linkdf.url.isin(articledf.url)]\n",
    "    #Remove links with no comments\n",
    "    commentdf['pid'] = commentdf.parent_id.apply(lambda x: str.split(str(x),'_')[1])\n",
    "    train_links = linkdf.loc[linkdf['id'].isin(commentdf['pid'])]\n",
    "    #Join Link Features and Article Features\n",
    "    pred_articles = articledf.merge(train_links, on='url',how='left')\n",
    "    pred_comments = commentdf[commentdf.pid.isin(train_links['id'])]\n",
    "    #Count Stemmed Tokens\n",
    "    pred_comments['tokens'] = pred_comments.body.apply(lambda text: Counter(stem_tokens(get_tokens(text), stemmer)))\n",
    "    #Comment Length\n",
    "    pred_comments['comment_length'] = pred_comments.body.apply(len)\n",
    "    #Number of Words\n",
    "    pred_comments['n_tokens'] = pred_comments.tokens.apply(len)\n",
    "    #Comment Keywords\n",
    "    pred_comments['keywords'] = pred_comments.body.apply(newspaper.nlp.keywords)\n",
    "    #Create Article Features\n",
    "    #########################\n",
    "    #Tokens\n",
    "    pred_articles['tokens'] = pred_articles['text'].apply(lambda text: Counter(stem_tokens(get_tokens(text), stemmer)))\n",
    "    #Article Length\n",
    "    pred_articles['article_len'] = pred_articles['text'].apply(len)\n",
    "    #Number of Words\n",
    "    pred_articles['n_tokens'] = pred_articles['text'].apply(len)\n",
    "\n",
    "    #Create Vocabularies for each user\n",
    "    user_vocab = {}\n",
    "    for author,vocab in pred_comments[['author','tokens']].groupby('author'):\n",
    "        user_vocab[author] = sum((Counter(dict(x)) for x in vocab.tokens),Counter())\n",
    "    \n",
    "    #Create Similarity Features\n",
    "    #########################\n",
    "    pred_comments['keyword_sim']=pred_comments.apply(keyword_sim,axis=1)\n",
    "    pred_comments['token_sim']=pred_comments.apply(token_sim,axis=1) \n",
    "    pred_comments['user_vocab_tokens']=pred_comments.apply(user_vocab_tokens_sim,axis=1)\n",
    "    pred_comments['user_vocab_kw']=pred_comments.apply(user_vocab_kw_sim,axis=1)\n",
    "    \n",
    "    #Return List of DataFrames\n",
    "    outdfs = {'pred_articles':pred_articles,'pred_comments':pred_comments}\n",
    "    print 'Done.'\n",
    "    return outdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Demonstrate with Sample Keywords</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'algae': 1.052325581395349,\n",
       " u'binds': 1.0348837209302326,\n",
       " u'cancer': 1.069767441860465,\n",
       " u'cells': 1.069767441860465,\n",
       " u'cellsthe': 1.0348837209302326,\n",
       " u'engineered': 1.0348837209302326,\n",
       " u'genetically': 1.0348837209302326,\n",
       " u'kill': 1.0348837209302326,\n",
       " u'molecules': 1.0348837209302326,\n",
       " u'target': 1.0348837209302326}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = newspaper.nlp.keywords(u'Algae has been genetically engineered to kill cancer cells without harming healthy cells.\\nThe algae nanoparticles, created by scientists in Australia, were found to kill 90% of cancer cells in cultured human cells.\\nThe antibody binds only to molecules found on cancer cells, thus delivering the toxic drug specifically to the target cells.\\nIn turn, the antibody binds only to molecules found on cancer cells, meaning it could deliver drugs to the target cells.\\nResearchers genetically engineered the algae to produce an antibody-binding protein on the surface of their shells.')\n",
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Run getPredictionData</h2>\n",
    "\n",
    "This sample only collects 5 similar articles. As this number increases the collection will take longer but there will be more comments to predict from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credentials Verified: Token Recived\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "sample_out = getPredictionData(keywords,5,1,'year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([          u'author',             u'body',        u'body_html',\n",
       "       u'controversiality',          u'created',      u'created_utc',\n",
       "          u'distinguished',            u'downs',           u'edited',\n",
       "                 u'gilded',               u'id',            u'likes',\n",
       "                u'link_id',             u'name',      u'num_reports',\n",
       "              u'parent_id',          u'replies',            u'score',\n",
       "              u'subreddit',              u'ups',              u'pid',\n",
       "                 u'tokens',   u'comment_length',         u'n_tokens',\n",
       "               u'keywords'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print sample_out['pred_comments'].columns\n",
    "print sample_out['pred_articles'].columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
