{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Find Your Expert</h1>\n",
    "<br>\n",
    "The purpose of our application is to identify the best comments for a user defined article. In order to provide the best and most relevent comments for articles that have somehow escapped the reddit verse, find similar articles that have been posted on and we present the best comments. We identify the best comments through a prediction model.\n",
    "<br>\n",
    "<h2>Prediction Strategy</h2>\n",
    "<img src=\"PredictionStrat.png\">\n",
    "The above proccess identifies our prediction strategy but we requrie a slightly different approach. In order to train our model we need to collect articles that *Do* exist in the reddit verse and the top comments related to that article. We can then use this data to extract the features from the comments and the relation to the article, and construct a model to predict the score of the comments. \n",
    "<br>\n",
    "<h2>Training Strategy</h2>\n",
    "<img src=\"TrainingStrat.png\">\n",
    "<br>\n",
    "<br>\n",
    "The collection strategy was designed to maximise effective data collection while adhearing to the <a href=\"https://www.reddit.com/wiki/licensing\">**Reddit API Guidlines**</a>.\n",
    "In order to provide the most effective training data the collection strategy was designed with the following coniderations\n",
    "<br>\n",
    "* Only collect comments directly in response to an orginal article post. We restrict our collection to first level responses in an effort to avoid collecting comments not directly related to the orginal article.\n",
    "* Collect from specific sub-reddits where the orginal content is usually an external source ie. r/news, r/TIL\n",
    "* Avoid sub-reddits where there is no orginal content like AMA's, or ELI5\n",
    "* Avoid multi-media subreddits like r/pics, r/videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Things!</h1>\n",
    "\n",
    "The Reddit API consits of several *things* (the base class). The API provided documentation on several different <a href='https://github.com/reddit/reddit/wiki/JSON'> *Things!*</a>; however we are most interested in the following:\n",
    "\n",
    "<h2>Links</h2>\n",
    "Links are the orginal post, all of the comments we are interested in our in response to a specific link. A link object has several tags that can all be found in the <a hfref= 'https://github.com/reddit/reddit/wiki/JSON'> *things documentation*</a>. The most relevent tags to our collection strategy are:\n",
    "\n",
    "|**Type**|**Tag**|**Description**|\n",
    "|--|--|-------------------------------|\n",
    "|String|id|this links identifier, e.g. \"8xwlg\"|\n",
    "|String|name|Fullname of link|\n",
    "|String|author|the account name of the poster.|\n",
    "|String|domain|the domain of this link. Self posts will be self.subreddit while other examples include en.wikipedia.org and s3.amazon.com\n",
    "|boolean|over_18|true if the post is tagged as NSFW. False if otherwise|\n",
    "|int|num_comments|the number of comments that belong to this link. includes removed comments.|\n",
    "|String|permalink|relative URL of the permanent link for this link|\n",
    "|int|score|the net-score of the link. note: A submission's score is simply the number of upvotes minus the number of downvotes. If five users like the submission and three users don't it will have a score of 2. Please note that the vote numbers are not \"real\" numbers, they have been \"fuzzed\" to prevent spam bots etc. So taking the above example, if five users upvoted the submission, and three users downvote it, the upvote/downvote numbers may say 23 upvotes and 21 downvotes, or 12 upvotes, and 10 downvotes. The points score is correct, but the vote totals are \"fuzzed\".|\n",
    "|String|subreddit|subreddit of thing excluding the /r/ prefix. \"pics\"|\n",
    "|String|title|the title of the link. may contain newlines for some reason|\n",
    "\n",
    "<br>\n",
    "<h2>Comments</h2>\n",
    "Comments are the actual comments made in regard to a specific link. The most relevent tags to our collection strategy are:\n",
    "\n",
    "|**Type**|**Tag**|**Description**|\n",
    "|--|--|-------------------------------|\n",
    "|String|id|this comment identifier, e.g. \"8xwlg\"|\n",
    "|String|name|Fullname of comment|\n",
    "|String|author|the account name of the poster.|\n",
    "|String|body|the raw text. this is the unformatted text which includes the raw markup characters are escaped.|\n",
    "|int|gilded|the number of times this comment received reddit gold|\n",
    "|String|link_id|ID of the link this comment is in|\n",
    "|String|parent_id|ID of the thing this comment is a reply to, either the link or a comment in it|\n",
    "|int|score|the net-score of the comment|\n",
    "\n",
    "\n",
    "<h2>Listings</h2>\n",
    "The last *Thing* we need is the listing class. In order to facilitate bulk data collection we can make use of the listing class to request list of the things we are interested in. The listing class literaly consists of a list of other things. A listing object is restricted to 100 Things so we also make use of the paging concept to get longer lists of things we want. The paging concept of the listing class is controlled by the following tags:\n",
    "\n",
    "|**Type**|**Tag**|**Description**|\n",
    "|--|--|-----------------| \n",
    "|String|before|The fullname of the listing that follows before this page. null if there is no previous page|\n",
    "|String|after|The fullname of the listing that follows after this page. null if there is no next page.|\n",
    "\n",
    "\n",
    "<h2>Header Information</h2>\n",
    "In addtion to the JSON objects themselves we also make use of the response headers to stay under the rate limiting. Reddit nicely asks to make no more than 30 requests per minute. We use the following response headers to ensure we play by the rules. \n",
    "\n",
    "|**Header Tag**|**Description**|\n",
    "|----|----------| \n",
    "|X-Ratelimit-Used|Approximate number of requests used in this period|\n",
    "|X-Ratelimit-Remaining| Approximate number of requests left to use|\n",
    "|X-Ratelimit-Reset|Approximate number of seconds to end of period|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create a Reddit API Request Token</h2>\n",
    "<br>\n",
    "Before we can start looking at *Things!* we need to we need to create a scripting application and an authorized token. The application is our vehicle for making authorized requests to the reddit API. The application **ExpertCollector/0.1 by cs109-2015** was created using https://github.com/reddit/reddit/wiki/OAuth2-Quick-Start-Example and named using reddit user agent naming conventions.\n",
    "<br>\n",
    "Once the application was created we can use the user and application credentials to request a token. User and application credentials are not to be shared and will only be distributed as required. The credentials text file is of the following format:\n",
    "<br>\n",
    "<br>\n",
    "client_id: application id<br>\n",
    "username: username<br>\n",
    "pw: password<br>\n",
    "secret_id<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import requests.auth\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getToken(creds):\n",
    "    client_auth = requests.auth.HTTPBasicAuth(creds['client_id'], creds['secret_id'])\n",
    "    post_data = {\"grant_type\": \"password\", \"username\": creds['username'], \"password\": creds['pw']}\n",
    "    headers = {\"User-Agent\": creds['user_agent']}\n",
    "    response = requests.post(\"https://www.reddit.com/api/v1/access_token\", auth=client_auth, data=post_data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        print 'Credentials Verified: Token Recived'\n",
    "    else:\n",
    "        print 'Invalid Creds'\n",
    "\n",
    "    auth = response.json()['token_type']+' '+response.json()['access_token']\n",
    "    return {\"Authorization\": auth, \"User-Agent\": creds['user_agent']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a created token and our application name to construct the authorized header for application requests. Our request token is valid for 1 hour so we may have to do this more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credentials Verified: Token Recived\n"
     ]
    }
   ],
   "source": [
    "with open(\"creds.txt\") as f:\n",
    "    creds = dict([line.strip().split(':') for line in f])\n",
    "token = getToken(creds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the created token to make authorized requests, through our application, directlty to the reddit API. As an example let's see how active our cs109-2015 reddit account has been."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'comment_karma': 0,\n",
       " u'created': 1448151333.0,\n",
       " u'created_utc': 1448122533.0,\n",
       " u'gold_creddits': 0,\n",
       " u'gold_expiration': None,\n",
       " u'has_mail': True,\n",
       " u'has_mod_mail': False,\n",
       " u'has_verified_email': False,\n",
       " u'hide_from_robots': False,\n",
       " u'id': u's9gd4',\n",
       " u'inbox_count': 1,\n",
       " u'is_gold': False,\n",
       " u'is_mod': False,\n",
       " u'is_suspended': False,\n",
       " u'link_karma': 1,\n",
       " u'name': u'cs109-2015',\n",
       " u'over_18': False,\n",
       " u'suspension_expiration_utc': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(\"https://oauth.reddit.com/api/v1/me\", headers=token)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Collecting Training Links</h2>\n",
    "<br>\n",
    "Now that we can make requests, our first step is to find links (articles) we can use for our training data. In order to get a broad set of relevent training data we aim to collect the Top *n* Links, in a specified time period,for a set of specific sub-reddits we are interested in. The following function lets us do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "GetTrainingLinks(subreddit_list,n_links,time_window)\n",
    "\n",
    "Description:\n",
    "############\n",
    "Collectes the top n links, in the specified time window, from each subreddit provided.\n",
    "\n",
    "Runtime(seconds) = n_links/50 * length(subreddit_list)\n",
    "\n",
    "Parameters:\n",
    "###########\n",
    "subreddit_list: a list of subreddit names\n",
    "\n",
    "n_links: total number of links per sub-reddit to collect\n",
    "\n",
    "time_window: one of ('hour', 'day', 'week', 'month', 'year', 'all')\n",
    "\n",
    "outfile: path to output location\n",
    "\n",
    "Outputs:\n",
    "#########\n",
    "Writes a single JSON per line to the outfile. \n",
    "\n",
    "Returns:\n",
    "##########\n",
    "\n",
    "\n",
    "'''\n",
    "url = \"https://oauth.reddit.com/r\"\n",
    "\n",
    "def getTrainingLinks(subreddit_list,n_links,time_window,outfile):\n",
    "    token = getToken(creds)\n",
    "    for subreddit in subreddit_list:\n",
    "        n=0\n",
    "        while n < n_links:\n",
    "            if n == 0:\n",
    "                query= 'top?limit=100&t={0}'.format(time_window)\n",
    "            else:\n",
    "                query= 'top?limit=100&t={0}&after={1}'.format(time_window,after)\n",
    "            request_url= \"/\".join([url,subreddit,query])\n",
    "            response = requests.get(request_url, headers=token)\n",
    "            after = response.json()['data']['after']\n",
    "            n+=100\n",
    "            if not os.path.exists(os.path.dirname(outfile)):\n",
    "                os.makedirs(os.path.dirname(outfile))\n",
    "            with open(outfile,'a') as link_file:\n",
    "                   for link in response.json()['data']['children']:\n",
    "                        json.dump(link['data'],link_file)\n",
    "                        link_file.write('\\n')\n",
    "            clear_output()\n",
    "            sys.stdout.write('{0} r/{1} Links Collected'.format(n,subreddit))\n",
    "            sys.stdout.flush()\n",
    "            time.sleep(2) # So we respect the rate limits! \n",
    "    \n",
    "    clear_output()\n",
    "    print 'Done.'\n",
    "    return outfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Sample Collection</h2>\n",
    "Here we specify our collection parameters, this is just a sample. The actual training collection prameters will be more extensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "subreddit_list = ['science','news','worldnews','dataisbeautiful','todayilearned']\n",
    "n_links = 1000\n",
    "time_window ='month'\n",
    "outfile ='./links.txt'\n",
    "\n",
    "training_links = getTrainingLinks(subreddit_list,n_links,time_window,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3988, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>domain</th>\n",
       "      <th>downs</th>\n",
       "      <th>gilded</th>\n",
       "      <th>is_self</th>\n",
       "      <th>likes</th>\n",
       "      <th>media</th>\n",
       "      <th>id</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>num_reports</th>\n",
       "      <th>over_18</th>\n",
       "      <th>permalink</th>\n",
       "      <th>score</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>title</th>\n",
       "      <th>ups</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bloomsey</td>\n",
       "      <td>1.446232e+09</td>\n",
       "      <td>thelatestnews.com</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3qvj7a</td>\n",
       "      <td>824</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/science/comments/3qvj7a/researchers_have_de...</td>\n",
       "      <td>8201</td>\n",
       "      <td></td>\n",
       "      <td>science</td>\n",
       "      <td>http://b.thumbs.redditmedia.com/RyxLNlGq2NUKfz...</td>\n",
       "      <td>Researchers have developed a blood test that c...</td>\n",
       "      <td>8201</td>\n",
       "      <td>http://www.thelatestnews.com/single-drop-of-bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trpftw</td>\n",
       "      <td>1.446582e+09</td>\n",
       "      <td>cnn.com</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3recdd</td>\n",
       "      <td>3965</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/science/comments/3recdd/mass_killings_and_s...</td>\n",
       "      <td>7174</td>\n",
       "      <td></td>\n",
       "      <td>science</td>\n",
       "      <td>http://b.thumbs.redditmedia.com/JA2kgMdHb-dCuQ...</td>\n",
       "      <td>Mass killings and school shootings spread \"con...</td>\n",
       "      <td>7174</td>\n",
       "      <td>http://www.cnn.com/2015/07/02/health/contagiou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the_phet</td>\n",
       "      <td>1.447239e+09</td>\n",
       "      <td>ibtimes.co.uk</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3se6lu</td>\n",
       "      <td>1078</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/science/comments/3se6lu/algae_has_been_gene...</td>\n",
       "      <td>6588</td>\n",
       "      <td></td>\n",
       "      <td>science</td>\n",
       "      <td>http://b.thumbs.redditmedia.com/y1CGKgl69hKw-s...</td>\n",
       "      <td>Algae has been genetically engineered to kill ...</td>\n",
       "      <td>6588</td>\n",
       "      <td>http://www.ibtimes.co.uk/algae-genetically-eng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>drewiepoodle</td>\n",
       "      <td>1.4456e+09</td>\n",
       "      <td>phys.org</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3pw7xy</td>\n",
       "      <td>1913</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/science/comments/3pw7xy/one_of_the_oddest_p...</td>\n",
       "      <td>6259</td>\n",
       "      <td></td>\n",
       "      <td>science</td>\n",
       "      <td>http://b.thumbs.redditmedia.com/pnxM2olTL3M4el...</td>\n",
       "      <td>One of the oddest predictions of quantum theor...</td>\n",
       "      <td>6259</td>\n",
       "      <td>http://phys.org/news/2015-10-zeno-effect-verif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>godsenfrik</td>\n",
       "      <td>1.447707e+09</td>\n",
       "      <td>news.stanford.edu</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3t2exx</td>\n",
       "      <td>708</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/science/comments/3t2exx/when_scientists_fal...</td>\n",
       "      <td>6151</td>\n",
       "      <td></td>\n",
       "      <td>science</td>\n",
       "      <td>http://b.thumbs.redditmedia.com/FQPskQP8EejVh9...</td>\n",
       "      <td>When scientists falsify data, they try to cove...</td>\n",
       "      <td>6151</td>\n",
       "      <td>http://news.stanford.edu/news/2015/november/fr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         author   created_utc             domain downs gilded is_self likes  \\\n",
       "0      Bloomsey  1.446232e+09  thelatestnews.com     0      0   False  None   \n",
       "1        trpftw  1.446582e+09            cnn.com     0      0   False  None   \n",
       "2      the_phet  1.447239e+09      ibtimes.co.uk     0      0   False  None   \n",
       "3  drewiepoodle    1.4456e+09           phys.org     0      0   False  None   \n",
       "4    godsenfrik  1.447707e+09  news.stanford.edu     0      0   False  None   \n",
       "\n",
       "  media      id num_comments num_reports over_18  \\\n",
       "0  None  3qvj7a          824        None   False   \n",
       "1  None  3recdd         3965        None   False   \n",
       "2  None  3se6lu         1078        None   False   \n",
       "3  None  3pw7xy         1913        None   False   \n",
       "4  None  3t2exx          708        None   False   \n",
       "\n",
       "                                           permalink score selftext subreddit  \\\n",
       "0  /r/science/comments/3qvj7a/researchers_have_de...  8201            science   \n",
       "1  /r/science/comments/3recdd/mass_killings_and_s...  7174            science   \n",
       "2  /r/science/comments/3se6lu/algae_has_been_gene...  6588            science   \n",
       "3  /r/science/comments/3pw7xy/one_of_the_oddest_p...  6259            science   \n",
       "4  /r/science/comments/3t2exx/when_scientists_fal...  6151            science   \n",
       "\n",
       "                                           thumbnail  \\\n",
       "0  http://b.thumbs.redditmedia.com/RyxLNlGq2NUKfz...   \n",
       "1  http://b.thumbs.redditmedia.com/JA2kgMdHb-dCuQ...   \n",
       "2  http://b.thumbs.redditmedia.com/y1CGKgl69hKw-s...   \n",
       "3  http://b.thumbs.redditmedia.com/pnxM2olTL3M4el...   \n",
       "4  http://b.thumbs.redditmedia.com/FQPskQP8EejVh9...   \n",
       "\n",
       "                                               title   ups  \\\n",
       "0  Researchers have developed a blood test that c...  8201   \n",
       "1  Mass killings and school shootings spread \"con...  7174   \n",
       "2  Algae has been genetically engineered to kill ...  6588   \n",
       "3  One of the oddest predictions of quantum theor...  6259   \n",
       "4  When scientists falsify data, they try to cove...  6151   \n",
       "\n",
       "                                                 url  \n",
       "0  http://www.thelatestnews.com/single-drop-of-bl...  \n",
       "1  http://www.cnn.com/2015/07/02/health/contagiou...  \n",
       "2  http://www.ibtimes.co.uk/algae-genetically-eng...  \n",
       "3  http://phys.org/news/2015-10-zeno-effect-verif...  \n",
       "4  http://news.stanford.edu/news/2015/november/fr...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = []\n",
    "#Tags to keep\n",
    "tags= [ u'author',u'created_utc', u'domain', u'downs', u'gilded',u'is_self', u'likes', u'media', 'id',\n",
    " u'num_comments', u'num_reports', u'over_18', u'permalink',u'score', u'selftext', u'subreddit', u'thumbnail', u'title', u'ups', u'url']\n",
    "\n",
    "with open(training_links) as data_file:\n",
    "    for link in data_file:\n",
    "        links.append(pd.read_json(link,orient='records',typ='series')[tags])\n",
    "linkdf = pd.concat(links,axis=1).transpose()\n",
    "print linkdf.shape\n",
    "linkdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Finding Usefull Links</h2>\n",
    "<br>\n",
    "Now we have a dataframe of potential links but before we can collect the comments we do some prunning. We remove links based on the following criteria:\n",
    "\n",
    "* Remove any potential NSFW links using the over_18 tag\n",
    "* Remove any links with no article using the is_self tag\n",
    "* Remove links with fewer than 10 comments\n",
    "* Remove any duplicate URLs. \n",
    "\n",
    "We write the list of unique article urls so we can go get the original article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_links = linkdf.loc[(~linkdf.is_self.fillna(False)) & (~linkdf.over_18.fillna(False)) & (linkdf.num_comments > 10),:]\n",
    "training_links.url.unique().tofile('train_urls.txt',\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Collecting Training Comments</h2>\n",
    "\n",
    "Now that we know what links (articles) we are interested in. We need to go get the top comments. To maximise the relevencey to the original article we restrict our comment collector to the direct children of the article. We also want to control how many comments for each article we recive. The following function let's us do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "GetTrainingComments(link_list,depth, outfile)\n",
    "\n",
    "Description:\n",
    "############\n",
    "Collectes the first children for each provided link. \n",
    "\n",
    "Runtime(seconds) = len(link_list)/2\n",
    "\n",
    "Parameters:\n",
    "###########\n",
    "linkdf: a dataframe where each record is a link. \n",
    "        Required columns: subreddit, name\n",
    "\n",
    "outfile: path to output location\n",
    "\n",
    "depth: the max depth of comments to return. \n",
    "\n",
    "\n",
    "Outputs:\n",
    "#########\n",
    "Writes a single comment JSON per line to the outfile. \n",
    "\n",
    "Returns:\n",
    "##########\n",
    "\n",
    "\n",
    "'''\n",
    "url = \"https://oauth.reddit.com/r\"\n",
    "\n",
    "def getTrainingLinks(df,depth, outfile):\n",
    "    token = getToken(creds)\n",
    "    for subreddit, name in [tuple(x) for x in df[['subreddit','id']].values]:\n",
    "            query= 'comments/{0}/?depth={1}'.format(name,depth)\n",
    "            request_url= \"/\".join([url,subreddit,query])\n",
    "            response = requests.get(request_url, headers=token)\n",
    "            if not os.path.exists(os.path.dirname(outfile)):\n",
    "                os.makedirs(os.path.dirname(outfile))\n",
    "            with open(outfile,'a') as comment_file:\n",
    "                   for comment in response.json()[1]['data']['children'][:-1]:\n",
    "                        json.dump(comment['data'],comment_file)\n",
    "                        comment_file.write('\\n')\n",
    "    time.sleep(2) # So we respect the rate limits! \n",
    "    print 'Done.'\n",
    "    return outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credentials Verified: Token Recived\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "training_comments = getTrainingLinks(linkdf,1,'./comments.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>body_html</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>created</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>downs</th>\n",
       "      <th>edited</th>\n",
       "      <th>gilded</th>\n",
       "      <th>id</th>\n",
       "      <th>likes</th>\n",
       "      <th>link_id</th>\n",
       "      <th>name</th>\n",
       "      <th>num_reports</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>replies</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>ups</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Voerendaalse</td>\n",
       "      <td>&amp;gt; Subsequent validation using a separate va...</td>\n",
       "      <td>&amp;lt;div class=\"md\"&amp;gt;&amp;lt;blockquote&amp;gt;\\n&amp;lt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.446262e+09</td>\n",
       "      <td>1.446233e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1.446234e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>cwipf39</td>\n",
       "      <td>None</td>\n",
       "      <td>t3_3qvj7a</td>\n",
       "      <td>t1_cwipf39</td>\n",
       "      <td>None</td>\n",
       "      <td>t3_3qvj7a</td>\n",
       "      <td>{u'kind': u'Listing', u'data': {u'modhash': No...</td>\n",
       "      <td>1685</td>\n",
       "      <td>science</td>\n",
       "      <td>1685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bloomsey</td>\n",
       "      <td>Peer-reviewed article: http://www.cell.com/can...</td>\n",
       "      <td>&amp;lt;div class=\"md\"&amp;gt;&amp;lt;p&amp;gt;Peer-reviewed a...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.446261e+09</td>\n",
       "      <td>1.446232e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>cwiohv5</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_3qvj7a</td>\n",
       "      <td>t1_cwiohv5</td>\n",
       "      <td>None</td>\n",
       "      <td>t3_3qvj7a</td>\n",
       "      <td>{u'kind': u'Listing', u'data': {u'modhash': No...</td>\n",
       "      <td>660</td>\n",
       "      <td>science</td>\n",
       "      <td>660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>schnupfndrache7</td>\n",
       "      <td>is this as revolutionary as it sounds?</td>\n",
       "      <td>&amp;lt;div class=\"md\"&amp;gt;&amp;lt;p&amp;gt;is this as revo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.446264e+09</td>\n",
       "      <td>1.446235e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>cwiqorg</td>\n",
       "      <td>None</td>\n",
       "      <td>t3_3qvj7a</td>\n",
       "      <td>t1_cwiqorg</td>\n",
       "      <td>None</td>\n",
       "      <td>t3_3qvj7a</td>\n",
       "      <td>{u'kind': u'Listing', u'data': {u'modhash': No...</td>\n",
       "      <td>275</td>\n",
       "      <td>science</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>geoffp82</td>\n",
       "      <td>And then what? Full body MRI?</td>\n",
       "      <td>&amp;lt;div class=\"md\"&amp;gt;&amp;lt;p&amp;gt;And then what? ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.446262e+09</td>\n",
       "      <td>1.446233e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>cwipio1</td>\n",
       "      <td>None</td>\n",
       "      <td>t3_3qvj7a</td>\n",
       "      <td>t1_cwipio1</td>\n",
       "      <td>None</td>\n",
       "      <td>t3_3qvj7a</td>\n",
       "      <td>{u'kind': u'Listing', u'data': {u'modhash': No...</td>\n",
       "      <td>64</td>\n",
       "      <td>science</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>momoneymoproblemss</td>\n",
       "      <td>Is it weird I would like to take this test on ...</td>\n",
       "      <td>&amp;lt;div class=\"md\"&amp;gt;&amp;lt;p&amp;gt;Is it weird I w...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.446263e+09</td>\n",
       "      <td>1.446235e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>cwiqags</td>\n",
       "      <td>None</td>\n",
       "      <td>t3_3qvj7a</td>\n",
       "      <td>t1_cwiqags</td>\n",
       "      <td>None</td>\n",
       "      <td>t3_3qvj7a</td>\n",
       "      <td>{u'kind': u'Listing', u'data': {u'modhash': No...</td>\n",
       "      <td>131</td>\n",
       "      <td>science</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                                               body  \\\n",
       "0        Voerendaalse  &gt; Subsequent validation using a separate va...   \n",
       "1            Bloomsey  Peer-reviewed article: http://www.cell.com/can...   \n",
       "2     schnupfndrache7             is this as revolutionary as it sounds?   \n",
       "3            geoffp82                      And then what? Full body MRI?   \n",
       "4  momoneymoproblemss  Is it weird I would like to take this test on ...   \n",
       "\n",
       "                                           body_html controversiality  \\\n",
       "0  &lt;div class=\"md\"&gt;&lt;blockquote&gt;\\n&lt;...                0   \n",
       "1  &lt;div class=\"md\"&gt;&lt;p&gt;Peer-reviewed a...                0   \n",
       "2  &lt;div class=\"md\"&gt;&lt;p&gt;is this as revo...                0   \n",
       "3  &lt;div class=\"md\"&gt;&lt;p&gt;And then what? ...                0   \n",
       "4  &lt;div class=\"md\"&gt;&lt;p&gt;Is it weird I w...                0   \n",
       "\n",
       "        created   created_utc distinguished downs        edited gilded  \\\n",
       "0  1.446262e+09  1.446233e+09          None     0  1.446234e+09      0   \n",
       "1  1.446261e+09  1.446232e+09          None     0         False      0   \n",
       "2  1.446264e+09  1.446235e+09          None     0         False      0   \n",
       "3  1.446262e+09  1.446233e+09          None     0         False      0   \n",
       "4  1.446263e+09  1.446235e+09          None     0         False      0   \n",
       "\n",
       "        id  likes    link_id        name num_reports  parent_id  \\\n",
       "0  cwipf39   None  t3_3qvj7a  t1_cwipf39        None  t3_3qvj7a   \n",
       "1  cwiohv5  False  t3_3qvj7a  t1_cwiohv5        None  t3_3qvj7a   \n",
       "2  cwiqorg   None  t3_3qvj7a  t1_cwiqorg        None  t3_3qvj7a   \n",
       "3  cwipio1   None  t3_3qvj7a  t1_cwipio1        None  t3_3qvj7a   \n",
       "4  cwiqags   None  t3_3qvj7a  t1_cwiqags        None  t3_3qvj7a   \n",
       "\n",
       "                                             replies score subreddit   ups  \n",
       "0  {u'kind': u'Listing', u'data': {u'modhash': No...  1685   science  1685  \n",
       "1  {u'kind': u'Listing', u'data': {u'modhash': No...   660   science   660  \n",
       "2  {u'kind': u'Listing', u'data': {u'modhash': No...   275   science   275  \n",
       "3  {u'kind': u'Listing', u'data': {u'modhash': No...    64   science    64  \n",
       "4  {u'kind': u'Listing', u'data': {u'modhash': No...   131   science   131  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = []\n",
    "#Tags to keep\n",
    "tags= [ u'author', u'body', u'body_html', u'controversiality', u'created', u'created_utc', u'distinguished', u'downs',\n",
    " u'edited', u'gilded', u'id', u'likes', u'link_id', u'name', u'num_reports', u'parent_id', u'replies', u'score',\n",
    " u'subreddit', u'ups']\n",
    "\n",
    "with open(training_comments) as data_file:\n",
    "    for comment in data_file:\n",
    "        comments.append(pd.read_json(comment,orient='records',typ='series')[tags])\n",
    "commentdf = pd.concat(comments,axis=1).transpose()\n",
    "commentdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109964, 20)\n"
     ]
    }
   ],
   "source": [
    "print commentdf.shape\n",
    "commentdf.to_csv('./training_comments.csv',sep=',',encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda/lib/python2.7/site-packages/pandas/io/parsers.py:1170: DtypeWarning: Columns (6,7,9,11,14,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = self._reader.read(nrows)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>body_html</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>created</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>downs</th>\n",
       "      <th>edited</th>\n",
       "      <th>gilded</th>\n",
       "      <th>id</th>\n",
       "      <th>likes</th>\n",
       "      <th>link_id</th>\n",
       "      <th>name</th>\n",
       "      <th>num_reports</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>replies</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>ups</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Voerendaalse</td>\n",
       "      <td>&amp;gt; Subsequent validation using a separate va...</td>\n",
       "      <td>&amp;lt;div class=\"md\"&amp;gt;&amp;lt;blockquote&amp;gt;\\n&amp;lt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>1446262140</td>\n",
       "      <td>1446233340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1446234176.0</td>\n",
       "      <td>0</td>\n",
       "      <td>cwipf39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t3_3qvj7a</td>\n",
       "      <td>t1_cwipf39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t3_3qvj7a</td>\n",
       "      <td>{u'kind': u'Listing', u'data': {u'modhash': No...</td>\n",
       "      <td>1685</td>\n",
       "      <td>science</td>\n",
       "      <td>1685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bloomsey</td>\n",
       "      <td>Peer-reviewed article: http://www.cell.com/can...</td>\n",
       "      <td>&amp;lt;div class=\"md\"&amp;gt;&amp;lt;p&amp;gt;Peer-reviewed a...</td>\n",
       "      <td>0</td>\n",
       "      <td>1446260748</td>\n",
       "      <td>1446231948</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>cwiohv5</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_3qvj7a</td>\n",
       "      <td>t1_cwiohv5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t3_3qvj7a</td>\n",
       "      <td>{u'kind': u'Listing', u'data': {u'modhash': No...</td>\n",
       "      <td>660</td>\n",
       "      <td>science</td>\n",
       "      <td>660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>schnupfndrache7</td>\n",
       "      <td>is this as revolutionary as it sounds?</td>\n",
       "      <td>&amp;lt;div class=\"md\"&amp;gt;&amp;lt;p&amp;gt;is this as revo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1446264074</td>\n",
       "      <td>1446235274</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>cwiqorg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t3_3qvj7a</td>\n",
       "      <td>t1_cwiqorg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t3_3qvj7a</td>\n",
       "      <td>{u'kind': u'Listing', u'data': {u'modhash': No...</td>\n",
       "      <td>275</td>\n",
       "      <td>science</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>geoffp82</td>\n",
       "      <td>And then what? Full body MRI?</td>\n",
       "      <td>&amp;lt;div class=\"md\"&amp;gt;&amp;lt;p&amp;gt;And then what? ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1446262287</td>\n",
       "      <td>1446233487</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>cwipio1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t3_3qvj7a</td>\n",
       "      <td>t1_cwipio1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t3_3qvj7a</td>\n",
       "      <td>{u'kind': u'Listing', u'data': {u'modhash': No...</td>\n",
       "      <td>64</td>\n",
       "      <td>science</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>momoneymoproblemss</td>\n",
       "      <td>Is it weird I would like to take this test on ...</td>\n",
       "      <td>&amp;lt;div class=\"md\"&amp;gt;&amp;lt;p&amp;gt;Is it weird I w...</td>\n",
       "      <td>0</td>\n",
       "      <td>1446263469</td>\n",
       "      <td>1446234669</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>cwiqags</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t3_3qvj7a</td>\n",
       "      <td>t1_cwiqags</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t3_3qvj7a</td>\n",
       "      <td>{u'kind': u'Listing', u'data': {u'modhash': No...</td>\n",
       "      <td>131</td>\n",
       "      <td>science</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                                               body  \\\n",
       "0        Voerendaalse  &gt; Subsequent validation using a separate va...   \n",
       "1            Bloomsey  Peer-reviewed article: http://www.cell.com/can...   \n",
       "2     schnupfndrache7             is this as revolutionary as it sounds?   \n",
       "3            geoffp82                      And then what? Full body MRI?   \n",
       "4  momoneymoproblemss  Is it weird I would like to take this test on ...   \n",
       "\n",
       "                                           body_html  controversiality  \\\n",
       "0  &lt;div class=\"md\"&gt;&lt;blockquote&gt;\\n&lt;...                 0   \n",
       "1  &lt;div class=\"md\"&gt;&lt;p&gt;Peer-reviewed a...                 0   \n",
       "2  &lt;div class=\"md\"&gt;&lt;p&gt;is this as revo...                 0   \n",
       "3  &lt;div class=\"md\"&gt;&lt;p&gt;And then what? ...                 0   \n",
       "4  &lt;div class=\"md\"&gt;&lt;p&gt;Is it weird I w...                 0   \n",
       "\n",
       "      created  created_utc distinguished downs        edited gilded       id  \\\n",
       "0  1446262140   1446233340           NaN     0  1446234176.0      0  cwipf39   \n",
       "1  1446260748   1446231948           NaN     0         False      0  cwiohv5   \n",
       "2  1446264074   1446235274           NaN     0         False      0  cwiqorg   \n",
       "3  1446262287   1446233487           NaN     0         False      0  cwipio1   \n",
       "4  1446263469   1446234669           NaN     0         False      0  cwiqags   \n",
       "\n",
       "   likes    link_id        name num_reports  parent_id  \\\n",
       "0    NaN  t3_3qvj7a  t1_cwipf39         NaN  t3_3qvj7a   \n",
       "1  False  t3_3qvj7a  t1_cwiohv5         NaN  t3_3qvj7a   \n",
       "2    NaN  t3_3qvj7a  t1_cwiqorg         NaN  t3_3qvj7a   \n",
       "3    NaN  t3_3qvj7a  t1_cwipio1         NaN  t3_3qvj7a   \n",
       "4    NaN  t3_3qvj7a  t1_cwiqags         NaN  t3_3qvj7a   \n",
       "\n",
       "                                             replies score subreddit   ups  \n",
       "0  {u'kind': u'Listing', u'data': {u'modhash': No...  1685   science  1685  \n",
       "1  {u'kind': u'Listing', u'data': {u'modhash': No...   660   science   660  \n",
       "2  {u'kind': u'Listing', u'data': {u'modhash': No...   275   science   275  \n",
       "3  {u'kind': u'Listing', u'data': {u'modhash': No...    64   science    64  \n",
       "4  {u'kind': u'Listing', u'data': {u'modhash': No...   131   science   131  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = pd.read_csv('./training_comments.csv',encoding='utf-8')\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Collecting Prediction Links!</h1>\n",
    "Use the *Search* URL to collect the links most similar to the requested article!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
