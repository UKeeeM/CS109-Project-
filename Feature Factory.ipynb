{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Feature Factory</h2>\n",
    "This is the mechanism for extracting usefull attributes from the comments, the articles and the relationship between them. Some of the fetures were created at collection time to reduce the amount of data that would be stored; but we expand on those features here. The premise of our application is that the top comments, will be sytanctically similar to the articles they are describing. For example, comments and users that use the same language of the article may increase the overall score of that article. The feature factory produces a large variety of these comparsions and we use EDA to determine the ones with the most predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import newspaper\n",
    "import ast\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Read in the Collected Comments</h2>\n",
    "\n",
    "This reads in the csv produced by the collection factory, reformats some of the field types and removes any duplicated records. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(275686, 25)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>body_html</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>created</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>downs</th>\n",
       "      <th>edited</th>\n",
       "      <th>gilded</th>\n",
       "      <th>...</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>replies</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>ups</th>\n",
       "      <th>pid</th>\n",
       "      <th>tokens</th>\n",
       "      <th>comment_length</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SirT6</td>\n",
       "      <td>The title sort of misses the point of the stud...</td>\n",
       "      <td>&amp;lt;div class=\"md\"&amp;gt;&amp;lt;p&amp;gt;The title sort ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1447279564</td>\n",
       "      <td>1447250764</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>t3_3se6lu</td>\n",
       "      <td>{u'kind': u'Listing', u'data': {u'modhash': No...</td>\n",
       "      <td>1359</td>\n",
       "      <td>science</td>\n",
       "      <td>1359</td>\n",
       "      <td>3se6lu</td>\n",
       "      <td>Counter({'alga': 5, 'cancer': 4, 'cell': 4, 'd...</td>\n",
       "      <td>869</td>\n",
       "      <td>52</td>\n",
       "      <td>{u'toxinalgae': 1.00993377483, u'cancer': 1.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DrBiochemistry</td>\n",
       "      <td>Just want to point out that until I see a deli...</td>\n",
       "      <td>&amp;lt;div class=\"md\"&amp;gt;&amp;lt;p&amp;gt;Just want to po...</td>\n",
       "      <td>0</td>\n",
       "      <td>1447277409</td>\n",
       "      <td>1447248609</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>t3_3se6lu</td>\n",
       "      <td>{u'kind': u'Listing', u'data': {u'modhash': No...</td>\n",
       "      <td>3209</td>\n",
       "      <td>science</td>\n",
       "      <td>3209</td>\n",
       "      <td>3se6lu</td>\n",
       "      <td>Counter({'kill': 2, 'deliveri': 2, 'cancer': 1...</td>\n",
       "      <td>307</td>\n",
       "      <td>30</td>\n",
       "      <td>{u'survives': 1.02941176471, u'thing': 1.02941...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Frogblood</td>\n",
       "      <td>It's an interesting idea but the in vitro and ...</td>\n",
       "      <td>&amp;lt;div class=\"md\"&amp;gt;&amp;lt;p&amp;gt;It&amp;amp;#39;s an...</td>\n",
       "      <td>0</td>\n",
       "      <td>1447276156</td>\n",
       "      <td>1447247356</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>t3_3se6lu</td>\n",
       "      <td>{u'kind': u'Listing', u'data': {u'modhash': No...</td>\n",
       "      <td>133</td>\n",
       "      <td>science</td>\n",
       "      <td>133</td>\n",
       "      <td>3se6lu</td>\n",
       "      <td>Counter({'idea': 2, 'target': 2, 'overexcit': ...</td>\n",
       "      <td>432</td>\n",
       "      <td>39</td>\n",
       "      <td>{u'tumour': 1.02173913043, u'targeting': 1.043...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mijn_ikke</td>\n",
       "      <td>Just waiting until somebody smarter than me co...</td>\n",
       "      <td>&amp;lt;div class=\"md\"&amp;gt;&amp;lt;p&amp;gt;Just waiting un...</td>\n",
       "      <td>0</td>\n",
       "      <td>1447275611</td>\n",
       "      <td>1447246811</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1447248944.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>t3_3se6lu</td>\n",
       "      <td>{u'kind': u'Listing', u'data': {u'modhash': No...</td>\n",
       "      <td>773</td>\n",
       "      <td>science</td>\n",
       "      <td>773</td>\n",
       "      <td>3se6lu</td>\n",
       "      <td>Counter({'thank': 1, 'gold': 1, 'point': 1, 'e...</td>\n",
       "      <td>163</td>\n",
       "      <td>12</td>\n",
       "      <td>{u'somebody': 1.05172413793, u'gold': 1.051724...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>awhitt8</td>\n",
       "      <td>Yes the title is sensationalized.\\n\\n&amp;gt;The m...</td>\n",
       "      <td>&amp;lt;div class=\"md\"&amp;gt;&amp;lt;p&amp;gt;Yes the title i...</td>\n",
       "      <td>0</td>\n",
       "      <td>1447284967</td>\n",
       "      <td>1447256167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1447259263.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>t3_3se6lu</td>\n",
       "      <td>{u'kind': u'Listing', u'data': {u'modhash': No...</td>\n",
       "      <td>16</td>\n",
       "      <td>science</td>\n",
       "      <td>16</td>\n",
       "      <td>3se6lu</td>\n",
       "      <td>Counter({'drug': 5, 'deliveri': 4, 'materi': 3...</td>\n",
       "      <td>1447</td>\n",
       "      <td>104</td>\n",
       "      <td>{u'silicon': 1.01530612245, u'tissue': 1.01530...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           author                                               body  \\\n",
       "0           SirT6  The title sort of misses the point of the stud...   \n",
       "1  DrBiochemistry  Just want to point out that until I see a deli...   \n",
       "2       Frogblood  It's an interesting idea but the in vitro and ...   \n",
       "3       mijn_ikke  Just waiting until somebody smarter than me co...   \n",
       "4         awhitt8  Yes the title is sensationalized.\\n\\n&gt;The m...   \n",
       "\n",
       "                                           body_html  controversiality  \\\n",
       "0  &lt;div class=\"md\"&gt;&lt;p&gt;The title sort ...                 0   \n",
       "1  &lt;div class=\"md\"&gt;&lt;p&gt;Just want to po...                 0   \n",
       "2  &lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s an...                 0   \n",
       "3  &lt;div class=\"md\"&gt;&lt;p&gt;Just waiting un...                 0   \n",
       "4  &lt;div class=\"md\"&gt;&lt;p&gt;Yes the title i...                 0   \n",
       "\n",
       "      created  created_utc distinguished downs        edited gilded  \\\n",
       "0  1447279564   1447250764           NaN     0         False      1   \n",
       "1  1447277409   1447248609           NaN     0         False      0   \n",
       "2  1447276156   1447247356           NaN     0         False      0   \n",
       "3  1447275611   1447246811           NaN     0  1447248944.0      1   \n",
       "4  1447284967   1447256167           NaN     0  1447259263.0      0   \n",
       "\n",
       "                         ...                          parent_id  \\\n",
       "0                        ...                          t3_3se6lu   \n",
       "1                        ...                          t3_3se6lu   \n",
       "2                        ...                          t3_3se6lu   \n",
       "3                        ...                          t3_3se6lu   \n",
       "4                        ...                          t3_3se6lu   \n",
       "\n",
       "                                             replies score subreddit   ups  \\\n",
       "0  {u'kind': u'Listing', u'data': {u'modhash': No...  1359   science  1359   \n",
       "1  {u'kind': u'Listing', u'data': {u'modhash': No...  3209   science  3209   \n",
       "2  {u'kind': u'Listing', u'data': {u'modhash': No...   133   science   133   \n",
       "3  {u'kind': u'Listing', u'data': {u'modhash': No...   773   science   773   \n",
       "4  {u'kind': u'Listing', u'data': {u'modhash': No...    16   science    16   \n",
       "\n",
       "      pid                                             tokens comment_length  \\\n",
       "0  3se6lu  Counter({'alga': 5, 'cancer': 4, 'cell': 4, 'd...            869   \n",
       "1  3se6lu  Counter({'kill': 2, 'deliveri': 2, 'cancer': 1...            307   \n",
       "2  3se6lu  Counter({'idea': 2, 'target': 2, 'overexcit': ...            432   \n",
       "3  3se6lu  Counter({'thank': 1, 'gold': 1, 'point': 1, 'e...            163   \n",
       "4  3se6lu  Counter({'drug': 5, 'deliveri': 4, 'materi': 3...           1447   \n",
       "\n",
       "  n_tokens                                           keywords  \n",
       "0       52  {u'toxinalgae': 1.00993377483, u'cancer': 1.03...  \n",
       "1       30  {u'survives': 1.02941176471, u'thing': 1.02941...  \n",
       "2       39  {u'tumour': 1.02173913043, u'targeting': 1.043...  \n",
       "3       12  {u'somebody': 1.05172413793, u'gold': 1.051724...  \n",
       "4      104  {u'silicon': 1.01530612245, u'tissue': 1.01530...  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = pd.read_csv('./train_comments.csv',low_memory=False)\n",
    "comments[['body']] = comments[['body']].astype(str)\n",
    "comments['keywords'] = comments.keywords.apply(ast.literal_eval)\n",
    "comments.drop_duplicates(inplace=True,subset='id')\n",
    "comments.reset_index(drop=True)\n",
    "print comments.shape\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Read in the Articles</h2>\n",
    "Here we read in the article csv produced by the collection factory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles = pd.read_csv('./train_articles.csv',low_memory=False)\n",
    "articles[['text','summary']] = articles[['text','summary']].astype(str)\n",
    "articles.drop_duplicates(inplace=True,subset = 'url')\n",
    "articles['keywords'] = articles.keywords.apply(ast.literal_eval)\n",
    "articles.reset_index(drop=True,inplace=True)\n",
    "print articles.shape\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create Comment Features</h2>\n",
    "Here we create freatures strictly from the comments, a large variety of attribute features were collected in the colelction factory however we are most interested in examining the content of the comments. In order to examine the content we want to create parse, stemmed token for each block of text. We also use newspaper.nlp.keywords() on the comment text so that we can compare the keywords of the comment to the keywords of the articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'author', u'body', u'body_html', u'controversiality', u'created',\n",
      "       u'created_utc', u'distinguished', u'downs', u'edited', u'gilded', u'id',\n",
      "       u'likes', u'link_id', u'name', u'num_reports', u'parent_id', u'replies',\n",
      "       u'score', u'subreddit', u'ups', u'pid', u'tokens', u'comment_length',\n",
      "       u'n_tokens', u'keywords'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Create Comment Features\n",
    "########################\n",
    "#Tokenized Comments\n",
    "\n",
    "import re, string\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "stopset = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def get_tokens(text):\n",
    "    lowers = text.lower()\n",
    "    clean = regex.sub('',lowers)\n",
    "    tokens=nltk.word_tokenize(clean)\n",
    "    return [w for w in tokens if not w in stopset]\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "#Count Stemmed Tokens\n",
    "comments['tokens'] = comments.body.apply(lambda text: Counter(stem_tokens(get_tokens(text), stemmer)))\n",
    "\n",
    "#Comment Length\n",
    "comments['comment_length'] = comments.body.apply(len)\n",
    "\n",
    "#Number of Words\n",
    "comments['n_tokens'] = comments.tokens.apply(len)\n",
    "\n",
    "#Comment Keywords\n",
    "comments['keywords'] = comments.body.apply(newspaper.nlp.keywords)\n",
    "\n",
    "print comments.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create Article Features</h2>\n",
    "Here we create freatures strictly from the articles, a large variety of attribute features were collected in the colelction factory however we are most interested in examining the content of the comments. In order to examine the content we want to create parse, stemmed token for each block of text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'authors', u'keywords', u'publish_date', u'summary', u'text', u'url',\n",
       "       u'author', u'created_utc', u'domain', u'downs', u'gilded', u'is_self',\n",
       "       u'likes', u'media', u'id', u'num_comments', u'num_reports', u'over_18',\n",
       "       u'permalink', u'score', u'selftext', u'subreddit', u'thumbnail',\n",
       "       u'title', u'ups', u'tokens', u'article_len', u'n_tokens'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create Article Features\n",
    "#########################\n",
    "#Tokens\n",
    "articles['tokens'] = articles['text'].apply(lambda text: Counter(stem_tokens(get_tokens(text), stemmer)))\n",
    "\n",
    "#Article Length\n",
    "articles['article_len'] = articles['text'].apply(len)\n",
    "\n",
    "#Number of Words\n",
    "articles['n_tokens'] = articles['text'].apply(len)\n",
    "\n",
    "articles.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets write the final article and comment data to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "articles.to_csv('./train_articles.csv',sep=',',index=False)\n",
    "comments.to_csv('./train_comments.csv',sep=',',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have raw content, stemmed tokens, keywords for each comment and article. Now we can start to look at the relationships between comment content and article content. The features we created are largly bag of word representations of either the full text or keywords of the articles and the comments. One way we can compare these bags of words is to use the cosine similarity. \n",
    "<br>\n",
    "<br>\n",
    "The cosine similarity, measures the similarity bewteen two bags of words or vectors where A and B represent two vectors.  \n",
    "\n",
    "$$\\frac{A*B}{|A||B|}$$\n",
    "\n",
    "The cosine distance was chosen becasue it accounts for the length of the vectors. This is important when we are comparing articles, which tend to be quite long, to comments which can be quite short. \n",
    "\n",
    "The downside of using cosine similarity is that is does not account for the global popularity of a term. This means that two vectors with a few rare terms in common is the same as two vectors with a few popular words in common. This is an obvious issue we would like to address by using the term frequency- inverse document frequency weight of each word. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def get_cosine(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions define the construction of several similarity features. Each function applies the get_cosine() function to different bags of words for the article and the comment.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Compute the similarity between the keyword of the comment and it's corresponding article. \n",
    "def keyword_sim(comment_row):\n",
    "    try:\n",
    "        c_kw = comment_row['keywords']\n",
    "        a_kw = articles[articles['id']==comment_row['pid']]['keywords'].iloc[0]\n",
    "        sim = get_cosine(c_kw,a_kw)\n",
    "    except:\n",
    "        sim = 0.0\n",
    "    return sim\n",
    "\n",
    "comments['keyword_sim']=comments.apply(keyword_sim,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Computes the similarity between the tokens of the comment and it's corresponding article.\n",
    "def token_sim(comment_row):\n",
    "    try:\n",
    "        c = comment_row['tokens']\n",
    "        a = articles[articles['id']==comment_row['pid']]['tokens'].iloc[0]\n",
    "        sim = get_cosine(c,a)\n",
    "    except:\n",
    "        sim = 0.0\n",
    "    return sim\n",
    "\n",
    "comments['token_sim']=comments.apply(token_sim,axis=1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>User Similarity</h2>\n",
    "In order to determine a users overall knowledge of a topic we compute a user vocabulary. The user vocabulary is a bag of words consisting of *All* of the comments a user has made (in the articles we collected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Here We Build a Vocab for each user\n",
    "user_vocab = {}\n",
    "for author,vocab in comments[['author','tokens']].groupby('author'):\n",
    "    user_vocab[author] = sum((Counter(dict(x)) for x in vocab.tokens),Counter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute the similarity between the posting users vocabulary and the token of the article corresponding to this comment.\n",
    "def user_vocab_tokens_sim(comment_row):\n",
    "    try:\n",
    "        c = user_vocab[comment_row.author]\n",
    "        a = articles[articles['id']==comment_row['pid']]['tokens'].iloc[0]\n",
    "        sim = get_cosine(c,a)\n",
    "    except:\n",
    "        sim = 0.0\n",
    "    return sim\n",
    "\n",
    "comments['user_vocab_tokens']=comments.apply(user_vocab_tokens_sim,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute the similarity between the posting users vocabulary and the keywords of the article corresponding to this comment.\n",
    "def user_vocab_kw_sim(comment_row):\n",
    "    try:\n",
    "        c = user_vocab[comment_row.author]\n",
    "        a = articles[articles['id']==comment_row['pid']]['keywords'].iloc[0]\n",
    "        sim = get_cosine(c,a)\n",
    "    except:\n",
    "        sim = 0.0\n",
    "    return sim\n",
    "\n",
    "comments['user_vocab_kw']=comments.apply(user_vocab_kw_sim,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Reduce the output</h2>\n",
    "Here we filter the output to attributes and the newly computed similarity features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'id', u'likes', u'link_id', u'name', u'parent_id', u'score',\n",
       "       u'subreddit', u'ups', u'pid', u'comment_length', u'n_tokens',\n",
       "       u'keyword_sim', u'token_sim', u'user_vocab_tokens', u'user_vocab_kw'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_features = comments[['id','likes','link_id','name','parent_id','score','subreddit','ups','pid','comment_length','n_tokens','keyword_sim','token_sim','user_vocab_tokens','user_vocab_kw']]\n",
    "sim_features.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sim_features.to_csv('./new_sim_features.csv',sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sim_features = pd.read_csv('./new_sim_features.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
